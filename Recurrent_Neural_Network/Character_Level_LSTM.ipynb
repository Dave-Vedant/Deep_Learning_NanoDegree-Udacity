{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Character_Level-LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPg37GstOLBZuIl4+mVBlWh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vedantdave77/project.Orca/blob/master/Character_Level_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OO8nxUN65Ce3",
        "colab_type": "text"
      },
      "source": [
        "# Character Level LSTM in PyTorch\n",
        "\n",
        "Here, I will train the character-level LSTM with PyTorch. The network will train character level RNN, which can able to generate new text from the reference text give to it. \n",
        "\n",
        "> I will use PyTorch library, in colab due to gpu library -> *.dll has issue with miniconda environment. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVltfOuv50pO",
        "colab_type": "text"
      },
      "source": [
        "## Load required libraries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bahon-HBtasK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pl6NaHi06JmL",
        "colab_type": "text"
      },
      "source": [
        "## Load Data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhsyA3nN6EON",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "d8b0fa3e-7776-4dc4-dbe5-05c8a6cac024"
      },
      "source": [
        "# we have text data\n",
        "with open('/content/anna.txt','r') as data:\n",
        "  text = data.read()\n",
        "\n",
        "# check first 100 characters of the book\n",
        "text[:100]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverythin'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91qfm1af7Tt7",
        "colab_type": "text"
      },
      "source": [
        "## Tockenization\n",
        "Tockenization is process to conver each character to numeric value. Neural Network is mathematical model and thats why we need to convert data to numeric value for training model. \n",
        "\n",
        "So, its one kind of encoding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jIm8jQde6v_I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "chars = tuple(set(text))                              # consider each unique charater and make set and converts tuple\n",
        "int2char = dict(enumerate(chars))                     # make dictionary of unique characters and index. ex: (1,a)\n",
        "char2int = {ch: n for n, ch in int2char.items()}      # reverse the dictionary ex: (a,1)\n",
        "\n",
        "# encode the text\n",
        "encoded = np.array([char2int[ch] for ch in text])"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S32hdeUX9E01",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "0ac3d208-64c7-4907-f9e6-4872c6744b55"
      },
      "source": [
        "encoded[:100]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([36, 76, 74, 34, 75,  9, 50, 28, 58, 68, 68, 68,  0, 74, 34, 34, 14,\n",
              "       28,  2, 74, 53, 12, 59, 12,  9,  8, 28, 74, 50,  9, 28, 74, 59, 59,\n",
              "       28, 74, 59, 12, 71,  9,  1, 28,  9,  7,  9, 50, 14, 28, 39,  5, 76,\n",
              "       74, 34, 34, 14, 28,  2, 74, 53, 12, 59, 14, 28, 12,  8, 28, 39,  5,\n",
              "       76, 74, 34, 34, 14, 28, 12,  5, 28, 12, 75,  8, 28, 11, 24,  5, 68,\n",
              "       24, 74, 14, 13, 68, 68, 41,  7,  9, 50, 14, 75, 76, 12,  5])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdjW1de2_2R4",
        "colab_type": "text"
      },
      "source": [
        "## Preprocessing of data \n",
        "\n",
        "LSTM need data in numeric form, but the encoding we did is not useful, so the idea is: first use all the characters as features, generate one vector which have 1 for corresponding index, and 0 for rest vector. \n",
        "\n",
        "Here is the function for that.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yo2Fwfrg9Rue",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def one_hot_encode(array, n_labels):\n",
        "  # initialize array \n",
        "  one_hot = np.zeros((array.size,n_labels),dtype = np.float32)\n",
        "  #print(one_hot.shape)\n",
        "\n",
        "  # fill the elements with ones for corresponding posisions\n",
        "  one_hot[np.arange(one_hot.shape[0]),array.flatten()] =1.\n",
        "\n",
        "  # finally reshape to original array\n",
        "  one_hot = one_hot.reshape((*array.shape,n_labels))\n",
        "\n",
        "  return one_hot "
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKdrRInPKTps",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "31071204-14fd-4d8b-953b-5e913c673433"
      },
      "source": [
        "# check the above function \n",
        "test_seq = np.array([[3,5,1]])\n",
        "one_hot_test = one_hot_encode(test_seq,8)\n",
        "\n",
        "print(one_hot_test)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3, 8)\n",
            "[[[0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "  [0. 1. 0. 0. 0. 0. 0. 0.]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frYsskH5ONYm",
        "colab_type": "text"
      },
      "source": [
        "# Creating batches\n",
        "\n",
        "Now, we have one long sequence. Its efficient to convert and seperate them into batch sizes. No of batch size returns equal amount of subsequence. Its also obvious that we can not use the full length of sequence for training, and thats why we need to choose sequence length. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aue6BXG7KlMp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_batches(array, batch_size, seq_length):\n",
        "  batch_size_total = batch_size * seq_length                  # total batch_size means total elements consider by lstm at a time. \n",
        "  n_batches = len(array)//batch_size_total                    # total no. of parts of sub_sequznece. \n",
        "\n",
        "  array = array[:n_batches * batch_size_total]                # ensure full batches (make even devide elements)\n",
        "  array = array.reshape((batch_size,-1))                      # reshape into batch_size_row.\n",
        "\n",
        "  for n in range(0,array.shape[1],seq_length):                # iterate though array, but takes one sequence at a time.\n",
        "    x = array[:,n:n+seq_length]                               # features => one feature length = seq_length\n",
        "    y = np.zeros_like(x)                                      # generate target and shifted by one.\n",
        "    try:                                                      # Exception for errors.\n",
        "      y[:,:-1],y[:,-1] = x[:,1:], array[:,n+seq_length]\n",
        "    except IndexError:\n",
        "      y[:,:-1],y[:,-1] = x[:,1:], array[:,0]\n",
        "    yield x,y"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bQ4Ga0Gsv5A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Unit Test:\n",
        "batches = get_batches(encoded, 8,50)\n",
        "x,y = next(batches)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDrwhGlotAc1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "2cd91d91-8f97-4e67-8c64-3155fe60c160"
      },
      "source": [
        "print('X',x[:10,:10])\n",
        "print(\"==================\")\n",
        "print('y',y[:10,:10])\n",
        "print(\" \")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X [[36 76 74 34 75  9 50 28 58 68]\n",
            " [ 8 11  5 28 75 76 74 75 28 74]\n",
            " [ 9  5 82 28 11 50 28 74 28  2]\n",
            " [ 8 28 75 76  9 28 10 76 12  9]\n",
            " [28  8 74 24 28 76  9 50 28 75]\n",
            " [10 39  8  8 12 11  5 28 74  5]\n",
            " [28 43  5  5 74 28 76 74 82 28]\n",
            " [54 73 59 11  5  8 71 14 13 28]]\n",
            "==================\n",
            "y [[76 74 34 75  9 50 28 58 68 68]\n",
            " [11  5 28 75 76 74 75 28 74 75]\n",
            " [ 5 82 28 11 50 28 74 28  2 11]\n",
            " [28 75 76  9 28 10 76 12  9  2]\n",
            " [ 8 74 24 28 76  9 50 28 75  9]\n",
            " [39  8  8 12 11  5 28 74  5 82]\n",
            " [43  5  5 74 28 76 74 82 28  8]\n",
            " [73 59 11  5  8 71 14 13 28 52]]\n",
            " \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3mpAiTtuJx7",
        "colab_type": "text"
      },
      "source": [
        "Here, Y is one step shifter to X.\n",
        "\n",
        "\n",
        "----\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1x35OMouulVQ",
        "colab_type": "text"
      },
      "source": [
        "## Defining network with PyTorch:\n",
        "\n",
        "I will use [PyTorch documentation for LSTM](https://pytorch.org/docs/stable/nn.html#lstm) to generate custom Character RNN, LSTM model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKtx6GrItYGE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class charLSTM(nn.Module):\n",
        "  \n",
        "  def __init__(self, tockens, n_hidden_size, n_layers,drop_prob, lr):\n",
        "    super().__init__()\n",
        "    self.drop_prob = drop_prob                        # drop out probability [avoid overfiting, generalize model]\n",
        "    self.n_layers = n_layers                          # no. of hidden layers\n",
        "    self.n_hidden_size = n_hidden_size                # hidden layer size\n",
        "    self.lr = lr                                      # learning_rate\n",
        "\n",
        "    # creating directory\n",
        "    self.chars = tockens\n",
        "    self.int2char =  dict(enumerate(self.chars))\n",
        "    self.char2int = {ch: n for n , ch in self.int2char.items()}\n",
        "\n",
        "    # defining layers (LSTM, dropout, fully connected)\n",
        "    self.lstm = nn.LSTM(len(self.chars),n_hidden_size,n_layers, dropout = drop_prob, batch_first=True)  \n",
        "    self.dropout = nn.Dropout(drop_prob)\n",
        "    self.fc = nn.Linear(n_hidden,len(self.chars))       # we had batch_first=True so, last_layer size = first_layer size\n",
        "\n",
        "  # generate forward flow function\n",
        "  def forward(self,x,hidden):\n",
        "    r_output, hidden = self.lstm(x,hidden)\n",
        "    out = self.dropout(r_output)\n",
        "    out = out.contiguous().view(-1,self.n_hidden_size)    # to reshape output, because of stacked RNN...\n",
        "    out = self.fc(out)\n",
        "    return out, hidden \n",
        "  \n",
        "  # generate hidden state due to stacked RNN model, so output of layer_1 lstm as input to next lstm layer_2.\n",
        "  def init_hidden(self, batch_size):\n",
        "    weight = next(self.parameters()).data\n",
        "    if (train_on_gpu):\n",
        "      hidden = (weight.new(self.n_layers, batch_size, self.n_hidden_size).zero_().cuda(),\n",
        "                    weight.new(self.n_layers, batch_size, self.n_hidden_size).zero_().cuda())\n",
        "    else:\n",
        "      hidden = (weight.new(self.n_layers, batch_size, self.n_hidden_size).zero_(),\n",
        "                      weight.new(self.n_layers, batch_size, self.n_hidden_size).zero_())\n",
        "        \n",
        "    return hidden"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrJ8ONlK0UB2",
        "colab_type": "text"
      },
      "source": [
        "### Training Algorithm\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RSFbdmu9vcQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "24256681-b830-494a-cf1b-eaf5188778ca"
      },
      "source": [
        "train_on_gpu = torch.cuda.is_available()\n",
        "if(train_on_gpu):\n",
        "  print(\"Training of GPU\")\n",
        "else:\n",
        "  print(\"No Gpu Available, training of CPU, plz consider you epoch very small\")"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training of GPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLCs8XN236hU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(net, data, epochs, batch_size, seq_length, lr, clips, val_fraction, print_every):\n",
        "  net.train()\n",
        "  optimizer = torch.optim.Adam(net.parameters(),lr=lr)\n",
        "  criterian = nn.CrossEntropyLoss()\n",
        "\n",
        "  # prepare training and valiataion data\n",
        "  val_idx = int(len(data)* (1-val_fraction))    # for val=0.2, it get index of data from where val data will start.\n",
        "  data,val_data = data[:val_idx],data[val_idx:]\n",
        "\n",
        "  if(train_on_gpu):\n",
        "    net.cuda()\n",
        "\n",
        "  counter = 0\n",
        "  n_chars = len(net.chars)\n",
        "  for e in range(epochs):\n",
        "    print(\"=========== New Epoch ===========\")\n",
        "    h = net.init_hidden(batch_size)\n",
        "\n",
        "    for x,y in get_batches(data, batch_size, seq_length):\n",
        "      counter +=1\n",
        "\n",
        "      # one hot encoding (data preprocessing)\n",
        "      x = one_hot_encode(x,n_chars)\n",
        "      inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
        "      if(train_on_gpu):\n",
        "        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "      \n",
        "      h = tuple([each.data for each in h])                                      # generate new variable and save data of hidden_state\n",
        "      \n",
        "      net.zero_grad()                                                           # zero accumulated gradients\n",
        "      output, h = net(inputs,h)                                                 # get model output\n",
        "      # loss and back propagation\n",
        "      loss = criterian(output,targets.view(batch_size * seq_length).long())\n",
        "      loss.backward()\n",
        "\n",
        "      nn.utils.clip_grad_norm_(net.parameters(),clips)                           # prevent gradient exploding, (treat vanishing gradient problem)\n",
        "      optimizer.step()\n",
        "\n",
        "      # calculate and update loss\n",
        "      if counter%print_every ==0:\n",
        "        # get validation loss\n",
        "        val_h = net.init_hidden(batch_size)\n",
        "        val_losses = []\n",
        "        net.eval()\n",
        "        for x,y in get_batches(val_data,batch_size,seq_length):\n",
        "          x = one_hot_encode(x,n_chars)\n",
        "          x,y = torch.from_numpy(x),torch.from_numpy(y)\n",
        "\n",
        "          val_h = tuple([each.data for each in val_h])\n",
        "          inputs, targets = x, y\n",
        "          if(train_on_gpu):\n",
        "            inputs,targets = inputs.cuda(), targets.cuda()\n",
        "          output, val_h = net(inputs,val_h)\n",
        "          val_loss = criterian(output,targets.view(batch_size * seq_length).long())\n",
        "          val_losses.append(val_loss.item())\n",
        "        \"\"\n",
        "        net.train() \n",
        "\n",
        "        print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "              \"steps: {}...\".format(counter),\n",
        "              \"Loss: {:.4f}\".format(loss.item()),\n",
        "              \"Val loss: {:.4f}\".format(np.mean(val_losses)))\n",
        "  \n",
        "\n",
        "\n",
        "  "
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiez13mTIFqd",
        "colab_type": "text"
      },
      "source": [
        "## Instantiating the model (define hyper parameters)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXZQgR8i36cC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define parameters\n",
        "n_hidden = 512\n",
        "n_layers = 2\n",
        "batch_size = 128\n",
        "seq_length = 100\n",
        "lr = 0.001\n",
        "print_every = 10\n",
        "clips = 5\n",
        "val_fraction = 0.1\n",
        "n_epochs = 20\n",
        "drop_prob = 0.5"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNDA-1xbKyTQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1fac3cdd-b06f-4e79-aead-375343188d17"
      },
      "source": [
        "# train model \n",
        "net = charLSTM(chars, n_hidden,n_layers,drop_prob,lr)\n",
        "print(net)\n",
        "print(\"=====================================\")\n",
        "Print(\"===============TRAINING==============\")\n",
        "print(\"======================================\")\n",
        "train(net,encoded, epochs = n_epochs, batch_size = batch_size, seq_length= seq_length, lr=lr , clips =clips, val_fraction = val_fraction, print_every = print_every)"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "charLSTM(\n",
            "  (lstm): LSTM(83, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (fc): Linear(in_features=512, out_features=83, bias=True)\n",
            ")\n",
            "=====================================\n",
            "=========== New Epoch ===========\n",
            "Epoch: 1/20... steps: 10... Loss: 3.2671 Val loss: 3.2193\n",
            "Epoch: 1/20... steps: 20... Loss: 3.1583 Val loss: 3.1437\n",
            "Epoch: 1/20... steps: 30... Loss: 3.1478 Val loss: 3.1260\n",
            "Epoch: 1/20... steps: 40... Loss: 3.1144 Val loss: 3.1200\n",
            "Epoch: 1/20... steps: 50... Loss: 3.1456 Val loss: 3.1180\n",
            "Epoch: 1/20... steps: 60... Loss: 3.1215 Val loss: 3.1162\n",
            "Epoch: 1/20... steps: 70... Loss: 3.1106 Val loss: 3.1151\n",
            "Epoch: 1/20... steps: 80... Loss: 3.1237 Val loss: 3.1122\n",
            "Epoch: 1/20... steps: 90... Loss: 3.1200 Val loss: 3.1052\n",
            "Epoch: 1/20... steps: 100... Loss: 3.0970 Val loss: 3.0881\n",
            "Epoch: 1/20... steps: 110... Loss: 3.0607 Val loss: 3.0493\n",
            "Epoch: 1/20... steps: 120... Loss: 2.9582 Val loss: 2.9975\n",
            "Epoch: 1/20... steps: 130... Loss: 2.9095 Val loss: 2.8752\n",
            "=========== New Epoch ===========\n",
            "Epoch: 2/20... steps: 140... Loss: 2.7935 Val loss: 2.7580\n",
            "Epoch: 2/20... steps: 150... Loss: 2.7366 Val loss: 2.6801\n",
            "Epoch: 2/20... steps: 160... Loss: 2.6178 Val loss: 2.5776\n",
            "Epoch: 2/20... steps: 170... Loss: 2.5360 Val loss: 2.5147\n",
            "Epoch: 2/20... steps: 180... Loss: 2.4953 Val loss: 2.4681\n",
            "Epoch: 2/20... steps: 190... Loss: 2.4475 Val loss: 2.4323\n",
            "Epoch: 2/20... steps: 200... Loss: 2.4445 Val loss: 2.4143\n",
            "Epoch: 2/20... steps: 210... Loss: 2.4085 Val loss: 2.3805\n",
            "Epoch: 2/20... steps: 220... Loss: 2.3699 Val loss: 2.3473\n",
            "Epoch: 2/20... steps: 230... Loss: 2.3605 Val loss: 2.3203\n",
            "Epoch: 2/20... steps: 240... Loss: 2.3288 Val loss: 2.2959\n",
            "Epoch: 2/20... steps: 250... Loss: 2.2733 Val loss: 2.2759\n",
            "Epoch: 2/20... steps: 260... Loss: 2.2487 Val loss: 2.2515\n",
            "Epoch: 2/20... steps: 270... Loss: 2.2503 Val loss: 2.2242\n",
            "=========== New Epoch ===========\n",
            "Epoch: 3/20... steps: 280... Loss: 2.2466 Val loss: 2.2019\n",
            "Epoch: 3/20... steps: 290... Loss: 2.2133 Val loss: 2.1792\n",
            "Epoch: 3/20... steps: 300... Loss: 2.1881 Val loss: 2.1548\n",
            "Epoch: 3/20... steps: 310... Loss: 2.1597 Val loss: 2.1321\n",
            "Epoch: 3/20... steps: 320... Loss: 2.1368 Val loss: 2.1192\n",
            "Epoch: 3/20... steps: 330... Loss: 2.0981 Val loss: 2.0980\n",
            "Epoch: 3/20... steps: 340... Loss: 2.1218 Val loss: 2.0792\n",
            "Epoch: 3/20... steps: 350... Loss: 2.0992 Val loss: 2.0556\n",
            "Epoch: 3/20... steps: 360... Loss: 2.0277 Val loss: 2.0363\n",
            "Epoch: 3/20... steps: 370... Loss: 2.0534 Val loss: 2.0231\n",
            "Epoch: 3/20... steps: 380... Loss: 2.0283 Val loss: 2.0041\n",
            "Epoch: 3/20... steps: 390... Loss: 2.0061 Val loss: 1.9890\n",
            "Epoch: 3/20... steps: 400... Loss: 1.9770 Val loss: 1.9685\n",
            "Epoch: 3/20... steps: 410... Loss: 1.9868 Val loss: 1.9507\n",
            "=========== New Epoch ===========\n",
            "Epoch: 4/20... steps: 420... Loss: 1.9688 Val loss: 1.9387\n",
            "Epoch: 4/20... steps: 430... Loss: 1.9573 Val loss: 1.9204\n",
            "Epoch: 4/20... steps: 440... Loss: 1.9391 Val loss: 1.9092\n",
            "Epoch: 4/20... steps: 450... Loss: 1.8821 Val loss: 1.8923\n",
            "Epoch: 4/20... steps: 460... Loss: 1.8653 Val loss: 1.8807\n",
            "Epoch: 4/20... steps: 470... Loss: 1.8993 Val loss: 1.8698\n",
            "Epoch: 4/20... steps: 480... Loss: 1.8834 Val loss: 1.8546\n",
            "Epoch: 4/20... steps: 490... Loss: 1.8832 Val loss: 1.8424\n",
            "Epoch: 4/20... steps: 500... Loss: 1.8782 Val loss: 1.8336\n",
            "Epoch: 4/20... steps: 510... Loss: 1.8393 Val loss: 1.8171\n",
            "Epoch: 4/20... steps: 520... Loss: 1.8635 Val loss: 1.8068\n",
            "Epoch: 4/20... steps: 530... Loss: 1.8167 Val loss: 1.7985\n",
            "Epoch: 4/20... steps: 540... Loss: 1.7864 Val loss: 1.7915\n",
            "Epoch: 4/20... steps: 550... Loss: 1.8251 Val loss: 1.7746\n",
            "=========== New Epoch ===========\n",
            "Epoch: 5/20... steps: 560... Loss: 1.7940 Val loss: 1.7659\n",
            "Epoch: 5/20... steps: 570... Loss: 1.7798 Val loss: 1.7568\n",
            "Epoch: 5/20... steps: 580... Loss: 1.7552 Val loss: 1.7474\n",
            "Epoch: 5/20... steps: 590... Loss: 1.7604 Val loss: 1.7376\n",
            "Epoch: 5/20... steps: 600... Loss: 1.7407 Val loss: 1.7305\n",
            "Epoch: 5/20... steps: 610... Loss: 1.7338 Val loss: 1.7262\n",
            "Epoch: 5/20... steps: 620... Loss: 1.7350 Val loss: 1.7181\n",
            "Epoch: 5/20... steps: 630... Loss: 1.7403 Val loss: 1.7119\n",
            "Epoch: 5/20... steps: 640... Loss: 1.7152 Val loss: 1.6984\n",
            "Epoch: 5/20... steps: 650... Loss: 1.7073 Val loss: 1.6922\n",
            "Epoch: 5/20... steps: 660... Loss: 1.6754 Val loss: 1.6844\n",
            "Epoch: 5/20... steps: 670... Loss: 1.7081 Val loss: 1.6779\n",
            "Epoch: 5/20... steps: 680... Loss: 1.7017 Val loss: 1.6711\n",
            "Epoch: 5/20... steps: 690... Loss: 1.6709 Val loss: 1.6627\n",
            "=========== New Epoch ===========\n",
            "Epoch: 6/20... steps: 700... Loss: 1.6725 Val loss: 1.6553\n",
            "Epoch: 6/20... steps: 710... Loss: 1.6648 Val loss: 1.6535\n",
            "Epoch: 6/20... steps: 720... Loss: 1.6565 Val loss: 1.6456\n",
            "Epoch: 6/20... steps: 730... Loss: 1.6713 Val loss: 1.6355\n",
            "Epoch: 6/20... steps: 740... Loss: 1.6359 Val loss: 1.6321\n",
            "Epoch: 6/20... steps: 750... Loss: 1.6110 Val loss: 1.6272\n",
            "Epoch: 6/20... steps: 760... Loss: 1.6471 Val loss: 1.6176\n",
            "Epoch: 6/20... steps: 770... Loss: 1.6342 Val loss: 1.6144\n",
            "Epoch: 6/20... steps: 780... Loss: 1.6108 Val loss: 1.6083\n",
            "Epoch: 6/20... steps: 790... Loss: 1.6009 Val loss: 1.6045\n",
            "Epoch: 6/20... steps: 800... Loss: 1.6178 Val loss: 1.6011\n",
            "Epoch: 6/20... steps: 810... Loss: 1.6056 Val loss: 1.5939\n",
            "Epoch: 6/20... steps: 820... Loss: 1.5623 Val loss: 1.5876\n",
            "Epoch: 6/20... steps: 830... Loss: 1.6084 Val loss: 1.5804\n",
            "=========== New Epoch ===========\n",
            "Epoch: 7/20... steps: 840... Loss: 1.5690 Val loss: 1.5760\n",
            "Epoch: 7/20... steps: 850... Loss: 1.5812 Val loss: 1.5735\n",
            "Epoch: 7/20... steps: 860... Loss: 1.5660 Val loss: 1.5660\n",
            "Epoch: 7/20... steps: 870... Loss: 1.5683 Val loss: 1.5630\n",
            "Epoch: 7/20... steps: 880... Loss: 1.5806 Val loss: 1.5589\n",
            "Epoch: 7/20... steps: 890... Loss: 1.5750 Val loss: 1.5573\n",
            "Epoch: 7/20... steps: 900... Loss: 1.5548 Val loss: 1.5538\n",
            "Epoch: 7/20... steps: 910... Loss: 1.5173 Val loss: 1.5475\n",
            "Epoch: 7/20... steps: 920... Loss: 1.5542 Val loss: 1.5428\n",
            "Epoch: 7/20... steps: 930... Loss: 1.5334 Val loss: 1.5377\n",
            "Epoch: 7/20... steps: 940... Loss: 1.5364 Val loss: 1.5369\n",
            "Epoch: 7/20... steps: 950... Loss: 1.5449 Val loss: 1.5327\n",
            "Epoch: 7/20... steps: 960... Loss: 1.5568 Val loss: 1.5275\n",
            "Epoch: 7/20... steps: 970... Loss: 1.5453 Val loss: 1.5247\n",
            "=========== New Epoch ===========\n",
            "Epoch: 8/20... steps: 980... Loss: 1.5300 Val loss: 1.5216\n",
            "Epoch: 8/20... steps: 990... Loss: 1.5240 Val loss: 1.5175\n",
            "Epoch: 8/20... steps: 1000... Loss: 1.5167 Val loss: 1.5115\n",
            "Epoch: 8/20... steps: 1010... Loss: 1.5537 Val loss: 1.5094\n",
            "Epoch: 8/20... steps: 1020... Loss: 1.5242 Val loss: 1.5105\n",
            "Epoch: 8/20... steps: 1030... Loss: 1.5043 Val loss: 1.5066\n",
            "Epoch: 8/20... steps: 1040... Loss: 1.5107 Val loss: 1.5085\n",
            "Epoch: 8/20... steps: 1050... Loss: 1.5045 Val loss: 1.4985\n",
            "Epoch: 8/20... steps: 1060... Loss: 1.4949 Val loss: 1.4938\n",
            "Epoch: 8/20... steps: 1070... Loss: 1.5021 Val loss: 1.4902\n",
            "Epoch: 8/20... steps: 1080... Loss: 1.4973 Val loss: 1.4859\n",
            "Epoch: 8/20... steps: 1090... Loss: 1.4867 Val loss: 1.4818\n",
            "Epoch: 8/20... steps: 1100... Loss: 1.4717 Val loss: 1.4836\n",
            "Epoch: 8/20... steps: 1110... Loss: 1.4773 Val loss: 1.4797\n",
            "=========== New Epoch ===========\n",
            "Epoch: 9/20... steps: 1120... Loss: 1.4944 Val loss: 1.4757\n",
            "Epoch: 9/20... steps: 1130... Loss: 1.4851 Val loss: 1.4726\n",
            "Epoch: 9/20... steps: 1140... Loss: 1.4895 Val loss: 1.4681\n",
            "Epoch: 9/20... steps: 1150... Loss: 1.5045 Val loss: 1.4685\n",
            "Epoch: 9/20... steps: 1160... Loss: 1.4462 Val loss: 1.4650\n",
            "Epoch: 9/20... steps: 1170... Loss: 1.4588 Val loss: 1.4671\n",
            "Epoch: 9/20... steps: 1180... Loss: 1.4595 Val loss: 1.4679\n",
            "Epoch: 9/20... steps: 1190... Loss: 1.4895 Val loss: 1.4583\n",
            "Epoch: 9/20... steps: 1200... Loss: 1.4397 Val loss: 1.4555\n",
            "Epoch: 9/20... steps: 1210... Loss: 1.4462 Val loss: 1.4568\n",
            "Epoch: 9/20... steps: 1220... Loss: 1.4527 Val loss: 1.4518\n",
            "Epoch: 9/20... steps: 1230... Loss: 1.4243 Val loss: 1.4488\n",
            "Epoch: 9/20... steps: 1240... Loss: 1.4371 Val loss: 1.4463\n",
            "Epoch: 9/20... steps: 1250... Loss: 1.4450 Val loss: 1.4431\n",
            "=========== New Epoch ===========\n",
            "Epoch: 10/20... steps: 1260... Loss: 1.4419 Val loss: 1.4411\n",
            "Epoch: 10/20... steps: 1270... Loss: 1.4499 Val loss: 1.4384\n",
            "Epoch: 10/20... steps: 1280... Loss: 1.4585 Val loss: 1.4338\n",
            "Epoch: 10/20... steps: 1290... Loss: 1.4378 Val loss: 1.4420\n",
            "Epoch: 10/20... steps: 1300... Loss: 1.4325 Val loss: 1.4338\n",
            "Epoch: 10/20... steps: 1310... Loss: 1.4364 Val loss: 1.4347\n",
            "Epoch: 10/20... steps: 1320... Loss: 1.4027 Val loss: 1.4368\n",
            "Epoch: 10/20... steps: 1330... Loss: 1.4173 Val loss: 1.4308\n",
            "Epoch: 10/20... steps: 1340... Loss: 1.4046 Val loss: 1.4254\n",
            "Epoch: 10/20... steps: 1350... Loss: 1.3927 Val loss: 1.4226\n",
            "Epoch: 10/20... steps: 1360... Loss: 1.4082 Val loss: 1.4239\n",
            "Epoch: 10/20... steps: 1370... Loss: 1.3922 Val loss: 1.4200\n",
            "Epoch: 10/20... steps: 1380... Loss: 1.4260 Val loss: 1.4195\n",
            "Epoch: 10/20... steps: 1390... Loss: 1.4290 Val loss: 1.4138\n",
            "=========== New Epoch ===========\n",
            "Epoch: 11/20... steps: 1400... Loss: 1.4390 Val loss: 1.4149\n",
            "Epoch: 11/20... steps: 1410... Loss: 1.4482 Val loss: 1.4139\n",
            "Epoch: 11/20... steps: 1420... Loss: 1.4380 Val loss: 1.4119\n",
            "Epoch: 11/20... steps: 1430... Loss: 1.4093 Val loss: 1.4124\n",
            "Epoch: 11/20... steps: 1440... Loss: 1.4342 Val loss: 1.4083\n",
            "Epoch: 11/20... steps: 1450... Loss: 1.3578 Val loss: 1.4082\n",
            "Epoch: 11/20... steps: 1460... Loss: 1.3825 Val loss: 1.4102\n",
            "Epoch: 11/20... steps: 1470... Loss: 1.3850 Val loss: 1.4127\n",
            "Epoch: 11/20... steps: 1480... Loss: 1.3942 Val loss: 1.4025\n",
            "Epoch: 11/20... steps: 1490... Loss: 1.3886 Val loss: 1.3978\n",
            "Epoch: 11/20... steps: 1500... Loss: 1.3746 Val loss: 1.4019\n",
            "Epoch: 11/20... steps: 1510... Loss: 1.3489 Val loss: 1.3953\n",
            "Epoch: 11/20... steps: 1520... Loss: 1.3969 Val loss: 1.3930\n",
            "=========== New Epoch ===========\n",
            "Epoch: 12/20... steps: 1530... Loss: 1.4324 Val loss: 1.3916\n",
            "Epoch: 12/20... steps: 1540... Loss: 1.3939 Val loss: 1.3906\n",
            "Epoch: 12/20... steps: 1550... Loss: 1.4066 Val loss: 1.3887\n",
            "Epoch: 12/20... steps: 1560... Loss: 1.4154 Val loss: 1.3878\n",
            "Epoch: 12/20... steps: 1570... Loss: 1.3569 Val loss: 1.3900\n",
            "Epoch: 12/20... steps: 1580... Loss: 1.3381 Val loss: 1.3862\n",
            "Epoch: 12/20... steps: 1590... Loss: 1.3364 Val loss: 1.3900\n",
            "Epoch: 12/20... steps: 1600... Loss: 1.3653 Val loss: 1.3880\n",
            "Epoch: 12/20... steps: 1610... Loss: 1.3495 Val loss: 1.3873\n",
            "Epoch: 12/20... steps: 1620... Loss: 1.3564 Val loss: 1.3813\n",
            "Epoch: 12/20... steps: 1630... Loss: 1.3684 Val loss: 1.3793\n",
            "Epoch: 12/20... steps: 1640... Loss: 1.3519 Val loss: 1.3809\n",
            "Epoch: 12/20... steps: 1650... Loss: 1.3277 Val loss: 1.3786\n",
            "Epoch: 12/20... steps: 1660... Loss: 1.3871 Val loss: 1.3777\n",
            "=========== New Epoch ===========\n",
            "Epoch: 13/20... steps: 1670... Loss: 1.3510 Val loss: 1.3736\n",
            "Epoch: 13/20... steps: 1680... Loss: 1.3637 Val loss: 1.3700\n",
            "Epoch: 13/20... steps: 1690... Loss: 1.3454 Val loss: 1.3690\n",
            "Epoch: 13/20... steps: 1700... Loss: 1.3463 Val loss: 1.3717\n",
            "Epoch: 13/20... steps: 1710... Loss: 1.3173 Val loss: 1.3712\n",
            "Epoch: 13/20... steps: 1720... Loss: 1.3305 Val loss: 1.3709\n",
            "Epoch: 13/20... steps: 1730... Loss: 1.3766 Val loss: 1.3701\n",
            "Epoch: 13/20... steps: 1740... Loss: 1.3344 Val loss: 1.3713\n",
            "Epoch: 13/20... steps: 1750... Loss: 1.3074 Val loss: 1.3719\n",
            "Epoch: 13/20... steps: 1760... Loss: 1.3283 Val loss: 1.3638\n",
            "Epoch: 13/20... steps: 1770... Loss: 1.3507 Val loss: 1.3642\n",
            "Epoch: 13/20... steps: 1780... Loss: 1.3237 Val loss: 1.3645\n",
            "Epoch: 13/20... steps: 1790... Loss: 1.3178 Val loss: 1.3634\n",
            "Epoch: 13/20... steps: 1800... Loss: 1.3379 Val loss: 1.3601\n",
            "=========== New Epoch ===========\n",
            "Epoch: 14/20... steps: 1810... Loss: 1.3506 Val loss: 1.3616\n",
            "Epoch: 14/20... steps: 1820... Loss: 1.3298 Val loss: 1.3571\n",
            "Epoch: 14/20... steps: 1830... Loss: 1.3422 Val loss: 1.3561\n",
            "Epoch: 14/20... steps: 1840... Loss: 1.2970 Val loss: 1.3591\n",
            "Epoch: 14/20... steps: 1850... Loss: 1.2769 Val loss: 1.3588\n",
            "Epoch: 14/20... steps: 1860... Loss: 1.3290 Val loss: 1.3555\n",
            "Epoch: 14/20... steps: 1870... Loss: 1.3385 Val loss: 1.3560\n",
            "Epoch: 14/20... steps: 1880... Loss: 1.3327 Val loss: 1.3554\n",
            "Epoch: 14/20... steps: 1890... Loss: 1.3525 Val loss: 1.3581\n",
            "Epoch: 14/20... steps: 1900... Loss: 1.3250 Val loss: 1.3537\n",
            "Epoch: 14/20... steps: 1910... Loss: 1.3218 Val loss: 1.3511\n",
            "Epoch: 14/20... steps: 1920... Loss: 1.3131 Val loss: 1.3494\n",
            "Epoch: 14/20... steps: 1930... Loss: 1.2793 Val loss: 1.3459\n",
            "Epoch: 14/20... steps: 1940... Loss: 1.3462 Val loss: 1.3471\n",
            "=========== New Epoch ===========\n",
            "Epoch: 15/20... steps: 1950... Loss: 1.3058 Val loss: 1.3462\n",
            "Epoch: 15/20... steps: 1960... Loss: 1.3079 Val loss: 1.3448\n",
            "Epoch: 15/20... steps: 1970... Loss: 1.3146 Val loss: 1.3440\n",
            "Epoch: 15/20... steps: 1980... Loss: 1.2884 Val loss: 1.3483\n",
            "Epoch: 15/20... steps: 1990... Loss: 1.2978 Val loss: 1.3429\n",
            "Epoch: 15/20... steps: 2000... Loss: 1.2787 Val loss: 1.3420\n",
            "Epoch: 15/20... steps: 2010... Loss: 1.3073 Val loss: 1.3424\n",
            "Epoch: 15/20... steps: 2020... Loss: 1.3256 Val loss: 1.3446\n",
            "Epoch: 15/20... steps: 2030... Loss: 1.2884 Val loss: 1.3436\n",
            "Epoch: 15/20... steps: 2040... Loss: 1.3000 Val loss: 1.3402\n",
            "Epoch: 15/20... steps: 2050... Loss: 1.2883 Val loss: 1.3380\n",
            "Epoch: 15/20... steps: 2060... Loss: 1.3043 Val loss: 1.3394\n",
            "Epoch: 15/20... steps: 2070... Loss: 1.3078 Val loss: 1.3383\n",
            "Epoch: 15/20... steps: 2080... Loss: 1.2941 Val loss: 1.3374\n",
            "=========== New Epoch ===========\n",
            "Epoch: 16/20... steps: 2090... Loss: 1.3017 Val loss: 1.3323\n",
            "Epoch: 16/20... steps: 2100... Loss: 1.2867 Val loss: 1.3358\n",
            "Epoch: 16/20... steps: 2110... Loss: 1.2789 Val loss: 1.3358\n",
            "Epoch: 16/20... steps: 2120... Loss: 1.2941 Val loss: 1.3344\n",
            "Epoch: 16/20... steps: 2130... Loss: 1.2772 Val loss: 1.3328\n",
            "Epoch: 16/20... steps: 2140... Loss: 1.2791 Val loss: 1.3300\n",
            "Epoch: 16/20... steps: 2150... Loss: 1.3022 Val loss: 1.3307\n",
            "Epoch: 16/20... steps: 2160... Loss: 1.2821 Val loss: 1.3291\n",
            "Epoch: 16/20... steps: 2170... Loss: 1.2802 Val loss: 1.3324\n",
            "Epoch: 16/20... steps: 2180... Loss: 1.2776 Val loss: 1.3280\n",
            "Epoch: 16/20... steps: 2190... Loss: 1.2900 Val loss: 1.3292\n",
            "Epoch: 16/20... steps: 2200... Loss: 1.2710 Val loss: 1.3277\n",
            "Epoch: 16/20... steps: 2210... Loss: 1.2462 Val loss: 1.3268\n",
            "Epoch: 16/20... steps: 2220... Loss: 1.2826 Val loss: 1.3264\n",
            "=========== New Epoch ===========\n",
            "Epoch: 17/20... steps: 2230... Loss: 1.2624 Val loss: 1.3246\n",
            "Epoch: 17/20... steps: 2240... Loss: 1.2812 Val loss: 1.3241\n",
            "Epoch: 17/20... steps: 2250... Loss: 1.2486 Val loss: 1.3232\n",
            "Epoch: 17/20... steps: 2260... Loss: 1.2652 Val loss: 1.3225\n",
            "Epoch: 17/20... steps: 2270... Loss: 1.2798 Val loss: 1.3236\n",
            "Epoch: 17/20... steps: 2280... Loss: 1.2905 Val loss: 1.3204\n",
            "Epoch: 17/20... steps: 2290... Loss: 1.2806 Val loss: 1.3189\n",
            "Epoch: 17/20... steps: 2300... Loss: 1.2447 Val loss: 1.3215\n",
            "Epoch: 17/20... steps: 2310... Loss: 1.2670 Val loss: 1.3211\n",
            "Epoch: 17/20... steps: 2320... Loss: 1.2602 Val loss: 1.3213\n",
            "Epoch: 17/20... steps: 2330... Loss: 1.2661 Val loss: 1.3209\n",
            "Epoch: 17/20... steps: 2340... Loss: 1.2775 Val loss: 1.3180\n",
            "Epoch: 17/20... steps: 2350... Loss: 1.2777 Val loss: 1.3167\n",
            "Epoch: 17/20... steps: 2360... Loss: 1.2788 Val loss: 1.3176\n",
            "=========== New Epoch ===========\n",
            "Epoch: 18/20... steps: 2370... Loss: 1.2553 Val loss: 1.3116\n",
            "Epoch: 18/20... steps: 2380... Loss: 1.2634 Val loss: 1.3147\n",
            "Epoch: 18/20... steps: 2390... Loss: 1.2656 Val loss: 1.3123\n",
            "Epoch: 18/20... steps: 2400... Loss: 1.2725 Val loss: 1.3129\n",
            "Epoch: 18/20... steps: 2410... Loss: 1.2826 Val loss: 1.3133\n",
            "Epoch: 18/20... steps: 2420... Loss: 1.2530 Val loss: 1.3096\n",
            "Epoch: 18/20... steps: 2430... Loss: 1.2682 Val loss: 1.3113\n",
            "Epoch: 18/20... steps: 2440... Loss: 1.2480 Val loss: 1.3138\n",
            "Epoch: 18/20... steps: 2450... Loss: 1.2431 Val loss: 1.3116\n",
            "Epoch: 18/20... steps: 2460... Loss: 1.2607 Val loss: 1.3123\n",
            "Epoch: 18/20... steps: 2470... Loss: 1.2521 Val loss: 1.3150\n",
            "Epoch: 18/20... steps: 2480... Loss: 1.2408 Val loss: 1.3143\n",
            "Epoch: 18/20... steps: 2490... Loss: 1.2448 Val loss: 1.3099\n",
            "Epoch: 18/20... steps: 2500... Loss: 1.2379 Val loss: 1.3103\n",
            "=========== New Epoch ===========\n",
            "Epoch: 19/20... steps: 2510... Loss: 1.2475 Val loss: 1.3057\n",
            "Epoch: 19/20... steps: 2520... Loss: 1.2604 Val loss: 1.3088\n",
            "Epoch: 19/20... steps: 2530... Loss: 1.2570 Val loss: 1.3048\n",
            "Epoch: 19/20... steps: 2540... Loss: 1.2859 Val loss: 1.3074\n",
            "Epoch: 19/20... steps: 2550... Loss: 1.2390 Val loss: 1.3068\n",
            "Epoch: 19/20... steps: 2560... Loss: 1.2488 Val loss: 1.3059\n",
            "Epoch: 19/20... steps: 2570... Loss: 1.2490 Val loss: 1.3065\n",
            "Epoch: 19/20... steps: 2580... Loss: 1.2832 Val loss: 1.3059\n",
            "Epoch: 19/20... steps: 2590... Loss: 1.2391 Val loss: 1.3048\n",
            "Epoch: 19/20... steps: 2600... Loss: 1.2351 Val loss: 1.3035\n",
            "Epoch: 19/20... steps: 2610... Loss: 1.2398 Val loss: 1.3066\n",
            "Epoch: 19/20... steps: 2620... Loss: 1.2216 Val loss: 1.3079\n",
            "Epoch: 19/20... steps: 2630... Loss: 1.2287 Val loss: 1.3056\n",
            "Epoch: 19/20... steps: 2640... Loss: 1.2470 Val loss: 1.2987\n",
            "=========== New Epoch ===========\n",
            "Epoch: 20/20... steps: 2650... Loss: 1.2453 Val loss: 1.3001\n",
            "Epoch: 20/20... steps: 2660... Loss: 1.2418 Val loss: 1.3015\n",
            "Epoch: 20/20... steps: 2670... Loss: 1.2632 Val loss: 1.2943\n",
            "Epoch: 20/20... steps: 2680... Loss: 1.2495 Val loss: 1.2994\n",
            "Epoch: 20/20... steps: 2690... Loss: 1.2491 Val loss: 1.3016\n",
            "Epoch: 20/20... steps: 2700... Loss: 1.2475 Val loss: 1.2977\n",
            "Epoch: 20/20... steps: 2710... Loss: 1.2179 Val loss: 1.3029\n",
            "Epoch: 20/20... steps: 2720... Loss: 1.2160 Val loss: 1.2999\n",
            "Epoch: 20/20... steps: 2730... Loss: 1.2078 Val loss: 1.2958\n",
            "Epoch: 20/20... steps: 2740... Loss: 1.2136 Val loss: 1.2960\n",
            "Epoch: 20/20... steps: 2750... Loss: 1.2151 Val loss: 1.2966\n",
            "Epoch: 20/20... steps: 2760... Loss: 1.2173 Val loss: 1.2977\n",
            "Epoch: 20/20... steps: 2770... Loss: 1.2576 Val loss: 1.2955\n",
            "Epoch: 20/20... steps: 2780... Loss: 1.2778 Val loss: 1.2937\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALBIgNb1lS5D",
        "colab_type": "text"
      },
      "source": [
        "## Result Conclusion and Model Improvement Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJL5YdK_x6vZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
