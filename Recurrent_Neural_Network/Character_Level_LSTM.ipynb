{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Character_Level-LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPEfS2RgJiteBvVmw4iThTD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vedantdave77/project.Orca/blob/master/Recurrent_Neural_Network/Character_Level_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OO8nxUN65Ce3",
        "colab_type": "text"
      },
      "source": [
        "# Character Level LSTM in PyTorch\n",
        "\n",
        "Here, I will train the character-level LSTM with PyTorch. The network will train character level RNN, which can able to generate new text from the reference text give to it. \n",
        "\n",
        "> I will use PyTorch library, in colab due to gpu library -> *.dll has issue with miniconda environment. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVltfOuv50pO",
        "colab_type": "text"
      },
      "source": [
        "## Load required libraries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bahon-HBtasK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pl6NaHi06JmL",
        "colab_type": "text"
      },
      "source": [
        "## Load Data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhsyA3nN6EON",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "b15d89c3-c294-473c-e4c4-7e2db064dc85"
      },
      "source": [
        "# we have text data\n",
        "with open('/content/anna.txt','r') as data:\n",
        "  text = data.read()\n",
        "\n",
        "# check first 100 characters of the book\n",
        "text[:100]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverythin'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91qfm1af7Tt7",
        "colab_type": "text"
      },
      "source": [
        "## Tockenization\n",
        "Tockenization is process to conver each character to numeric value. Neural Network is mathematical model and thats why we need to convert data to numeric value for training model. \n",
        "\n",
        "So, its one kind of encoding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jIm8jQde6v_I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "chars = tuple(set(text))                              # consider each unique charater and make set and converts tuple\n",
        "int2char = dict(enumerate(chars))                     # make dictionary of unique characters and index. ex: (1,a)\n",
        "char2int = {ch: n for n, ch in int2char.items()}      # reverse the dictionary ex: (a,1)\n",
        "\n",
        "# encode the text\n",
        "encoded = np.array([char2int[ch] for ch in text])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S32hdeUX9E01",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "26cdba34-2c56-48c3-dc94-4653a26a4136"
      },
      "source": [
        "encoded[:100]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([63, 46, 45,  6, 19, 80, 65, 39, 50, 82, 82, 82, 61, 45,  6,  6,  9,\n",
              "       39, 22, 45, 72, 21, 42, 21, 80, 71, 39, 45, 65, 80, 39, 45, 42, 42,\n",
              "       39, 45, 42, 21, 78, 80, 66, 39, 80, 10, 80, 65,  9, 39, 43, 12, 46,\n",
              "       45,  6,  6,  9, 39, 22, 45, 72, 21, 42,  9, 39, 21, 71, 39, 43, 12,\n",
              "       46, 45,  6,  6,  9, 39, 21, 12, 39, 21, 19, 71, 39, 26, 16, 12, 82,\n",
              "       16, 45,  9, 47, 82, 82, 67, 10, 80, 65,  9, 19, 46, 21, 12])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdjW1de2_2R4",
        "colab_type": "text"
      },
      "source": [
        "## Preprocessing of data \n",
        "\n",
        "LSTM need data in numeric form, but the encoding we did is not useful, so the idea is: first use all the characters as features, generate one vector which have 1 for corresponding index, and 0 for rest vector. \n",
        "\n",
        "Here is the function for that.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yo2Fwfrg9Rue",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def one_hot_encode(array, n_labels):\n",
        "  # initialize array \n",
        "  one_hot = np.zeros((array.size,n_labels),dtype = np.float32)\n",
        "  #print(one_hot.shape)\n",
        "\n",
        "  # fill the elements with ones for corresponding posisions\n",
        "  one_hot[np.arange(one_hot.shape[0]),array.flatten()] =1.\n",
        "\n",
        "  # finally reshape to original array\n",
        "  one_hot = one_hot.reshape((*array.shape,n_labels))\n",
        "\n",
        "  return one_hot "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKdrRInPKTps",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "e5515bdb-6ce1-4067-a895-c2728b2b92c6"
      },
      "source": [
        "# check the above function \n",
        "test_seq = np.array([[3,5,1]])\n",
        "one_hot_test = one_hot_encode(test_seq,8)\n",
        "\n",
        "print(one_hot_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "  [0. 1. 0. 0. 0. 0. 0. 0.]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frYsskH5ONYm",
        "colab_type": "text"
      },
      "source": [
        "# Creating batches\n",
        "\n",
        "Now, we have one long sequence. Its efficient to convert and seperate them into batch sizes. No of batch size returns equal amount of subsequence. Its also obvious that we can not use the full length of sequence for training, and thats why we need to choose sequence length. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aue6BXG7KlMp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_batches(array, batch_size, seq_length):\n",
        "  batch_size_total = batch_size * seq_length                  # total batch_size means total elements consider by lstm at a time. \n",
        "  n_batches = len(array)//batch_size_total                    # total no. of parts of sub_sequznece. \n",
        "\n",
        "  array = array[:n_batches * batch_size_total]                # ensure full batches (make even devide elements)\n",
        "  array = array.reshape((batch_size,-1))                      # reshape into batch_size_row.\n",
        "\n",
        "  for n in range(0,array.shape[1],seq_length):                # iterate though array, but takes one sequence at a time.\n",
        "    x = array[:,n:n+seq_length]                               # features => one feature length = seq_length\n",
        "    y = np.zeros_like(x)                                      # generate target and shifted by one.\n",
        "    try:                                                      # Exception for errors.\n",
        "      y[:,:-1],y[:,-1] = x[:,1:], array[:,n+seq_length]\n",
        "    except IndexError:\n",
        "      y[:,:-1],y[:,-1] = x[:,1:], array[:,0]\n",
        "    yield x,y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bQ4Ga0Gsv5A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Unit Test:\n",
        "batches = get_batches(encoded, 8,50)\n",
        "x,y = next(batches)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDrwhGlotAc1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "6c2c83d6-7a5a-4c41-9210-c078378eef98"
      },
      "source": [
        "print('X',x[:10,:10])\n",
        "print(\"==================\")\n",
        "print('y',y[:10,:10])\n",
        "print(\" \")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X [[63 46 45  6 19 80 65 39 50 82]\n",
            " [71 26 12 39 19 46 45 19 39 45]\n",
            " [80 12  2 39 26 65 39 45 39 22]\n",
            " [71 39 19 46 80 39 23 46 21 80]\n",
            " [39 71 45 16 39 46 80 65 39 19]\n",
            " [23 43 71 71 21 26 12 39 45 12]\n",
            " [39 52 12 12 45 39 46 45  2 39]\n",
            " [70 31 42 26 12 71 78  9 47 39]]\n",
            "==================\n",
            "y [[46 45  6 19 80 65 39 50 82 82]\n",
            " [26 12 39 19 46 45 19 39 45 19]\n",
            " [12  2 39 26 65 39 45 39 22 26]\n",
            " [39 19 46 80 39 23 46 21 80 22]\n",
            " [71 45 16 39 46 80 65 39 19 80]\n",
            " [43 71 71 21 26 12 39 45 12  2]\n",
            " [52 12 12 45 39 46 45  2 39 71]\n",
            " [31 42 26 12 71 78  9 47 39 44]]\n",
            " \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3mpAiTtuJx7",
        "colab_type": "text"
      },
      "source": [
        "Here, Y is one step shifter to X.\n",
        "\n",
        "\n",
        "----\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1x35OMouulVQ",
        "colab_type": "text"
      },
      "source": [
        "## Defining network with PyTorch:\n",
        "\n",
        "I will use [PyTorch documentation for LSTM](https://pytorch.org/docs/stable/nn.html#lstm) to generate custom Character RNN, LSTM model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKtx6GrItYGE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class charLSTM(nn.Module):\n",
        "  \n",
        "  def __init__(self, tockens, n_hidden_size, n_layers,drop_prob, lr):\n",
        "    super().__init__()\n",
        "    self.drop_prob = drop_prob                        # drop out probability [avoid overfiting, generalize model]\n",
        "    self.n_layers = n_layers                          # no. of hidden layers\n",
        "    self.n_hidden_size = n_hidden_size                # hidden layer size\n",
        "    self.lr = lr                                      # learning_rate\n",
        "\n",
        "    # creating directory\n",
        "    self.chars = tockens\n",
        "    self.int2char =  dict(enumerate(self.chars))\n",
        "    self.char2int = {ch: n for n , ch in self.int2char.items()}\n",
        "\n",
        "    # defining layers (LSTM, dropout, fully connected)\n",
        "    self.lstm = nn.LSTM(len(self.chars),n_hidden_size,n_layers, dropout = drop_prob, batch_first=True)  \n",
        "    self.dropout = nn.Dropout(drop_prob)\n",
        "    self.fc = nn.Linear(n_hidden,len(self.chars))       # we had batch_first=True so, last_layer size = first_layer size\n",
        "\n",
        "  # generate forward flow function\n",
        "  def forward(self,x,hidden):\n",
        "    r_output, hidden = self.lstm(x,hidden)\n",
        "    out = self.dropout(r_output)\n",
        "    out = out.contiguous().view(-1,self.n_hidden_size)    # to reshape output, because of stacked RNN...\n",
        "    out = self.fc(out)\n",
        "    return out, hidden \n",
        "  \n",
        "  # generate hidden state due to stacked RNN model, so output of layer_1 lstm as input to next lstm layer_2.\n",
        "  def init_hidden(self, batch_size):\n",
        "    weight = next(self.parameters()).data\n",
        "    if (train_on_gpu):\n",
        "      hidden = (weight.new(self.n_layers, batch_size, self.n_hidden_size).zero_().cuda(),\n",
        "                    weight.new(self.n_layers, batch_size, self.n_hidden_size).zero_().cuda())\n",
        "    else:\n",
        "      hidden = (weight.new(self.n_layers, batch_size, self.n_hidden_size).zero_(),\n",
        "                      weight.new(self.n_layers, batch_size, self.n_hidden_size).zero_())\n",
        "        \n",
        "    return hidden"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrJ8ONlK0UB2",
        "colab_type": "text"
      },
      "source": [
        "### Training Algorithm\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RSFbdmu9vcQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b26172a4-8c1d-46dc-9f7a-223a481defd8"
      },
      "source": [
        "train_on_gpu = torch.cuda.is_available()\n",
        "if(train_on_gpu):\n",
        "  print(\"Training of GPU\")\n",
        "else:\n",
        "  print(\"No Gpu Available, training of CPU, plz consider you epoch very small\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training of GPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLCs8XN236hU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(net, data, epochs, batch_size, seq_length, lr, clips, val_fraction, print_every):\n",
        "  net.train()\n",
        "  optimizer = torch.optim.Adam(net.parameters(),lr=lr)\n",
        "  criterian = nn.CrossEntropyLoss()\n",
        "\n",
        "  # prepare training and valiataion data\n",
        "  val_idx = int(len(data)* (1-val_fraction))    # for val=0.2, it get index of data from where val data will start.\n",
        "  data,val_data = data[:val_idx],data[val_idx:]\n",
        "\n",
        "  if(train_on_gpu):\n",
        "    net.cuda()\n",
        "\n",
        "  counter = 0\n",
        "  n_chars = len(net.chars)\n",
        "  for e in range(epochs):\n",
        "    print(\"=========== New Epoch ===========\")\n",
        "    h = net.init_hidden(batch_size)\n",
        "\n",
        "    for x,y in get_batches(data, batch_size, seq_length):\n",
        "      counter +=1\n",
        "\n",
        "      # one hot encoding (data preprocessing)\n",
        "      x = one_hot_encode(x,n_chars)\n",
        "      inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
        "      if(train_on_gpu):\n",
        "        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "      \n",
        "      h = tuple([each.data for each in h])                                      # generate new variable and save data of hidden_state\n",
        "      \n",
        "      net.zero_grad()                                                           # zero accumulated gradients\n",
        "      output, h = net(inputs,h)                                                 # get model output\n",
        "      # loss and back propagation\n",
        "      loss = criterian(output,targets.view(batch_size * seq_length).long())\n",
        "      loss.backward()\n",
        "\n",
        "      nn.utils.clip_grad_norm_(net.parameters(),clips)                           # prevent gradient exploding, (treat vanishing gradient problem)\n",
        "      optimizer.step()\n",
        "\n",
        "      # calculate and update loss\n",
        "      if counter%print_every ==0:\n",
        "        # get validation loss\n",
        "        val_h = net.init_hidden(batch_size)\n",
        "        val_losses = []\n",
        "        net.eval()\n",
        "        for x,y in get_batches(val_data,batch_size,seq_length):\n",
        "          x = one_hot_encode(x,n_chars)\n",
        "          x,y = torch.from_numpy(x),torch.from_numpy(y)\n",
        "\n",
        "          val_h = tuple([each.data for each in val_h])\n",
        "          inputs, targets = x, y\n",
        "          if(train_on_gpu):\n",
        "            inputs,targets = inputs.cuda(), targets.cuda()\n",
        "          output, val_h = net(inputs,val_h)\n",
        "          val_loss = criterian(output,targets.view(batch_size * seq_length).long())\n",
        "          val_losses.append(val_loss.item())\n",
        "        \"\"\n",
        "        net.train() \n",
        "\n",
        "        print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "              \"steps: {}...\".format(counter),\n",
        "              \"Loss: {:.4f}\".format(loss.item()),\n",
        "              \"Val loss: {:.4f}\".format(np.mean(val_losses)))\n",
        "  \n",
        "\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiez13mTIFqd",
        "colab_type": "text"
      },
      "source": [
        "## Instantiating the model (define hyper parameters)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXZQgR8i36cC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define parameters\n",
        "print_every = 10\n",
        "clips = 5\n",
        "val_fraction = 0.1\n",
        "n_epochs = 20\n",
        "drop_prob = 0.5\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 128         # number of sequences run through the network in one pass\n",
        "seq_length = 100         # number of character in the squence, larger is better, network can learn for more long range. \n",
        "lr = 0.001\n",
        "n_hidden = 512\n",
        "n_layers = 2\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNDA-1xbKyTQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2b3ef080-11be-4858-aab0-cfe827f3c414"
      },
      "source": [
        "# train model \n",
        "net = charLSTM(chars, n_hidden,n_layers,drop_prob,lr)\n",
        "print(net)\n",
        "print(\"=====================================\")\n",
        "print(\"============== TRAINING =============\")\n",
        "print(\"======================================\")\n",
        "train(net,encoded, epochs = n_epochs, batch_size = batch_size, seq_length= seq_length, lr=lr , clips =clips, val_fraction = val_fraction, print_every = print_every)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "charLSTM(\n",
            "  (lstm): LSTM(83, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (fc): Linear(in_features=512, out_features=83, bias=True)\n",
            ")\n",
            "=====================================\n",
            "============== TRAINING =============\n",
            "======================================\n",
            "=========== New Epoch ===========\n",
            "Epoch: 1/20... steps: 10... Loss: 3.2635 Val loss: 3.2081\n",
            "Epoch: 1/20... steps: 20... Loss: 3.1542 Val loss: 3.1374\n",
            "Epoch: 1/20... steps: 30... Loss: 3.1478 Val loss: 3.1249\n",
            "Epoch: 1/20... steps: 40... Loss: 3.1177 Val loss: 3.1200\n",
            "Epoch: 1/20... steps: 50... Loss: 3.1425 Val loss: 3.1179\n",
            "Epoch: 1/20... steps: 60... Loss: 3.1188 Val loss: 3.1152\n",
            "Epoch: 1/20... steps: 70... Loss: 3.1090 Val loss: 3.1133\n",
            "Epoch: 1/20... steps: 80... Loss: 3.1205 Val loss: 3.1081\n",
            "Epoch: 1/20... steps: 90... Loss: 3.1116 Val loss: 3.0954\n",
            "Epoch: 1/20... steps: 100... Loss: 3.0780 Val loss: 3.0655\n",
            "Epoch: 1/20... steps: 110... Loss: 3.0218 Val loss: 3.0020\n",
            "Epoch: 1/20... steps: 120... Loss: 2.9289 Val loss: 2.9729\n",
            "Epoch: 1/20... steps: 130... Loss: 2.8562 Val loss: 2.8195\n",
            "=========== New Epoch ===========\n",
            "Epoch: 2/20... steps: 140... Loss: 2.7447 Val loss: 2.6858\n",
            "Epoch: 2/20... steps: 150... Loss: 2.6556 Val loss: 2.5913\n",
            "Epoch: 2/20... steps: 160... Loss: 2.5818 Val loss: 2.5368\n",
            "Epoch: 2/20... steps: 170... Loss: 2.5121 Val loss: 2.4941\n",
            "Epoch: 2/20... steps: 180... Loss: 2.4799 Val loss: 2.4590\n",
            "Epoch: 2/20... steps: 190... Loss: 2.4309 Val loss: 2.4240\n",
            "Epoch: 2/20... steps: 200... Loss: 2.4294 Val loss: 2.4106\n",
            "Epoch: 2/20... steps: 210... Loss: 2.3954 Val loss: 2.3746\n",
            "Epoch: 2/20... steps: 220... Loss: 2.3663 Val loss: 2.3405\n",
            "Epoch: 2/20... steps: 230... Loss: 2.3436 Val loss: 2.3170\n",
            "Epoch: 2/20... steps: 240... Loss: 2.3272 Val loss: 2.2922\n",
            "Epoch: 2/20... steps: 250... Loss: 2.2653 Val loss: 2.2675\n",
            "Epoch: 2/20... steps: 260... Loss: 2.2520 Val loss: 2.2416\n",
            "Epoch: 2/20... steps: 270... Loss: 2.2524 Val loss: 2.2220\n",
            "=========== New Epoch ===========\n",
            "Epoch: 3/20... steps: 280... Loss: 2.2476 Val loss: 2.2010\n",
            "Epoch: 3/20... steps: 290... Loss: 2.2073 Val loss: 2.1746\n",
            "Epoch: 3/20... steps: 300... Loss: 2.1848 Val loss: 2.1496\n",
            "Epoch: 3/20... steps: 310... Loss: 2.1567 Val loss: 2.1302\n",
            "Epoch: 3/20... steps: 320... Loss: 2.1333 Val loss: 2.1101\n",
            "Epoch: 3/20... steps: 330... Loss: 2.1004 Val loss: 2.0970\n",
            "Epoch: 3/20... steps: 340... Loss: 2.1137 Val loss: 2.0822\n",
            "Epoch: 3/20... steps: 350... Loss: 2.1007 Val loss: 2.0530\n",
            "Epoch: 3/20... steps: 360... Loss: 2.0342 Val loss: 2.0359\n",
            "Epoch: 3/20... steps: 370... Loss: 2.0512 Val loss: 2.0207\n",
            "Epoch: 3/20... steps: 380... Loss: 2.0224 Val loss: 1.9994\n",
            "Epoch: 3/20... steps: 390... Loss: 2.0066 Val loss: 1.9845\n",
            "Epoch: 3/20... steps: 400... Loss: 1.9719 Val loss: 1.9683\n",
            "Epoch: 3/20... steps: 410... Loss: 1.9906 Val loss: 1.9539\n",
            "=========== New Epoch ===========\n",
            "Epoch: 4/20... steps: 420... Loss: 1.9644 Val loss: 1.9422\n",
            "Epoch: 4/20... steps: 430... Loss: 1.9585 Val loss: 1.9219\n",
            "Epoch: 4/20... steps: 440... Loss: 1.9422 Val loss: 1.9123\n",
            "Epoch: 4/20... steps: 450... Loss: 1.8820 Val loss: 1.8953\n",
            "Epoch: 4/20... steps: 460... Loss: 1.8758 Val loss: 1.8865\n",
            "Epoch: 4/20... steps: 470... Loss: 1.9059 Val loss: 1.8762\n",
            "Epoch: 4/20... steps: 480... Loss: 1.8866 Val loss: 1.8615\n",
            "Epoch: 4/20... steps: 490... Loss: 1.8879 Val loss: 1.8499\n",
            "Epoch: 4/20... steps: 500... Loss: 1.8880 Val loss: 1.8399\n",
            "Epoch: 4/20... steps: 510... Loss: 1.8583 Val loss: 1.8253\n",
            "Epoch: 4/20... steps: 520... Loss: 1.8689 Val loss: 1.8160\n",
            "Epoch: 4/20... steps: 530... Loss: 1.8280 Val loss: 1.8043\n",
            "Epoch: 4/20... steps: 540... Loss: 1.8016 Val loss: 1.7936\n",
            "Epoch: 4/20... steps: 550... Loss: 1.8302 Val loss: 1.7824\n",
            "=========== New Epoch ===========\n",
            "Epoch: 5/20... steps: 560... Loss: 1.7988 Val loss: 1.7712\n",
            "Epoch: 5/20... steps: 570... Loss: 1.7854 Val loss: 1.7624\n",
            "Epoch: 5/20... steps: 580... Loss: 1.7574 Val loss: 1.7543\n",
            "Epoch: 5/20... steps: 590... Loss: 1.7593 Val loss: 1.7438\n",
            "Epoch: 5/20... steps: 600... Loss: 1.7476 Val loss: 1.7400\n",
            "Epoch: 5/20... steps: 610... Loss: 1.7427 Val loss: 1.7324\n",
            "Epoch: 5/20... steps: 620... Loss: 1.7507 Val loss: 1.7234\n",
            "Epoch: 5/20... steps: 630... Loss: 1.7596 Val loss: 1.7145\n",
            "Epoch: 5/20... steps: 640... Loss: 1.7290 Val loss: 1.7048\n",
            "Epoch: 5/20... steps: 650... Loss: 1.7138 Val loss: 1.7015\n",
            "Epoch: 5/20... steps: 660... Loss: 1.6806 Val loss: 1.6936\n",
            "Epoch: 5/20... steps: 670... Loss: 1.7028 Val loss: 1.6885\n",
            "Epoch: 5/20... steps: 680... Loss: 1.7100 Val loss: 1.6774\n",
            "Epoch: 5/20... steps: 690... Loss: 1.6851 Val loss: 1.6730\n",
            "=========== New Epoch ===========\n",
            "Epoch: 6/20... steps: 700... Loss: 1.6782 Val loss: 1.6615\n",
            "Epoch: 6/20... steps: 710... Loss: 1.6760 Val loss: 1.6590\n",
            "Epoch: 6/20... steps: 720... Loss: 1.6535 Val loss: 1.6509\n",
            "Epoch: 6/20... steps: 730... Loss: 1.6759 Val loss: 1.6428\n",
            "Epoch: 6/20... steps: 740... Loss: 1.6433 Val loss: 1.6395\n",
            "Epoch: 6/20... steps: 750... Loss: 1.6190 Val loss: 1.6344\n",
            "Epoch: 6/20... steps: 760... Loss: 1.6551 Val loss: 1.6285\n",
            "Epoch: 6/20... steps: 770... Loss: 1.6395 Val loss: 1.6209\n",
            "Epoch: 6/20... steps: 780... Loss: 1.6204 Val loss: 1.6157\n",
            "Epoch: 6/20... steps: 790... Loss: 1.6076 Val loss: 1.6135\n",
            "Epoch: 6/20... steps: 800... Loss: 1.6201 Val loss: 1.6085\n",
            "Epoch: 6/20... steps: 810... Loss: 1.6129 Val loss: 1.6023\n",
            "Epoch: 6/20... steps: 820... Loss: 1.5765 Val loss: 1.5943\n",
            "Epoch: 6/20... steps: 830... Loss: 1.6177 Val loss: 1.5875\n",
            "=========== New Epoch ===========\n",
            "Epoch: 7/20... steps: 840... Loss: 1.5782 Val loss: 1.5863\n",
            "Epoch: 7/20... steps: 850... Loss: 1.5963 Val loss: 1.5821\n",
            "Epoch: 7/20... steps: 860... Loss: 1.5806 Val loss: 1.5736\n",
            "Epoch: 7/20... steps: 870... Loss: 1.5839 Val loss: 1.5706\n",
            "Epoch: 7/20... steps: 880... Loss: 1.5822 Val loss: 1.5657\n",
            "Epoch: 7/20... steps: 890... Loss: 1.5813 Val loss: 1.5608\n",
            "Epoch: 7/20... steps: 900... Loss: 1.5566 Val loss: 1.5580\n",
            "Epoch: 7/20... steps: 910... Loss: 1.5341 Val loss: 1.5568\n",
            "Epoch: 7/20... steps: 920... Loss: 1.5608 Val loss: 1.5516\n",
            "Epoch: 7/20... steps: 930... Loss: 1.5339 Val loss: 1.5465\n",
            "Epoch: 7/20... steps: 940... Loss: 1.5435 Val loss: 1.5438\n",
            "Epoch: 7/20... steps: 950... Loss: 1.5561 Val loss: 1.5359\n",
            "Epoch: 7/20... steps: 960... Loss: 1.5514 Val loss: 1.5328\n",
            "Epoch: 7/20... steps: 970... Loss: 1.5575 Val loss: 1.5289\n",
            "=========== New Epoch ===========\n",
            "Epoch: 8/20... steps: 980... Loss: 1.5326 Val loss: 1.5248\n",
            "Epoch: 8/20... steps: 990... Loss: 1.5250 Val loss: 1.5211\n",
            "Epoch: 8/20... steps: 1000... Loss: 1.5206 Val loss: 1.5192\n",
            "Epoch: 8/20... steps: 1010... Loss: 1.5476 Val loss: 1.5136\n",
            "Epoch: 8/20... steps: 1020... Loss: 1.5280 Val loss: 1.5111\n",
            "Epoch: 8/20... steps: 1030... Loss: 1.5109 Val loss: 1.5074\n",
            "Epoch: 8/20... steps: 1040... Loss: 1.5181 Val loss: 1.5062\n",
            "Epoch: 8/20... steps: 1050... Loss: 1.5004 Val loss: 1.5018\n",
            "Epoch: 8/20... steps: 1060... Loss: 1.5070 Val loss: 1.5016\n",
            "Epoch: 8/20... steps: 1070... Loss: 1.5111 Val loss: 1.4955\n",
            "Epoch: 8/20... steps: 1080... Loss: 1.4994 Val loss: 1.4964\n",
            "Epoch: 8/20... steps: 1090... Loss: 1.4861 Val loss: 1.4886\n",
            "Epoch: 8/20... steps: 1100... Loss: 1.4809 Val loss: 1.4841\n",
            "Epoch: 8/20... steps: 1110... Loss: 1.4862 Val loss: 1.4850\n",
            "=========== New Epoch ===========\n",
            "Epoch: 9/20... steps: 1120... Loss: 1.4960 Val loss: 1.4836\n",
            "Epoch: 9/20... steps: 1130... Loss: 1.4860 Val loss: 1.4738\n",
            "Epoch: 9/20... steps: 1140... Loss: 1.4890 Val loss: 1.4716\n",
            "Epoch: 9/20... steps: 1150... Loss: 1.5046 Val loss: 1.4681\n",
            "Epoch: 9/20... steps: 1160... Loss: 1.4585 Val loss: 1.4700\n",
            "Epoch: 9/20... steps: 1170... Loss: 1.4741 Val loss: 1.4670\n",
            "Epoch: 9/20... steps: 1180... Loss: 1.4579 Val loss: 1.4605\n",
            "Epoch: 9/20... steps: 1190... Loss: 1.4904 Val loss: 1.4589\n",
            "Epoch: 9/20... steps: 1200... Loss: 1.4395 Val loss: 1.4570\n",
            "Epoch: 9/20... steps: 1210... Loss: 1.4454 Val loss: 1.4567\n",
            "Epoch: 9/20... steps: 1220... Loss: 1.4484 Val loss: 1.4545\n",
            "Epoch: 9/20... steps: 1230... Loss: 1.4361 Val loss: 1.4527\n",
            "Epoch: 9/20... steps: 1240... Loss: 1.4381 Val loss: 1.4497\n",
            "Epoch: 9/20... steps: 1250... Loss: 1.4475 Val loss: 1.4475\n",
            "=========== New Epoch ===========\n",
            "Epoch: 10/20... steps: 1260... Loss: 1.4469 Val loss: 1.4472\n",
            "Epoch: 10/20... steps: 1270... Loss: 1.4471 Val loss: 1.4419\n",
            "Epoch: 10/20... steps: 1280... Loss: 1.4580 Val loss: 1.4390\n",
            "Epoch: 10/20... steps: 1290... Loss: 1.4420 Val loss: 1.4395\n",
            "Epoch: 10/20... steps: 1300... Loss: 1.4445 Val loss: 1.4377\n",
            "Epoch: 10/20... steps: 1310... Loss: 1.4463 Val loss: 1.4329\n",
            "Epoch: 10/20... steps: 1320... Loss: 1.4090 Val loss: 1.4325\n",
            "Epoch: 10/20... steps: 1330... Loss: 1.4141 Val loss: 1.4306\n",
            "Epoch: 10/20... steps: 1340... Loss: 1.4008 Val loss: 1.4262\n",
            "Epoch: 10/20... steps: 1350... Loss: 1.3953 Val loss: 1.4243\n",
            "Epoch: 10/20... steps: 1360... Loss: 1.4054 Val loss: 1.4256\n",
            "Epoch: 10/20... steps: 1370... Loss: 1.3925 Val loss: 1.4180\n",
            "Epoch: 10/20... steps: 1380... Loss: 1.4310 Val loss: 1.4164\n",
            "Epoch: 10/20... steps: 1390... Loss: 1.4429 Val loss: 1.4162\n",
            "=========== New Epoch ===========\n",
            "Epoch: 11/20... steps: 1400... Loss: 1.4333 Val loss: 1.4177\n",
            "Epoch: 11/20... steps: 1410... Loss: 1.4475 Val loss: 1.4098\n",
            "Epoch: 11/20... steps: 1420... Loss: 1.4397 Val loss: 1.4063\n",
            "Epoch: 11/20... steps: 1430... Loss: 1.3948 Val loss: 1.4116\n",
            "Epoch: 11/20... steps: 1440... Loss: 1.4304 Val loss: 1.4076\n",
            "Epoch: 11/20... steps: 1450... Loss: 1.3575 Val loss: 1.4026\n",
            "Epoch: 11/20... steps: 1460... Loss: 1.3772 Val loss: 1.4040\n",
            "Epoch: 11/20... steps: 1470... Loss: 1.3793 Val loss: 1.4031\n",
            "Epoch: 11/20... steps: 1480... Loss: 1.3927 Val loss: 1.3981\n",
            "Epoch: 11/20... steps: 1490... Loss: 1.3856 Val loss: 1.3984\n",
            "Epoch: 11/20... steps: 1500... Loss: 1.3766 Val loss: 1.3967\n",
            "Epoch: 11/20... steps: 1510... Loss: 1.3555 Val loss: 1.3945\n",
            "Epoch: 11/20... steps: 1520... Loss: 1.3927 Val loss: 1.3908\n",
            "=========== New Epoch ===========\n",
            "Epoch: 12/20... steps: 1530... Loss: 1.4457 Val loss: 1.3960\n",
            "Epoch: 12/20... steps: 1540... Loss: 1.3962 Val loss: 1.3949\n",
            "Epoch: 12/20... steps: 1550... Loss: 1.3981 Val loss: 1.3895\n",
            "Epoch: 12/20... steps: 1560... Loss: 1.4136 Val loss: 1.3857\n",
            "Epoch: 12/20... steps: 1570... Loss: 1.3549 Val loss: 1.3915\n",
            "Epoch: 12/20... steps: 1580... Loss: 1.3349 Val loss: 1.3878\n",
            "Epoch: 12/20... steps: 1590... Loss: 1.3297 Val loss: 1.3836\n",
            "Epoch: 12/20... steps: 1600... Loss: 1.3533 Val loss: 1.3848\n",
            "Epoch: 12/20... steps: 1610... Loss: 1.3484 Val loss: 1.3835\n",
            "Epoch: 12/20... steps: 1620... Loss: 1.3511 Val loss: 1.3786\n",
            "Epoch: 12/20... steps: 1630... Loss: 1.3758 Val loss: 1.3770\n",
            "Epoch: 12/20... steps: 1640... Loss: 1.3517 Val loss: 1.3801\n",
            "Epoch: 12/20... steps: 1650... Loss: 1.3280 Val loss: 1.3754\n",
            "Epoch: 12/20... steps: 1660... Loss: 1.3849 Val loss: 1.3727\n",
            "=========== New Epoch ===========\n",
            "Epoch: 13/20... steps: 1670... Loss: 1.3515 Val loss: 1.3827\n",
            "Epoch: 13/20... steps: 1680... Loss: 1.3623 Val loss: 1.3759\n",
            "Epoch: 13/20... steps: 1690... Loss: 1.3364 Val loss: 1.3682\n",
            "Epoch: 13/20... steps: 1700... Loss: 1.3386 Val loss: 1.3702\n",
            "Epoch: 13/20... steps: 1710... Loss: 1.3265 Val loss: 1.3717\n",
            "Epoch: 13/20... steps: 1720... Loss: 1.3224 Val loss: 1.3687\n",
            "Epoch: 13/20... steps: 1730... Loss: 1.3678 Val loss: 1.3653\n",
            "Epoch: 13/20... steps: 1740... Loss: 1.3271 Val loss: 1.3653\n",
            "Epoch: 13/20... steps: 1750... Loss: 1.3036 Val loss: 1.3643\n",
            "Epoch: 13/20... steps: 1760... Loss: 1.3308 Val loss: 1.3602\n",
            "Epoch: 13/20... steps: 1770... Loss: 1.3497 Val loss: 1.3613\n",
            "Epoch: 13/20... steps: 1780... Loss: 1.3219 Val loss: 1.3586\n",
            "Epoch: 13/20... steps: 1790... Loss: 1.3171 Val loss: 1.3599\n",
            "Epoch: 13/20... steps: 1800... Loss: 1.3304 Val loss: 1.3561\n",
            "=========== New Epoch ===========\n",
            "Epoch: 14/20... steps: 1810... Loss: 1.3423 Val loss: 1.3680\n",
            "Epoch: 14/20... steps: 1820... Loss: 1.3285 Val loss: 1.3576\n",
            "Epoch: 14/20... steps: 1830... Loss: 1.3456 Val loss: 1.3524\n",
            "Epoch: 14/20... steps: 1840... Loss: 1.2942 Val loss: 1.3566\n",
            "Epoch: 14/20... steps: 1850... Loss: 1.2775 Val loss: 1.3554\n",
            "Epoch: 14/20... steps: 1860... Loss: 1.3393 Val loss: 1.3527\n",
            "Epoch: 14/20... steps: 1870... Loss: 1.3377 Val loss: 1.3508\n",
            "Epoch: 14/20... steps: 1880... Loss: 1.3284 Val loss: 1.3547\n",
            "Epoch: 14/20... steps: 1890... Loss: 1.3455 Val loss: 1.3533\n",
            "Epoch: 14/20... steps: 1900... Loss: 1.3276 Val loss: 1.3464\n",
            "Epoch: 14/20... steps: 1910... Loss: 1.3241 Val loss: 1.3481\n",
            "Epoch: 14/20... steps: 1920... Loss: 1.3136 Val loss: 1.3472\n",
            "Epoch: 14/20... steps: 1930... Loss: 1.2816 Val loss: 1.3483\n",
            "Epoch: 14/20... steps: 1940... Loss: 1.3344 Val loss: 1.3431\n",
            "=========== New Epoch ===========\n",
            "Epoch: 15/20... steps: 1950... Loss: 1.3094 Val loss: 1.3517\n",
            "Epoch: 15/20... steps: 1960... Loss: 1.3070 Val loss: 1.3449\n",
            "Epoch: 15/20... steps: 1970... Loss: 1.3021 Val loss: 1.3374\n",
            "Epoch: 15/20... steps: 1980... Loss: 1.3028 Val loss: 1.3422\n",
            "Epoch: 15/20... steps: 1990... Loss: 1.2921 Val loss: 1.3435\n",
            "Epoch: 15/20... steps: 2000... Loss: 1.2804 Val loss: 1.3398\n",
            "Epoch: 15/20... steps: 2010... Loss: 1.2999 Val loss: 1.3361\n",
            "Epoch: 15/20... steps: 2020... Loss: 1.3173 Val loss: 1.3377\n",
            "Epoch: 15/20... steps: 2030... Loss: 1.2866 Val loss: 1.3407\n",
            "Epoch: 15/20... steps: 2040... Loss: 1.3045 Val loss: 1.3332\n",
            "Epoch: 15/20... steps: 2050... Loss: 1.2853 Val loss: 1.3341\n",
            "Epoch: 15/20... steps: 2060... Loss: 1.3005 Val loss: 1.3345\n",
            "Epoch: 15/20... steps: 2070... Loss: 1.3026 Val loss: 1.3326\n",
            "Epoch: 15/20... steps: 2080... Loss: 1.2999 Val loss: 1.3299\n",
            "=========== New Epoch ===========\n",
            "Epoch: 16/20... steps: 2090... Loss: 1.3054 Val loss: 1.3378\n",
            "Epoch: 16/20... steps: 2100... Loss: 1.2806 Val loss: 1.3331\n",
            "Epoch: 16/20... steps: 2110... Loss: 1.2840 Val loss: 1.3277\n",
            "Epoch: 16/20... steps: 2120... Loss: 1.2965 Val loss: 1.3292\n",
            "Epoch: 16/20... steps: 2130... Loss: 1.2623 Val loss: 1.3279\n",
            "Epoch: 16/20... steps: 2140... Loss: 1.2673 Val loss: 1.3232\n",
            "Epoch: 16/20... steps: 2150... Loss: 1.3019 Val loss: 1.3256\n",
            "Epoch: 16/20... steps: 2160... Loss: 1.2787 Val loss: 1.3239\n",
            "Epoch: 16/20... steps: 2170... Loss: 1.2799 Val loss: 1.3240\n",
            "Epoch: 16/20... steps: 2180... Loss: 1.2706 Val loss: 1.3234\n",
            "Epoch: 16/20... steps: 2190... Loss: 1.2969 Val loss: 1.3208\n",
            "Epoch: 16/20... steps: 2200... Loss: 1.2734 Val loss: 1.3236\n",
            "Epoch: 16/20... steps: 2210... Loss: 1.2366 Val loss: 1.3243\n",
            "Epoch: 16/20... steps: 2220... Loss: 1.2855 Val loss: 1.3185\n",
            "=========== New Epoch ===========\n",
            "Epoch: 17/20... steps: 2230... Loss: 1.2562 Val loss: 1.3259\n",
            "Epoch: 17/20... steps: 2240... Loss: 1.2704 Val loss: 1.3229\n",
            "Epoch: 17/20... steps: 2250... Loss: 1.2583 Val loss: 1.3147\n",
            "Epoch: 17/20... steps: 2260... Loss: 1.2661 Val loss: 1.3214\n",
            "Epoch: 17/20... steps: 2270... Loss: 1.2689 Val loss: 1.3196\n",
            "Epoch: 17/20... steps: 2280... Loss: 1.2758 Val loss: 1.3149\n",
            "Epoch: 17/20... steps: 2290... Loss: 1.2784 Val loss: 1.3137\n",
            "Epoch: 17/20... steps: 2300... Loss: 1.2440 Val loss: 1.3154\n",
            "Epoch: 17/20... steps: 2310... Loss: 1.2634 Val loss: 1.3128\n",
            "Epoch: 17/20... steps: 2320... Loss: 1.2650 Val loss: 1.3136\n",
            "Epoch: 17/20... steps: 2330... Loss: 1.2548 Val loss: 1.3158\n",
            "Epoch: 17/20... steps: 2340... Loss: 1.2732 Val loss: 1.3141\n",
            "Epoch: 17/20... steps: 2350... Loss: 1.2621 Val loss: 1.3139\n",
            "Epoch: 17/20... steps: 2360... Loss: 1.2844 Val loss: 1.3136\n",
            "=========== New Epoch ===========\n",
            "Epoch: 18/20... steps: 2370... Loss: 1.2498 Val loss: 1.3198\n",
            "Epoch: 18/20... steps: 2380... Loss: 1.2586 Val loss: 1.3159\n",
            "Epoch: 18/20... steps: 2390... Loss: 1.2524 Val loss: 1.3073\n",
            "Epoch: 18/20... steps: 2400... Loss: 1.2721 Val loss: 1.3096\n",
            "Epoch: 18/20... steps: 2410... Loss: 1.2737 Val loss: 1.3076\n",
            "Epoch: 18/20... steps: 2420... Loss: 1.2511 Val loss: 1.3065\n",
            "Epoch: 18/20... steps: 2430... Loss: 1.2679 Val loss: 1.3093\n",
            "Epoch: 18/20... steps: 2440... Loss: 1.2430 Val loss: 1.3054\n",
            "Epoch: 18/20... steps: 2450... Loss: 1.2448 Val loss: 1.3059\n",
            "Epoch: 18/20... steps: 2460... Loss: 1.2621 Val loss: 1.3083\n",
            "Epoch: 18/20... steps: 2470... Loss: 1.2478 Val loss: 1.3072\n",
            "Epoch: 18/20... steps: 2480... Loss: 1.2424 Val loss: 1.3117\n",
            "Epoch: 18/20... steps: 2490... Loss: 1.2295 Val loss: 1.3056\n",
            "Epoch: 18/20... steps: 2500... Loss: 1.2360 Val loss: 1.3075\n",
            "=========== New Epoch ===========\n",
            "Epoch: 19/20... steps: 2510... Loss: 1.2463 Val loss: 1.3071\n",
            "Epoch: 19/20... steps: 2520... Loss: 1.2632 Val loss: 1.3068\n",
            "Epoch: 19/20... steps: 2530... Loss: 1.2593 Val loss: 1.2984\n",
            "Epoch: 19/20... steps: 2540... Loss: 1.2707 Val loss: 1.3028\n",
            "Epoch: 19/20... steps: 2550... Loss: 1.2402 Val loss: 1.3028\n",
            "Epoch: 19/20... steps: 2560... Loss: 1.2472 Val loss: 1.2995\n",
            "Epoch: 19/20... steps: 2570... Loss: 1.2384 Val loss: 1.3012\n",
            "Epoch: 19/20... steps: 2580... Loss: 1.2719 Val loss: 1.3006\n",
            "Epoch: 19/20... steps: 2590... Loss: 1.2316 Val loss: 1.3002\n",
            "Epoch: 19/20... steps: 2600... Loss: 1.2244 Val loss: 1.2990\n",
            "Epoch: 19/20... steps: 2610... Loss: 1.2433 Val loss: 1.3000\n",
            "Epoch: 19/20... steps: 2620... Loss: 1.2141 Val loss: 1.3015\n",
            "Epoch: 19/20... steps: 2630... Loss: 1.2241 Val loss: 1.3000\n",
            "Epoch: 19/20... steps: 2640... Loss: 1.2369 Val loss: 1.2947\n",
            "=========== New Epoch ===========\n",
            "Epoch: 20/20... steps: 2650... Loss: 1.2454 Val loss: 1.2994\n",
            "Epoch: 20/20... steps: 2660... Loss: 1.2376 Val loss: 1.3025\n",
            "Epoch: 20/20... steps: 2670... Loss: 1.2572 Val loss: 1.2905\n",
            "Epoch: 20/20... steps: 2680... Loss: 1.2444 Val loss: 1.2954\n",
            "Epoch: 20/20... steps: 2690... Loss: 1.2348 Val loss: 1.2964\n",
            "Epoch: 20/20... steps: 2700... Loss: 1.2396 Val loss: 1.2964\n",
            "Epoch: 20/20... steps: 2710... Loss: 1.2155 Val loss: 1.2954\n",
            "Epoch: 20/20... steps: 2720... Loss: 1.2161 Val loss: 1.2917\n",
            "Epoch: 20/20... steps: 2730... Loss: 1.2086 Val loss: 1.2896\n",
            "Epoch: 20/20... steps: 2740... Loss: 1.2110 Val loss: 1.2959\n",
            "Epoch: 20/20... steps: 2750... Loss: 1.2212 Val loss: 1.2898\n",
            "Epoch: 20/20... steps: 2760... Loss: 1.1997 Val loss: 1.2933\n",
            "Epoch: 20/20... steps: 2770... Loss: 1.2455 Val loss: 1.2927\n",
            "Epoch: 20/20... steps: 2780... Loss: 1.2720 Val loss: 1.2895\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALBIgNb1lS5D",
        "colab_type": "text"
      },
      "source": [
        "## Result Conclusion and Model Improvement Analysis\n",
        "\n",
        "For winning model according to my reading, \n",
        "there are two main scenario.\n",
        "1. If your training loss is lower and validation loss is quiet highter then model will be overfiting. The probable solutions are ...\n",
        "- - use shallow network (I mean... less larger than current)\n",
        "- - increase dropout between (0,1)\n",
        "- - increase data for generalize model\n",
        "\n",
        "2. The other scenario is your training and validation loss will be same. so, it means you have underfit model.\n",
        "\n",
        "- - larger your network\n",
        "- - increase your data\n",
        "\n",
        "Some Real Examples: \n",
        "- 1 MB text files = 1 Million Parameters (approx)\n",
        "- 100 MB data and RNN trained with 150K parameters, means you have 0.15 Million parameters for 1 M data. So, obviousely you need more larger network, so incrase network size will be better. \n",
        "- Increase network size means more computing time and need computing power. So, use dropout = 0.5 will boost you network by 2X. \n",
        "\n",
        "---\n",
        "\n",
        "According to best ML practice...\n",
        "* Keep track of your models, and thats why always save your model as checkpoints. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJL5YdK_x6vZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# use best practice\n",
        "model_name = 'rnn_20_epoch.net'\n",
        "\n",
        "checkpoint = {'n_hidden_size' : net.n_hidden_size,\n",
        "              'n_layers' : net.n_layers,\n",
        "              'state_dict' : net.state_dict(),\n",
        "              'tokens' : net.chars}\n",
        "\n",
        "with open(model_name, 'wb') as f:\n",
        "  torch.save(checkpoint, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEmeYTT-b9Sl",
        "colab_type": "text"
      },
      "source": [
        "## Prediction \n",
        "\n",
        "Model is trained, so it will give us information about the next probable character. Ofcause the loss for model is large. Thats why it can not give use best performace. But, still can generate randomsome text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tS7L_JI0XpEN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(net, char, h = None, top_k = None):\n",
        "  x = np.array([[net.char2int[char]]])\n",
        "  x = one_hot_encode(x, len(net.chars))\n",
        "  inputs = torch.from_numpy(x)\n",
        "\n",
        "  if(train_on_gpu):\n",
        "    inputs = inputs.cuda()\n",
        "  \n",
        "  # detach hidden state \n",
        "  h = tuple([each.data for each in h])\n",
        "  output,h = net(inputs,h)\n",
        "\n",
        "  p = F.softmax(output,dim=1).data\n",
        "  if(train_on_gpu):\n",
        "    p = p.cpu()                                 # probability is output so we need to save on cpu, or local instance\n",
        "\n",
        "    if top_k is None:\n",
        "      top_ch = np.arrange(len(net.chars))\n",
        "    else: \n",
        "      p,top_ch = p.topk(top_k)\n",
        "      top_ch = top_ch.numpy().squeeze()\n",
        "\n",
        "      # select the likely character form achived probability\n",
        "      p = p.numpy().squeeze()\n",
        "      char = np.random.choice(top_ch, p = p/p.sum())\n",
        "\n",
        "      return net.int2char[char],h"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6A1z9ccelXt",
        "colab_type": "text"
      },
      "source": [
        "## Priming and generating text\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WlqaNB-WebM9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "35c32901-39fa-4782-b993-fec4d721a0f5"
      },
      "source": [
        "print(net)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "charLSTM(\n",
            "  (lstm): LSTM(83, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (fc): Linear(in_features=512, out_features=83, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6RDV8CgolxR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample(net, size, prime = 'The', top_k = None):\n",
        "  \n",
        "  if(train_on_gpu):\n",
        "    net.cuda()\n",
        "  else:\n",
        "    net.cpu()\n",
        "\n",
        "  net.eval()\n",
        "\n",
        "  chars = [ch for ch in prime]\n",
        "  h = net.init_hidden(1)\n",
        "  for ch in prime:\n",
        "    char, h = predict(net,ch,h,top_k= top_k)\n",
        "  chars.append(char)\n",
        "\n",
        "  # pass previous character to generate next one\n",
        "  for i in range(size):\n",
        "    char, h = predict(net,chars[-1], h, top_k = top_k)\n",
        "    chars.append(char)\n",
        "  return ''.join(chars)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pii6sCUgr9Cu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "outputId": "acb86831-def0-4037-ade1-ae74081896af"
      },
      "source": [
        "print(sample(net,1000,prime= 'Machine Learning',top_k= 5))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Machine Learning and the pate with his caparly and companion of the death.\n",
            "\n",
            "\"Ah! have you soon!\" he answered to the carriage with a smile and taking his hand.\n",
            "\n",
            "The doors with start of the measing on the course of some subjock thought in society, and happy ordinarily he had not a children sorry\n",
            "of the breins to the carriage, at the strange was not the\n",
            "father, and treed another shrupked and heart and straight about him. But alone without when he had\n",
            "come. All the condition of the\n",
            "painter there and\n",
            "all the people and her hand was an interesting of the man, who came to see her face where was a minute all one on her arrangements, and asked her\n",
            "husband's hungress,\n",
            "with the church. And had chertiously did, he flowned his sole and terror of the princess who had supposed, he could not himself for me. And well, they\n",
            "said steping too, as they were\n",
            "calling his his housible from\n",
            "something, that he had seen him. As he went in to the performable station that they\n",
            "seemed so as to say a suffering. The stable was serene\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxLySGFFtFx5",
        "colab_type": "text"
      },
      "source": [
        "## Loading a checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DvTRkUfVsFBw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9839816a-e019-4f8b-c8bd-d9b3d21ab1ca"
      },
      "source": [
        "with open('rnn_20_epoch.net','rb') as f:\n",
        "  checkpoint = torch.load(f)\n",
        "\n",
        "loaded = charLSTM(checkpoint['tokens'],n_hidden_size=checkpoint['n_hidden_size'],n_layers = checkpoint['n_layers'],drop_prob=drop_prob,lr=lr)\n",
        "loaded.load_state_dict(checkpoint['state_dict'])"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVafwYGVwumQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 734
        },
        "outputId": "3eb568a4-57d4-42d0-faca-00cb2acfb9a8"
      },
      "source": [
        "# another sample for loaded model\n",
        "print(sample(loaded, 2000, top_k=5, prime= \"He Said\"))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "He Said.\n",
            "\n",
            "\"I shall be the cheeks at once to\n",
            "be a confusion.\"\n",
            "\n",
            "\"I am away one of this tone, and that was a man who was not in an insten in him that the\n",
            "conversation of it as he came to be as a characteristic attitude of that\n",
            "three there then was not\n",
            "seeing\n",
            "the messed. It all the\n",
            "considerations in a corract, which all sense. She saw that the point there was in the face.\n",
            "\n",
            "\"I would have to said that?\" he answered.\n",
            "\n",
            "\"I won't arrange\n",
            "your fate.\"\n",
            "\n",
            "Alexey Alexandrovitch, with the same correction. And\n",
            "walked out of the crack of the day of the door that were close a mother in his hind that was said at the\n",
            "matter, where a lad coleness, who were all\n",
            "to the proportion with which he\n",
            "had been taken out of the right sense of holsing. But he\n",
            "was always before the men seemed as as to\n",
            "the music and at all, the coupt in his honsess of her. And he could not have been a secret, who would\n",
            "be treated to his sone in the second candle. She came upon a peasants.\n",
            "\n",
            "\"Ah, your humilatting. I am not so introducing the more simple,\" said Stepan Arkadyevitch.\n",
            "\n",
            "\"Well, an whole and thinks that I could not be already in the firm of the managenerty thought about it.\n",
            "\n",
            "\"All the carriage is so all about how trative to you?\" said Alexey Alexandrovitch.\n",
            "\n",
            "\"Well, what wishes as you aming to be at the time,\" said Anna, and took a little tears of the crublly smile, she finished her hind the\n",
            "feeling of her head.\n",
            "\n",
            "\"Why,\" he said to herself, but he had been drawing up, and\n",
            "with the plays as had been taken to saying the same charged hand of substinaties.\n",
            "\n",
            "The directions was too, which had been answering was she troubled the porition. She was silent. He felt that he was carryed, and was silent for tome with some officer of the court of her still. But she had to spoke that there were the country was a minute. She would have\n",
            "so long said, shall have but helped at the provinces. He went out of his strange feeling with an achion of this time and serene hand were struck the\n",
            "marshle, who had not called all the drawing round of her\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFnGKAEixl--",
        "colab_type": "text"
      },
      "source": [
        "## Try to improve network with more iterations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nc3IbI6rxavV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define parameters\n",
        "print_every = 10\n",
        "clips = 5\n",
        "val_fraction = 0.1\n",
        "n_epochs = 100\n",
        "drop_prob = 0.5\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 128         # number of sequences run through the network in one pass\n",
        "seq_length = 500         # number of character in the squence, larger is better, network can learn for more long range. \n",
        "lr = 0.001\n",
        "n_hidden = 512\n",
        "n_layers = 2\n"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZqdLTVoyOkc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2c84c98f-2c34-4f47-8ddf-9aefb963c6e9"
      },
      "source": [
        "# train model \n",
        "net = charLSTM(chars, n_hidden,n_layers,drop_prob,lr)\n",
        "print(net)\n",
        "print(\"=====================================\")\n",
        "print(\"============== TRAINING =============\")\n",
        "print(\"======================================\")\n",
        "train(net,encoded, epochs = n_epochs, batch_size = batch_size, seq_length= seq_length, lr=lr , clips =clips, val_fraction = val_fraction, print_every = print_every)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "charLSTM(\n",
            "  (lstm): LSTM(83, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (fc): Linear(in_features=512, out_features=83, bias=True)\n",
            ")\n",
            "=====================================\n",
            "============== TRAINING =============\n",
            "======================================\n",
            "=========== New Epoch ===========\n",
            "Epoch: 1/100... steps: 10... Loss: 3.2766 Val loss: 3.2041\n",
            "Epoch: 1/100... steps: 20... Loss: 3.1804 Val loss: 3.1305\n",
            "=========== New Epoch ===========\n",
            "Epoch: 2/100... steps: 30... Loss: 3.1369 Val loss: 3.1208\n",
            "Epoch: 2/100... steps: 40... Loss: 3.1304 Val loss: 3.1189\n",
            "Epoch: 2/100... steps: 50... Loss: 3.1330 Val loss: 3.1176\n",
            "=========== New Epoch ===========\n",
            "Epoch: 3/100... steps: 60... Loss: 3.1113 Val loss: 3.1153\n",
            "Epoch: 3/100... steps: 70... Loss: 3.1086 Val loss: 3.1125\n",
            "Epoch: 3/100... steps: 80... Loss: 3.1113 Val loss: 3.1062\n",
            "=========== New Epoch ===========\n",
            "Epoch: 4/100... steps: 90... Loss: 3.0815 Val loss: 3.0893\n",
            "Epoch: 4/100... steps: 100... Loss: 3.0574 Val loss: 3.0480\n",
            "=========== New Epoch ===========\n",
            "Epoch: 5/100... steps: 110... Loss: 2.9689 Val loss: 2.9682\n",
            "Epoch: 5/100... steps: 120... Loss: 2.8825 Val loss: 2.8833\n",
            "Epoch: 5/100... steps: 130... Loss: 2.8071 Val loss: 2.8066\n",
            "=========== New Epoch ===========\n",
            "Epoch: 6/100... steps: 140... Loss: 2.7181 Val loss: 2.6862\n",
            "Epoch: 6/100... steps: 150... Loss: 2.6195 Val loss: 2.5889\n",
            "Epoch: 6/100... steps: 160... Loss: 2.5574 Val loss: 2.5203\n",
            "=========== New Epoch ===========\n",
            "Epoch: 7/100... steps: 170... Loss: 2.4932 Val loss: 2.4738\n",
            "Epoch: 7/100... steps: 180... Loss: 2.4614 Val loss: 2.4451\n",
            "=========== New Epoch ===========\n",
            "Epoch: 8/100... steps: 190... Loss: 2.4426 Val loss: 2.4008\n",
            "Epoch: 8/100... steps: 200... Loss: 2.3882 Val loss: 2.3734\n",
            "Epoch: 8/100... steps: 210... Loss: 2.3989 Val loss: 2.3441\n",
            "=========== New Epoch ===========\n",
            "Epoch: 9/100... steps: 220... Loss: 2.3642 Val loss: 2.3675\n",
            "Epoch: 9/100... steps: 230... Loss: 2.3310 Val loss: 2.3036\n",
            "Epoch: 9/100... steps: 240... Loss: 2.2853 Val loss: 2.2682\n",
            "=========== New Epoch ===========\n",
            "Epoch: 10/100... steps: 250... Loss: 2.2640 Val loss: 2.2372\n",
            "Epoch: 10/100... steps: 260... Loss: 2.2714 Val loss: 2.2078\n",
            "Epoch: 10/100... steps: 270... Loss: 2.2578 Val loss: 2.1830\n",
            "=========== New Epoch ===========\n",
            "Epoch: 11/100... steps: 280... Loss: 2.1948 Val loss: 2.1618\n",
            "Epoch: 11/100... steps: 290... Loss: 2.1867 Val loss: 2.1406\n",
            "=========== New Epoch ===========\n",
            "Epoch: 12/100... steps: 300... Loss: 2.1306 Val loss: 2.1097\n",
            "Epoch: 12/100... steps: 310... Loss: 2.1170 Val loss: 2.0877\n",
            "Epoch: 12/100... steps: 320... Loss: 2.1129 Val loss: 2.0602\n",
            "=========== New Epoch ===========\n",
            "Epoch: 13/100... steps: 330... Loss: 2.0758 Val loss: 2.0419\n",
            "Epoch: 13/100... steps: 340... Loss: 2.0628 Val loss: 2.0164\n",
            "Epoch: 13/100... steps: 350... Loss: 2.0522 Val loss: 1.9980\n",
            "=========== New Epoch ===========\n",
            "Epoch: 14/100... steps: 360... Loss: 2.0011 Val loss: 1.9746\n",
            "Epoch: 14/100... steps: 370... Loss: 2.0061 Val loss: 1.9550\n",
            "=========== New Epoch ===========\n",
            "Epoch: 15/100... steps: 380... Loss: 1.9651 Val loss: 1.9377\n",
            "Epoch: 15/100... steps: 390... Loss: 1.9488 Val loss: 1.9225\n",
            "Epoch: 15/100... steps: 400... Loss: 1.9595 Val loss: 1.9026\n",
            "=========== New Epoch ===========\n",
            "Epoch: 16/100... steps: 410... Loss: 1.9316 Val loss: 1.8918\n",
            "Epoch: 16/100... steps: 420... Loss: 1.9030 Val loss: 1.8722\n",
            "Epoch: 16/100... steps: 430... Loss: 1.9255 Val loss: 1.8538\n",
            "=========== New Epoch ===========\n",
            "Epoch: 17/100... steps: 440... Loss: 1.8769 Val loss: 1.8382\n",
            "Epoch: 17/100... steps: 450... Loss: 1.8891 Val loss: 1.8256\n",
            "=========== New Epoch ===========\n",
            "Epoch: 18/100... steps: 460... Loss: 1.8577 Val loss: 1.8130\n",
            "Epoch: 18/100... steps: 470... Loss: 1.8252 Val loss: 1.7978\n",
            "Epoch: 18/100... steps: 480... Loss: 1.8559 Val loss: 1.7980\n",
            "=========== New Epoch ===========\n",
            "Epoch: 19/100... steps: 490... Loss: 1.7973 Val loss: 1.7790\n",
            "Epoch: 19/100... steps: 500... Loss: 1.8099 Val loss: 1.7651\n",
            "Epoch: 19/100... steps: 510... Loss: 1.8152 Val loss: 1.7551\n",
            "=========== New Epoch ===========\n",
            "Epoch: 20/100... steps: 520... Loss: 1.7652 Val loss: 1.7419\n",
            "Epoch: 20/100... steps: 530... Loss: 1.7907 Val loss: 1.7286\n",
            "Epoch: 20/100... steps: 540... Loss: 1.8068 Val loss: 1.7214\n",
            "=========== New Epoch ===========\n",
            "Epoch: 21/100... steps: 550... Loss: 1.7479 Val loss: 1.7088\n",
            "Epoch: 21/100... steps: 560... Loss: 1.7536 Val loss: 1.6984\n",
            "=========== New Epoch ===========\n",
            "Epoch: 22/100... steps: 570... Loss: 1.7159 Val loss: 1.6903\n",
            "Epoch: 22/100... steps: 580... Loss: 1.7169 Val loss: 1.6787\n",
            "Epoch: 22/100... steps: 590... Loss: 1.7269 Val loss: 1.6713\n",
            "=========== New Epoch ===========\n",
            "Epoch: 23/100... steps: 600... Loss: 1.6899 Val loss: 1.6624\n",
            "Epoch: 23/100... steps: 610... Loss: 1.6939 Val loss: 1.6531\n",
            "Epoch: 23/100... steps: 620... Loss: 1.7027 Val loss: 1.6447\n",
            "=========== New Epoch ===========\n",
            "Epoch: 24/100... steps: 630... Loss: 1.6605 Val loss: 1.6380\n",
            "Epoch: 24/100... steps: 640... Loss: 1.6781 Val loss: 1.6287\n",
            "=========== New Epoch ===========\n",
            "Epoch: 25/100... steps: 650... Loss: 1.6471 Val loss: 1.6247\n",
            "Epoch: 25/100... steps: 660... Loss: 1.6424 Val loss: 1.6174\n",
            "Epoch: 25/100... steps: 670... Loss: 1.6629 Val loss: 1.6104\n",
            "=========== New Epoch ===========\n",
            "Epoch: 26/100... steps: 680... Loss: 1.6274 Val loss: 1.6003\n",
            "Epoch: 26/100... steps: 690... Loss: 1.6101 Val loss: 1.5942\n",
            "Epoch: 26/100... steps: 700... Loss: 1.6285 Val loss: 1.5874\n",
            "=========== New Epoch ===========\n",
            "Epoch: 27/100... steps: 710... Loss: 1.6007 Val loss: 1.5838\n",
            "Epoch: 27/100... steps: 720... Loss: 1.6338 Val loss: 1.5760\n",
            "=========== New Epoch ===========\n",
            "Epoch: 28/100... steps: 730... Loss: 1.6013 Val loss: 1.5723\n",
            "Epoch: 28/100... steps: 740... Loss: 1.5738 Val loss: 1.5639\n",
            "Epoch: 28/100... steps: 750... Loss: 1.5935 Val loss: 1.5618\n",
            "=========== New Epoch ===========\n",
            "Epoch: 29/100... steps: 760... Loss: 1.5599 Val loss: 1.5501\n",
            "Epoch: 29/100... steps: 770... Loss: 1.5780 Val loss: 1.5480\n",
            "Epoch: 29/100... steps: 780... Loss: 1.5926 Val loss: 1.5437\n",
            "=========== New Epoch ===========\n",
            "Epoch: 30/100... steps: 790... Loss: 1.5442 Val loss: 1.5328\n",
            "Epoch: 30/100... steps: 800... Loss: 1.5812 Val loss: 1.5299\n",
            "Epoch: 30/100... steps: 810... Loss: 1.6028 Val loss: 1.5271\n",
            "=========== New Epoch ===========\n",
            "Epoch: 31/100... steps: 820... Loss: 1.5393 Val loss: 1.5208\n",
            "Epoch: 31/100... steps: 830... Loss: 1.5563 Val loss: 1.5172\n",
            "=========== New Epoch ===========\n",
            "Epoch: 32/100... steps: 840... Loss: 1.5223 Val loss: 1.5113\n",
            "Epoch: 32/100... steps: 850... Loss: 1.5271 Val loss: 1.5081\n",
            "Epoch: 32/100... steps: 860... Loss: 1.5395 Val loss: 1.5048\n",
            "=========== New Epoch ===========\n",
            "Epoch: 33/100... steps: 870... Loss: 1.5113 Val loss: 1.4978\n",
            "Epoch: 33/100... steps: 880... Loss: 1.5163 Val loss: 1.4958\n",
            "Epoch: 33/100... steps: 890... Loss: 1.5299 Val loss: 1.4893\n",
            "=========== New Epoch ===========\n",
            "Epoch: 34/100... steps: 900... Loss: 1.4908 Val loss: 1.4842\n",
            "Epoch: 34/100... steps: 910... Loss: 1.5149 Val loss: 1.4799\n",
            "=========== New Epoch ===========\n",
            "Epoch: 35/100... steps: 920... Loss: 1.4861 Val loss: 1.4775\n",
            "Epoch: 35/100... steps: 930... Loss: 1.4786 Val loss: 1.4750\n",
            "Epoch: 35/100... steps: 940... Loss: 1.5157 Val loss: 1.4698\n",
            "=========== New Epoch ===========\n",
            "Epoch: 36/100... steps: 950... Loss: 1.4749 Val loss: 1.4650\n",
            "Epoch: 36/100... steps: 960... Loss: 1.4616 Val loss: 1.4649\n",
            "Epoch: 36/100... steps: 970... Loss: 1.5055 Val loss: 1.4606\n",
            "=========== New Epoch ===========\n",
            "Epoch: 37/100... steps: 980... Loss: 1.4572 Val loss: 1.4558\n",
            "Epoch: 37/100... steps: 990... Loss: 1.4949 Val loss: 1.4562\n",
            "=========== New Epoch ===========\n",
            "Epoch: 38/100... steps: 1000... Loss: 1.4609 Val loss: 1.4556\n",
            "Epoch: 38/100... steps: 1010... Loss: 1.4390 Val loss: 1.4511\n",
            "Epoch: 38/100... steps: 1020... Loss: 1.4619 Val loss: 1.4442\n",
            "=========== New Epoch ===========\n",
            "Epoch: 39/100... steps: 1030... Loss: 1.4243 Val loss: 1.4408\n",
            "Epoch: 39/100... steps: 1040... Loss: 1.4503 Val loss: 1.4392\n",
            "Epoch: 39/100... steps: 1050... Loss: 1.4707 Val loss: 1.4366\n",
            "=========== New Epoch ===========\n",
            "Epoch: 40/100... steps: 1060... Loss: 1.4245 Val loss: 1.4311\n",
            "Epoch: 40/100... steps: 1070... Loss: 1.4576 Val loss: 1.4289\n",
            "Epoch: 40/100... steps: 1080... Loss: 1.4809 Val loss: 1.4267\n",
            "=========== New Epoch ===========\n",
            "Epoch: 41/100... steps: 1090... Loss: 1.4232 Val loss: 1.4255\n",
            "Epoch: 41/100... steps: 1100... Loss: 1.4399 Val loss: 1.4230\n",
            "=========== New Epoch ===========\n",
            "Epoch: 42/100... steps: 1110... Loss: 1.4089 Val loss: 1.4198\n",
            "Epoch: 42/100... steps: 1120... Loss: 1.4111 Val loss: 1.4184\n",
            "Epoch: 42/100... steps: 1130... Loss: 1.4296 Val loss: 1.4158\n",
            "=========== New Epoch ===========\n",
            "Epoch: 43/100... steps: 1140... Loss: 1.4069 Val loss: 1.4143\n",
            "Epoch: 43/100... steps: 1150... Loss: 1.4114 Val loss: 1.4109\n",
            "Epoch: 43/100... steps: 1160... Loss: 1.4271 Val loss: 1.4103\n",
            "=========== New Epoch ===========\n",
            "Epoch: 44/100... steps: 1170... Loss: 1.3886 Val loss: 1.4084\n",
            "Epoch: 44/100... steps: 1180... Loss: 1.4169 Val loss: 1.4044\n",
            "=========== New Epoch ===========\n",
            "Epoch: 45/100... steps: 1190... Loss: 1.3892 Val loss: 1.4021\n",
            "Epoch: 45/100... steps: 1200... Loss: 1.3813 Val loss: 1.3990\n",
            "Epoch: 45/100... steps: 1210... Loss: 1.4149 Val loss: 1.3958\n",
            "=========== New Epoch ===========\n",
            "Epoch: 46/100... steps: 1220... Loss: 1.3769 Val loss: 1.3938\n",
            "Epoch: 46/100... steps: 1230... Loss: 1.3727 Val loss: 1.3937\n",
            "Epoch: 46/100... steps: 1240... Loss: 1.4111 Val loss: 1.3934\n",
            "=========== New Epoch ===========\n",
            "Epoch: 47/100... steps: 1250... Loss: 1.3709 Val loss: 1.3883\n",
            "Epoch: 47/100... steps: 1260... Loss: 1.4038 Val loss: 1.3866\n",
            "=========== New Epoch ===========\n",
            "Epoch: 48/100... steps: 1270... Loss: 1.3763 Val loss: 1.3872\n",
            "Epoch: 48/100... steps: 1280... Loss: 1.3517 Val loss: 1.3858\n",
            "Epoch: 48/100... steps: 1290... Loss: 1.3741 Val loss: 1.3789\n",
            "=========== New Epoch ===========\n",
            "Epoch: 49/100... steps: 1300... Loss: 1.3388 Val loss: 1.3789\n",
            "Epoch: 49/100... steps: 1310... Loss: 1.3666 Val loss: 1.3762\n",
            "Epoch: 49/100... steps: 1320... Loss: 1.3860 Val loss: 1.3760\n",
            "=========== New Epoch ===========\n",
            "Epoch: 50/100... steps: 1330... Loss: 1.3425 Val loss: 1.3768\n",
            "Epoch: 50/100... steps: 1340... Loss: 1.3764 Val loss: 1.3696\n",
            "Epoch: 50/100... steps: 1350... Loss: 1.4016 Val loss: 1.3706\n",
            "=========== New Epoch ===========\n",
            "Epoch: 51/100... steps: 1360... Loss: 1.3433 Val loss: 1.3678\n",
            "Epoch: 51/100... steps: 1370... Loss: 1.3589 Val loss: 1.3671\n",
            "=========== New Epoch ===========\n",
            "Epoch: 52/100... steps: 1380... Loss: 1.3324 Val loss: 1.3644\n",
            "Epoch: 52/100... steps: 1390... Loss: 1.3355 Val loss: 1.3725\n",
            "Epoch: 52/100... steps: 1400... Loss: 1.3248 Val loss: 1.3676\n",
            "=========== New Epoch ===========\n",
            "Epoch: 53/100... steps: 1410... Loss: 1.3325 Val loss: 1.3618\n",
            "Epoch: 53/100... steps: 1420... Loss: 1.3159 Val loss: 1.3610\n",
            "Epoch: 53/100... steps: 1430... Loss: 1.3187 Val loss: 1.3597\n",
            "=========== New Epoch ===========\n",
            "Epoch: 54/100... steps: 1440... Loss: 1.3058 Val loss: 1.3595\n",
            "Epoch: 54/100... steps: 1450... Loss: 1.3117 Val loss: 1.3576\n",
            "=========== New Epoch ===========\n",
            "Epoch: 55/100... steps: 1460... Loss: 1.3226 Val loss: 1.3534\n",
            "Epoch: 55/100... steps: 1470... Loss: 1.2948 Val loss: 1.3552\n",
            "Epoch: 55/100... steps: 1480... Loss: 1.3148 Val loss: 1.3492\n",
            "=========== New Epoch ===========\n",
            "Epoch: 56/100... steps: 1490... Loss: 1.3157 Val loss: 1.3507\n",
            "Epoch: 56/100... steps: 1500... Loss: 1.2920 Val loss: 1.3491\n",
            "Epoch: 56/100... steps: 1510... Loss: 1.3102 Val loss: 1.3486\n",
            "=========== New Epoch ===========\n",
            "Epoch: 57/100... steps: 1520... Loss: 1.2941 Val loss: 1.3444\n",
            "Epoch: 57/100... steps: 1530... Loss: 1.3049 Val loss: 1.3437\n",
            "=========== New Epoch ===========\n",
            "Epoch: 58/100... steps: 1540... Loss: 1.3146 Val loss: 1.3427\n",
            "Epoch: 58/100... steps: 1550... Loss: 1.2838 Val loss: 1.3429\n",
            "Epoch: 58/100... steps: 1560... Loss: 1.2748 Val loss: 1.3395\n",
            "=========== New Epoch ===========\n",
            "Epoch: 59/100... steps: 1570... Loss: 1.2796 Val loss: 1.3397\n",
            "Epoch: 59/100... steps: 1580... Loss: 1.2939 Val loss: 1.3401\n",
            "Epoch: 59/100... steps: 1590... Loss: 1.2885 Val loss: 1.3415\n",
            "=========== New Epoch ===========\n",
            "Epoch: 60/100... steps: 1600... Loss: 1.2862 Val loss: 1.3363\n",
            "Epoch: 60/100... steps: 1610... Loss: 1.2805 Val loss: 1.3352\n",
            "Epoch: 60/100... steps: 1620... Loss: 1.3003 Val loss: 1.3327\n",
            "=========== New Epoch ===========\n",
            "Epoch: 61/100... steps: 1630... Loss: 1.2750 Val loss: 1.3343\n",
            "Epoch: 61/100... steps: 1640... Loss: 1.2665 Val loss: 1.3332\n",
            "=========== New Epoch ===========\n",
            "Epoch: 62/100... steps: 1650... Loss: 1.2770 Val loss: 1.3341\n",
            "Epoch: 62/100... steps: 1660... Loss: 1.2696 Val loss: 1.3347\n",
            "Epoch: 62/100... steps: 1670... Loss: 1.2638 Val loss: 1.3279\n",
            "=========== New Epoch ===========\n",
            "Epoch: 63/100... steps: 1680... Loss: 1.2804 Val loss: 1.3305\n",
            "Epoch: 63/100... steps: 1690... Loss: 1.2615 Val loss: 1.3296\n",
            "Epoch: 63/100... steps: 1700... Loss: 1.2666 Val loss: 1.3267\n",
            "=========== New Epoch ===========\n",
            "Epoch: 64/100... steps: 1710... Loss: 1.2543 Val loss: 1.3293\n",
            "Epoch: 64/100... steps: 1720... Loss: 1.2582 Val loss: 1.3241\n",
            "=========== New Epoch ===========\n",
            "Epoch: 65/100... steps: 1730... Loss: 1.2686 Val loss: 1.3226\n",
            "Epoch: 65/100... steps: 1740... Loss: 1.2455 Val loss: 1.3250\n",
            "Epoch: 65/100... steps: 1750... Loss: 1.2646 Val loss: 1.3209\n",
            "=========== New Epoch ===========\n",
            "Epoch: 66/100... steps: 1760... Loss: 1.2656 Val loss: 1.3189\n",
            "Epoch: 66/100... steps: 1770... Loss: 1.2434 Val loss: 1.3256\n",
            "Epoch: 66/100... steps: 1780... Loss: 1.2589 Val loss: 1.3217\n",
            "=========== New Epoch ===========\n",
            "Epoch: 67/100... steps: 1790... Loss: 1.2466 Val loss: 1.3187\n",
            "Epoch: 67/100... steps: 1800... Loss: 1.2594 Val loss: 1.3183\n",
            "=========== New Epoch ===========\n",
            "Epoch: 68/100... steps: 1810... Loss: 1.2671 Val loss: 1.3152\n",
            "Epoch: 68/100... steps: 1820... Loss: 1.2376 Val loss: 1.3136\n",
            "Epoch: 68/100... steps: 1830... Loss: 1.2273 Val loss: 1.3193\n",
            "=========== New Epoch ===========\n",
            "Epoch: 69/100... steps: 1840... Loss: 1.2346 Val loss: 1.3143\n",
            "Epoch: 69/100... steps: 1850... Loss: 1.2495 Val loss: 1.3160\n",
            "Epoch: 69/100... steps: 1860... Loss: 1.2454 Val loss: 1.3141\n",
            "=========== New Epoch ===========\n",
            "Epoch: 70/100... steps: 1870... Loss: 1.2367 Val loss: 1.3112\n",
            "Epoch: 70/100... steps: 1880... Loss: 1.2344 Val loss: 1.3135\n",
            "Epoch: 70/100... steps: 1890... Loss: 1.2564 Val loss: 1.3109\n",
            "=========== New Epoch ===========\n",
            "Epoch: 71/100... steps: 1900... Loss: 1.2323 Val loss: 1.3106\n",
            "Epoch: 71/100... steps: 1910... Loss: 1.2256 Val loss: 1.3100\n",
            "=========== New Epoch ===========\n",
            "Epoch: 72/100... steps: 1920... Loss: 1.2375 Val loss: 1.3090\n",
            "Epoch: 72/100... steps: 1930... Loss: 1.2273 Val loss: 1.3070\n",
            "Epoch: 72/100... steps: 1940... Loss: 1.2183 Val loss: 1.3057\n",
            "=========== New Epoch ===========\n",
            "Epoch: 73/100... steps: 1950... Loss: 1.2332 Val loss: 1.3035\n",
            "Epoch: 73/100... steps: 1960... Loss: 1.2162 Val loss: 1.3103\n",
            "Epoch: 73/100... steps: 1970... Loss: 1.2238 Val loss: 1.3131\n",
            "=========== New Epoch ===========\n",
            "Epoch: 74/100... steps: 1980... Loss: 1.2137 Val loss: 1.3047\n",
            "Epoch: 74/100... steps: 1990... Loss: 1.2138 Val loss: 1.3098\n",
            "=========== New Epoch ===========\n",
            "Epoch: 75/100... steps: 2000... Loss: 1.2297 Val loss: 1.3041\n",
            "Epoch: 75/100... steps: 2010... Loss: 1.2077 Val loss: 1.3040\n",
            "Epoch: 75/100... steps: 2020... Loss: 1.2192 Val loss: 1.3019\n",
            "=========== New Epoch ===========\n",
            "Epoch: 76/100... steps: 2030... Loss: 1.2214 Val loss: 1.3015\n",
            "Epoch: 76/100... steps: 2040... Loss: 1.2020 Val loss: 1.3020\n",
            "Epoch: 76/100... steps: 2050... Loss: 1.2225 Val loss: 1.3092\n",
            "=========== New Epoch ===========\n",
            "Epoch: 77/100... steps: 2060... Loss: 1.2074 Val loss: 1.3012\n",
            "Epoch: 77/100... steps: 2070... Loss: 1.2162 Val loss: 1.3045\n",
            "=========== New Epoch ===========\n",
            "Epoch: 78/100... steps: 2080... Loss: 1.2269 Val loss: 1.3042\n",
            "Epoch: 78/100... steps: 2090... Loss: 1.2003 Val loss: 1.3040\n",
            "Epoch: 78/100... steps: 2100... Loss: 1.1920 Val loss: 1.3045\n",
            "=========== New Epoch ===========\n",
            "Epoch: 79/100... steps: 2110... Loss: 1.1989 Val loss: 1.2963\n",
            "Epoch: 79/100... steps: 2120... Loss: 1.2081 Val loss: 1.2978\n",
            "Epoch: 79/100... steps: 2130... Loss: 1.2074 Val loss: 1.2973\n",
            "=========== New Epoch ===========\n",
            "Epoch: 80/100... steps: 2140... Loss: 1.2042 Val loss: 1.2983\n",
            "Epoch: 80/100... steps: 2150... Loss: 1.1976 Val loss: 1.2998\n",
            "Epoch: 80/100... steps: 2160... Loss: 1.2191 Val loss: 1.2977\n",
            "=========== New Epoch ===========\n",
            "Epoch: 81/100... steps: 2170... Loss: 1.1964 Val loss: 1.3005\n",
            "Epoch: 81/100... steps: 2180... Loss: 1.1878 Val loss: 1.2991\n",
            "=========== New Epoch ===========\n",
            "Epoch: 82/100... steps: 2190... Loss: 1.1960 Val loss: 1.2960\n",
            "Epoch: 82/100... steps: 2200... Loss: 1.1881 Val loss: 1.2980\n",
            "Epoch: 82/100... steps: 2210... Loss: 1.1848 Val loss: 1.2982\n",
            "=========== New Epoch ===========\n",
            "Epoch: 83/100... steps: 2220... Loss: 1.2013 Val loss: 1.2909\n",
            "Epoch: 83/100... steps: 2230... Loss: 1.1817 Val loss: 1.3017\n",
            "Epoch: 83/100... steps: 2240... Loss: 1.1883 Val loss: 1.2966\n",
            "=========== New Epoch ===========\n",
            "Epoch: 84/100... steps: 2250... Loss: 1.1792 Val loss: 1.2950\n",
            "Epoch: 84/100... steps: 2260... Loss: 1.1870 Val loss: 1.2981\n",
            "=========== New Epoch ===========\n",
            "Epoch: 85/100... steps: 2270... Loss: 1.1969 Val loss: 1.2971\n",
            "Epoch: 85/100... steps: 2280... Loss: 1.1757 Val loss: 1.2966\n",
            "Epoch: 85/100... steps: 2290... Loss: 1.1901 Val loss: 1.2942\n",
            "=========== New Epoch ===========\n",
            "Epoch: 86/100... steps: 2300... Loss: 1.1902 Val loss: 1.2950\n",
            "Epoch: 86/100... steps: 2310... Loss: 1.1711 Val loss: 1.2908\n",
            "Epoch: 86/100... steps: 2320... Loss: 1.1853 Val loss: 1.2985\n",
            "=========== New Epoch ===========\n",
            "Epoch: 87/100... steps: 2330... Loss: 1.1799 Val loss: 1.2934\n",
            "Epoch: 87/100... steps: 2340... Loss: 1.1801 Val loss: 1.2922\n",
            "=========== New Epoch ===========\n",
            "Epoch: 88/100... steps: 2350... Loss: 1.1926 Val loss: 1.2912\n",
            "Epoch: 88/100... steps: 2360... Loss: 1.1689 Val loss: 1.2903\n",
            "Epoch: 88/100... steps: 2370... Loss: 1.1561 Val loss: 1.2978\n",
            "=========== New Epoch ===========\n",
            "Epoch: 89/100... steps: 2380... Loss: 1.1660 Val loss: 1.2966\n",
            "Epoch: 89/100... steps: 2390... Loss: 1.1797 Val loss: 1.2895\n",
            "Epoch: 89/100... steps: 2400... Loss: 1.1788 Val loss: 1.2905\n",
            "=========== New Epoch ===========\n",
            "Epoch: 90/100... steps: 2410... Loss: 1.1736 Val loss: 1.2855\n",
            "Epoch: 90/100... steps: 2420... Loss: 1.1680 Val loss: 1.2903\n",
            "Epoch: 90/100... steps: 2430... Loss: 1.1918 Val loss: 1.2902\n",
            "=========== New Epoch ===========\n",
            "Epoch: 91/100... steps: 2440... Loss: 1.1698 Val loss: 1.2887\n",
            "Epoch: 91/100... steps: 2450... Loss: 1.1571 Val loss: 1.2947\n",
            "=========== New Epoch ===========\n",
            "Epoch: 92/100... steps: 2460... Loss: 1.1718 Val loss: 1.2964\n",
            "Epoch: 92/100... steps: 2470... Loss: 1.1615 Val loss: 1.2903\n",
            "Epoch: 92/100... steps: 2480... Loss: 1.1547 Val loss: 1.2913\n",
            "=========== New Epoch ===========\n",
            "Epoch: 93/100... steps: 2490... Loss: 1.1705 Val loss: 1.2909\n",
            "Epoch: 93/100... steps: 2500... Loss: 1.1529 Val loss: 1.2878\n",
            "Epoch: 93/100... steps: 2510... Loss: 1.1610 Val loss: 1.2937\n",
            "=========== New Epoch ===========\n",
            "Epoch: 94/100... steps: 2520... Loss: 1.1499 Val loss: 1.2899\n",
            "Epoch: 94/100... steps: 2530... Loss: 1.1576 Val loss: 1.2907\n",
            "=========== New Epoch ===========\n",
            "Epoch: 95/100... steps: 2540... Loss: 1.1668 Val loss: 1.2876\n",
            "Epoch: 95/100... steps: 2550... Loss: 1.1461 Val loss: 1.2891\n",
            "Epoch: 95/100... steps: 2560... Loss: 1.1588 Val loss: 1.2949\n",
            "=========== New Epoch ===========\n",
            "Epoch: 96/100... steps: 2570... Loss: 1.1604 Val loss: 1.2956\n",
            "Epoch: 96/100... steps: 2580... Loss: 1.1431 Val loss: 1.2867\n",
            "Epoch: 96/100... steps: 2590... Loss: 1.1580 Val loss: 1.2926\n",
            "=========== New Epoch ===========\n",
            "Epoch: 97/100... steps: 2600... Loss: 1.1515 Val loss: 1.2845\n",
            "Epoch: 97/100... steps: 2610... Loss: 1.1592 Val loss: 1.2868\n",
            "=========== New Epoch ===========\n",
            "Epoch: 98/100... steps: 2620... Loss: 1.1679 Val loss: 1.2868\n",
            "Epoch: 98/100... steps: 2630... Loss: 1.1440 Val loss: 1.2935\n",
            "Epoch: 98/100... steps: 2640... Loss: 1.1352 Val loss: 1.2976\n",
            "=========== New Epoch ===========\n",
            "Epoch: 99/100... steps: 2650... Loss: 1.1392 Val loss: 1.2894\n",
            "Epoch: 99/100... steps: 2660... Loss: 1.1529 Val loss: 1.2865\n",
            "Epoch: 99/100... steps: 2670... Loss: 1.1513 Val loss: 1.2853\n",
            "=========== New Epoch ===========\n",
            "Epoch: 100/100... steps: 2680... Loss: 1.1464 Val loss: 1.2889\n",
            "Epoch: 100/100... steps: 2690... Loss: 1.1422 Val loss: 1.2874\n",
            "Epoch: 100/100... steps: 2700... Loss: 1.1624 Val loss: 1.2912\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6tDVDW4UygiL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "outputId": "414f1cba-c0c8-4941-fa18-2c3585566510"
      },
      "source": [
        "# another sample for loaded model\n",
        "print(sample(net, 2000, top_k=5, prime= \"He Said\"))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "He Said, he saw what he had been a personage, so that the clavirgation of her husband was at him on the mother. But to herself the doctor, who had sent the doctor's serious sufferings, which took his eyes and he was already\n",
            "struggled again at the\n",
            "possibility, to say that there\n",
            "was a step of all these words wishing to the midst of her matter and her, that it was sitting to be a side. He shook her side of his statical smile. His house with the palicion at that start of her set had been all his honse for the first other portrait in the sense of the first of the life. He was nithing a\n",
            "little bad and docross to his surdame in. He could not be sitting, and so in spite of his birst\n",
            "and hearing her face and her paces of all, with the clasp she could, she was not saying what had supered to have the count of the servant was the sort of comprehense of the\n",
            "companions, and that would have said this to speaking; he saw that this worst that his best was\n",
            "as happy and so many first most only the same in the position in which he was ashamed.\n",
            "\n",
            "Some peasants with her husband was\n",
            "alone with\n",
            "the\n",
            "subject,\n",
            "the peasants and\n",
            "harronsing the whit had been attanded thers. As he should be store.\n",
            "\n",
            "\"I am all the called with a man that I am spollance, and there's not a lear to\n",
            "him.\" She had asked him,\n",
            "breaking the condition of the divorce, the second train he was not feeling to spood at one after what his head or to come on with which he would be doting, and there was no concentration and askard they were all sidional, and always so simple and contraling the presedn of the marsh, and the details of the white subject of the\n",
            "same words and without any sort of soring from his sister's son, as\n",
            "a position was a letter what was a low of hand\n",
            "in the morning. She was continually and changing him, so that the mother's angries\n",
            "always did not\n",
            "learn that there's nothing\n",
            "so haven in his hand with her foresent.\n",
            "\n",
            "\"If\n",
            "it's to go to the prince a man of seeing\n",
            "yourself.\"\n",
            "\n",
            "\"No, to me, what has he to say, too,\" she said. \"I'm\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TFE1Mkyyv-5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# use best practice\n",
        "model_name = 'rnn_100_epoch.net'\n",
        "\n",
        "checkpoint = {'n_hidden_size' : net.n_hidden_size,\n",
        "              'n_layers' : net.n_layers,\n",
        "              'state_dict' : net.state_dict(),\n",
        "              'tokens' : net.chars}\n",
        "\n",
        "with open(model_name, 'wb') as f:\n",
        "  torch.save(checkpoint, f)"
      ],
      "execution_count": 48,
      "outputs": []
    }
  ]
}