{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Boston_House(AWS_SageMaker.Deploy_Model).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vedantdave77/project.Orca/blob/master/ML%20deployment-%20AWS.SageMaker/Boston_House(AWS_SageMaker_Deploy_Model).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIWbGtVPjtnQ",
        "colab_type": "text"
      },
      "source": [
        "# AWS SAGEMAKER \n",
        "Hello, I am [Vedant_Dave](vedantdave77@gmail.com), a data enthusiast with deep interest in machine learning and deep learning. \n",
        "\n",
        "## Intro To Topic\n",
        "Today, I am going to deploy Boston Housing data Project using AWS Sagemaker's High level API called - \"Python SDK\". \n",
        "> This API has facility to train and deploy model in cloud directly from innner Jupyter notebook creation. So, I will use simple Machine learning workflow as usaual. \n",
        "\n",
        "> Data loading --> Data Preparation --> Model Training --> HP Tuning --> Deployment in AWS. (Hopefully, try to make Web Application). \n",
        "\n",
        "---\n",
        "---\n",
        "\n",
        "First of all, I will use SageMaker's batch transform feature, which  is a high-performance and high-throughput method for transforming data and generating inferences. \n",
        "\n",
        "- I personally think, It's ideal for scenarios where you're dealing with large batches of data, don't need sub-second latency, or need to both preprocess and transform the training data. \n",
        "\n",
        "- My main focus is to deploy model, so on analytic point of view, I tried to use Sagemaker's ML library and find median housing price for specific housing requrements in certain areas. \n",
        "\n",
        " \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DB3yDsFIJB_W",
        "colab_type": "text"
      },
      "source": [
        "## Set Environment (lib & SageMaker)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gslpcnrwJiMC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Setting-up Notebook in relevant environment.\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from sklearn.datasets import load_boston\n",
        "import sklearn.model_selection"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOtUDtqJmNcg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set sagemaker in env.\n",
        "import sagemaker\n",
        "from sagemaker import get_execution_role\n",
        "from sagemaker.amazon.amazon_estimator import image_url\n",
        "from sagemaker.predictor import csv_serializer\n",
        "\n",
        "# Object to represent current active session of sagemaker - contains some useful info. for future usage.\n",
        "session = sagemaker.Session()\n",
        "\n",
        "# object shows IAM role - will help us to assign training job to sagemaker.\n",
        "role = get_execution_role()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gP7UA5RHoG1c",
        "colab_type": "text"
      },
      "source": [
        "## Download Data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tGz26OMNoDnN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "boston = load_boston()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQ1Ldf2HoXgX",
        "colab_type": "text"
      },
      "source": [
        "## Data preparation and splitting.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5HXQWXdoDkW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# prepare data for python notebook\n",
        "X_bos_pd = pd.DataFrame(boston.data, columns=boston, feature_names)\n",
        "y_bos_pd = pd.DataFrame(bosotn.target)\n",
        "\n",
        "# splitting into train and test\n",
        "x_train,S_test,Y_train,Y_test = sklearn.model_selection.train_test_split(X_bos_pd, Y_bos_pd, test_size =0.33)\n",
        "\n",
        "# further splitting of train to train(2/3) and validation(1/3)\n",
        "X_train, X_val, Y_train, Y_val = sklearn.model_selection.train_test_split(X_train, Y_train, test_size=0.33)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6p9mDXm4qQPo",
        "colab_type": "text"
      },
      "source": [
        "## Uploading dataa files to S3.\n",
        "\n",
        "Keep in mind that, \n",
        "\n",
        "- When a training job is constructed using SageMaker, a container is executed which performs the training operation.\n",
        "- This container is given access to data that is stored in S3. This means that we need to upload the data we want to use for training to S3. \n",
        "- In addition, when we perform a batch transform job, SageMaker expects the input data to be stored on S3. We can use the SageMaker API to do this and hide some of the details, but first data saved locally and then uploaded to S3 container.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yc2SI2v4xy92",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define & ensure data dictionary...\n",
        "data_dir = '.../data/boston'\n",
        "if not os.path.exists(data_dir):\n",
        "  os.makedirs(data_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYQAaia5oDiK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# In data_dir, I amd creating csv file format for all data, and in validation and train set target data comes in first columns.\n",
        "\n",
        "X_test.to_csv(os.path.join(data_dir,'test.csv'),header = False, index = False)\n",
        " \n",
        "pd.concat([Y_val,X_val], axis =1).to_csv(os.path.join(data_dir, 'validation.csv'),header=False, index= False)\n",
        "pd.concat([Y_train,X_train],axis =1).to_csv(os.path.join(data_dir,'train.csv'),header=Flase,index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4w9BxAIs13oi",
        "colab_type": "text"
      },
      "source": [
        "### Upload to S3 - data storage.\n",
        "Its good prectice to give prefix to your S3 bucket, so you can easily get idea about specific container for relevant project. \n",
        "- Here, I am giving name as \"dataset_name-algorithm_name-API_level\".\n",
        "\n",
        "I will use xgboost algorithm, which is one of the modern approach for supervised learning. It boost our algorithm gradient and give high accuracy result with good F1 score Matrix. \n",
        "\n",
        "> For more info, visit [XGBoost](https://xgboost.readthedocs.io/en/latest/) official documentation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rq5Uhd5doDel",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prefix = 'boston-xgboost-HL'\n",
        "\n",
        "test_location = session.upload_data(os.path.join(data_dir,'test.csv'),key_prefix = prefix)\n",
        "val_location = session.upload_data(os.path.join(data_dir,'validation.csv'), key_prefix = prefix)\n",
        "train_location = session.upload_data(os.path.join(data_dir,'train.csv'),key_prefix = prefix)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXKWD1V_36wO",
        "colab_type": "text"
      },
      "source": [
        "## Training XGBoost Model\n",
        "There are two options for training model either use high-level API in which Sage-Maker will train algorithm ownself or from low level API inwhich we need to define our own work. \n",
        "\n",
        "I will go with both the cases to represent difference. Before this we must need some important information for sagemaker to give permissions and you can find them from [common_para_list](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-algo-docker-registry-paths.html)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXtdSvnHoDa3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define container with uri to define container with session and ml-model info.\n",
        "container = get_image_uri(session.boto_region_name),'xgboost')\n",
        "\n",
        "# Now construct container object with requried parametrs.\n",
        "xgb = sagemaker.estimator.Estimator(continer,                                   # our training container\n",
        "                                    role,                                       # defined IAM role for training\n",
        "                                    train_instance_count=1,                     # instaces, depend how many you created - for lengthy job need more\n",
        "                                    train_instance_type = 'ml.m4.xlarge',       # type of instace type for deployjent - can use m2 to m5 (AWS rate will according to that, check here --> https://aws.amazon.com/sagemaker/pricing/)\n",
        "                                    output_path = 's3://{}/{}/output'.format(session.default_bucket(),prefix),   # output destination\n",
        "                                    sagemaker_session=session)                  # current session (because instance are on regionwise servers, s3 bucket is globalize platform)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjnFyDbzCRO9",
        "colab_type": "text"
      },
      "source": [
        "> ***SageMaker has xgb HP tuning parameters as follow, you  can check it*** [here](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost_hyperparameters.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uFTgAYkoDYW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set HyperParameter of model.\n",
        "xgv.set_hyperparameters(max_depth= 5,\n",
        "                        etz = 0.2,\n",
        "                        min_child_weight= 6,\n",
        "                        subsample = 0.8,\n",
        "                        objective = 'reg\"linear',\n",
        "                        early_stopping_round = 10,\n",
        "                        num_round = 200)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHH4FRGZoDVs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# give some more info to sagemaker about our input's data structure and arrangement\n",
        "s3_input_train = sagemaker.s3_input(s3_data= train_location, content_type='csv')\n",
        "s3_input_validation = sagemaker.s3_input(s3_data= val_location, content_type='csv')\n",
        "\n",
        "xgb.fit({'train':s3_input_train,'validation': s3_input_validation})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSQ_lAVreeJo",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "\n",
        "**Attension:**\n",
        "***Hello, I am from past,*** if you visited my notebook on batch-transform method then you see the testing model phase, but in case of deployment of model the scenario is changed\n",
        "\n",
        "After deploying, I will test model in deployment environment and it helps us to get endpoint. - where we can get opportunity to give our own dataset outside of current dataset, menas you can apply your custom dataset by your own.\n",
        "\n",
        "Simply, \n",
        "\n",
        "> **AWS[S3(DATA) + sagemaker(MODEL) -+- [{ENDPOINT(access_link)} ] -+- host_platform(WEB_APPLICATION)] - user**\n",
        "\n",
        "Well, its use for application purpose. In app deployment we will see it further. \n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fr_EtAQUDXrM",
        "colab_type": "text"
      },
      "source": [
        "## Deploy Model\n",
        "Now that we have fit our model to the training data, and used the validation data to avoid overfitting, we can deploy our model and test it. Deploying is very simple when we use the high level API, we need only call the `deploy` method of our trained estimator\n",
        "\n",
        "**NOTE:** When deploying a model you are asking SageMaker to launch an compute instance that will wait for data to be sent to it. As a result, this compute instance will continue to run until *you* shut it down. This is important to know since the cost of a deployed endpoint depends on how long it has been running for.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YeUMCepHoDT0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xgb_predictor = xgb.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DS-cj2aPfrca",
        "colab_type": "text"
      },
      "source": [
        "### Use Model in deployed environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxfJWfd1oDQs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We need to tell the endpoint what format the data we are sending is in\n",
        "xgb_predictor.content_type = 'text/csv'\n",
        "xgb_predictor.serializer = csv_serializer\n",
        "\n",
        "Y_pred = xgb_predictor.predict(X_test.values).decode('utf-8')\n",
        "# predictions is currently a comma delimited string and so we would like to break it up as a numpy array.\n",
        "Y_pred = np.fromstring(Y_pred, sep=',')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QV0QZe-oFdZG",
        "colab_type": "text"
      },
      "source": [
        "Our Output will give us result after execution, here we get output from endpoint. So, no need to save output in S3 or no need to save model for future purpose. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXx09MqeGTA8",
        "colab_type": "text"
      },
      "source": [
        "## Ouput visualization (scatter plot)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CP4VMuG-FTR6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y_pred = pd.read_csv(os.path.join(data_dir, 'test.csv.out'), header=None)\n",
        "plt.scatter(Y_test, Y_pred)\n",
        "plt.xlabel(\"Median Price\")\n",
        "plt.ylabel(\"Predicted Price\")\n",
        "plt.title(\"Median Price vs Predicted Price\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0NQHJXCgc5v",
        "colab_type": "text"
      },
      "source": [
        "### Delete the endpoint\n",
        "End point must be shutdown, otherwise it will cost us heavyly as it runs continuesly without input data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bXBxLm4ggZQP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xgb_predictor.delete_endpoing()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLniLdj9GtLN",
        "colab_type": "text"
      },
      "source": [
        "### Important Observation:\n",
        "The sagemaker does not have lot of disk space regards to notebook space. After the ML modeling,training and testing it will fully used some time. And, give some errors. Which is not easy to diagnosed. So, you must remove data_dir and executed file\n",
        "\n",
        "- You can do it from terminate notebook stance also.\n",
        "\n",
        "***Keep in mind*, IT WILL LOSS YOUR ALL DATA**, even given **INPUT DATA**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEUH0aZ-FTPr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# remove all the files from data_dir\n",
        "!rm $data_dir/*\n",
        "\n",
        "# remove directory\n",
        "!rmdir $data_dir"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fAwYFU2aH6LC",
        "colab_type": "text"
      },
      "source": [
        "### Summary \n",
        "Let's revise \n",
        "The flow of notebook  ...\n",
        "1. Download or otherwise retrieve the data.\n",
        "2. Process / Prepare the data.\n",
        "3. Upload the processed data to S3.\n",
        "4. Train a chosen model.\n",
        "5. Test the trained model (typically using a batch transform job).\n",
        "6. Deploy the trained model.\n",
        "7. Use the deployed model.\n",
        "\n",
        "Thats all, please visit [image_folder]() to see aws working screenshots."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqrUT1T-FTMU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# \"Keep Learning, Enjoy Empowering\" @dave117"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}