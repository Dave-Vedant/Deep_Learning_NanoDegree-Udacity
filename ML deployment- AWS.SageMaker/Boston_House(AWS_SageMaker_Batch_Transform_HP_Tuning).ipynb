{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Boston_House(AWS-SageMaker.Batch_Transform/HP_Tuning).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO8fRQCUlOMbUYhkz1VU7FE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vedantdave77/project.Orca/blob/master/ML%20deployment-%20AWS.SageMaker/Boston_House(AWS_SageMaker_Batch_Transform_HP_Tuning).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIWbGtVPjtnQ",
        "colab_type": "text"
      },
      "source": [
        "# AWS SAGEMAKER \n",
        "**--------------------------------------**\n",
        "Hello, I am [Vedant_Dave](vedantdave77@gmail.com), a data enthusiast with deep interest in machine learning and deep learning. \n",
        "\n",
        "## Intro To Topic\n",
        "Today, I am going to deploy Boston Housing data Project using AWS Sagemaker's High level API called - \"Python SDK\". \n",
        "> This API has facility to train and deploy model in cloud directly from innner Jupyter notebook creation. So, I will use simple Machine learning workflow as usaual. \n",
        "\n",
        "> Data loading --> Data Preparation --> Model Training --> HP Tuning --> Deployment in AWS. (Hopefully, try to make Web Application). \n",
        "\n",
        "---\n",
        "---\n",
        "\n",
        "First of all, I will use SageMaker's batch transform feature, which  is a high-performance and high-throughput method for transforming data and generating inferences. \n",
        "\n",
        "- I personally think, It's ideal for scenarios where you're dealing with large batches of data, don't need sub-second latency, or need to both preprocess and transform the training data. \n",
        "\n",
        "- My main focus is to deploy model, so on analytic point of view, I tried to use Sagemaker's ML library and find median housing price for specific housing requrements in certain areas. \n",
        "\n",
        " \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DB3yDsFIJB_W",
        "colab_type": "text"
      },
      "source": [
        "## Set Environment (lib & SageMaker)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gslpcnrwJiMC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Setting-up Notebook in relevant environment.\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from sklearn.datasets import load_boston\n",
        "import sklearn.model_selection"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOtUDtqJmNcg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set sagemaker in env.\n",
        "import sagemaker\n",
        "from sagemaker import get_execution_role\n",
        "from sagemaker.amazon.amazon_estimator import image_url\n",
        "from sagemaker.predictor import csv_serializer\n",
        "\n",
        "# Object to represent current active session of sagemaker - contains some useful info. for future usage.\n",
        "session = sagemaker.Session()\n",
        "\n",
        "# object shows IAM role - will help us to assign training job to sagemaker.\n",
        "role = get_execution_role()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gP7UA5RHoG1c",
        "colab_type": "text"
      },
      "source": [
        "## Download Data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tGz26OMNoDnN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "boston = load_boston()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQ1Ldf2HoXgX",
        "colab_type": "text"
      },
      "source": [
        "## Data preparation and splitting.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5HXQWXdoDkW",
        "colab_type": "code",
        "outputId": "0537510b-a48e-4560-d81b-16e64e6edfca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 132
        }
      },
      "source": [
        "# prepare data for python notebook\n",
        "X_bos_pd = pd.DataFrame(boston.data, columns=boston, feature_names)\n",
        "y_bos_pd = pd.DataFrame(bosotn.target)\n",
        "\n",
        "# splitting into train and test\n",
        "x_train,S_test,Y_train,Y_test = sklearn.model_selection.train_test_split(X_bos_pd, Y_bos_pd, test_size =0.33)\n",
        "\n",
        "# further splitting of train to train(2/3) and validation(1/3)\n",
        "X_train, X_val, Y_train, Y_val = sklearn.model_selection.train_test_split(X_train, Y_train, test_size=0.33)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-da5df2199d7d>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    X_bos_pd = pd.DataFrame(boston.data, columns=boston, feature_names)\u001b[0m\n\u001b[0m                                                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m positional argument follows keyword argument\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6p9mDXm4qQPo",
        "colab_type": "text"
      },
      "source": [
        "## Uploading dataa files to S3.\n",
        "\n",
        "Keep in mind that, \n",
        "\n",
        "- When a training job is constructed using SageMaker, a container is executed which performs the training operation.\n",
        "- This container is given access to data that is stored in S3. This means that we need to upload the data we want to use for training to S3. \n",
        "- In addition, when we perform a batch transform job, SageMaker expects the input data to be stored on S3. We can use the SageMaker API to do this and hide some of the details, but first data saved locally and then uploaded to S3 container.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yc2SI2v4xy92",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define & ensure data dictionary...\n",
        "data_dir = '.../data/boston'\n",
        "if not os.path.exists(data_dir):\n",
        "  os.makedirs(data_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYQAaia5oDiK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# In data_dir, I amd creating csv file format for all data, and in validation and train set target data comes in first columns.\n",
        "\n",
        "X_test.to_csv(os.path.join(data_dir,'test.csv'),header = False, index = False)\n",
        " \n",
        "pd.concat([Y_val,X_val], axis =1).to_csv(os.path.join(data_dir, 'validation.csv'),header=False, index= False)\n",
        "pd.concat([Y_train,X_train],axis =1).to_csv(os.path.join(data_dir,'train.csv'),header=Flase,index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4w9BxAIs13oi",
        "colab_type": "text"
      },
      "source": [
        "### Upload to S3 - data storage.\n",
        "Its good prectice to give prefix to your S3 bucket, so you can easily get idea about specific container for relevant project. \n",
        "- Here, I am giving name as \"dataset_name-algorithm_name-API_level\".\n",
        "\n",
        "I will use xgboost algorithm, which is one of the modern approach for supervised learning. It boost our algorithm gradient and give high accuracy result with good F1 score Matrix. \n",
        "\n",
        "> For more info, visit [XGBoost](https://xgboost.readthedocs.io/en/latest/) official documentation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rq5Uhd5doDel",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prefix = 'boston-xgboost-HL'\n",
        "\n",
        "test_location = session.upload_data(os.path.join(data_dir,'test.csv'),key_prefix = prefix)\n",
        "val_location = session.upload_data(os.path.join(data_dir,'validation.csv'), key_prefix = prefix)\n",
        "train_location = session.upload_data(os.path.join(data_dir,'train.csv'),key_prefix = prefix)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXKWD1V_36wO",
        "colab_type": "text"
      },
      "source": [
        "## Training XGBoost Model\n",
        "There are two options for training model either use high-level API in which Sage-Maker will train algorithm ownself or from low level API inwhich we need to define our own work. \n",
        "\n",
        "I will go with both the cases to represent difference. Before this we must need some important information for sagemaker to give permissions and you can find them from [common_para_list](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-algo-docker-registry-paths.html)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXtdSvnHoDa3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define container with uri to define container with session and ml-model info.\n",
        "container = get_image_uri(session.boto_region_name),'xgboost')\n",
        "\n",
        "# Now construct container object with requried parametrs.\n",
        "xgb = sagemaker.estimator.Estimator(continer,                                   # our training container\n",
        "                                    role,                                       # defined IAM role for training\n",
        "                                    train_instance_count=1,                     # instaces, depend how many you created - for lengthy job need more\n",
        "                                    train_instance_type = 'ml.m4.xlarge',       # type of instace type for deployjent - can use m2 to m5 (AWS rate will according to that, check here --> https://aws.amazon.com/sagemaker/pricing/)\n",
        "                                    output_path = 's3://{}/{}/output'.format(session.default_bucket(),prefix),   # output destination\n",
        "                                    sagemaker_session=session)                  # current session (because instance are on regionwise servers, s3 bucket is globalize platform)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjnFyDbzCRO9",
        "colab_type": "text"
      },
      "source": [
        "> ***SageMaker has xgb HP tuning parameters as follow, you  can check it*** [here](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost_hyperparameters.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uFTgAYkoDYW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set HyperParameter of model.\n",
        "xgb.set_hyperparameters(max_depth= 5,\n",
        "                        etz = 0.2,\n",
        "                        min_child_weight= 6,\n",
        "                        subsample = 0.8,\n",
        "                        objective = 'reg\"linear',\n",
        "                        early_stopping_round = 10,\n",
        "                        num_round = 200)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3aRkAIhvL9Yk",
        "colab_type": "text"
      },
      "source": [
        "The best thing about AWS service is about Hyper Parameter tuning freatures. \n",
        "> If you have little bit experience with ML and DL deep network training on complex data then you know, How much frustrating to set HP for better model accuracy, we all face different problems for that such as overfitting, gradient explodation/ vanishing, underfitting, time/computation complexity etc...\n",
        "\n",
        "With HP tuner we can provide range for output and wait for best model HP, sagemaker will return the best executed model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bojDn-F5L8lP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sagemaker.tuner import IntegerParameter, ContinuousParameter, HyperparameterTuner\n",
        "\n",
        "xgb_hyperparameter_tuner = HyperparameterTuner(estimator = xgb,                                    # choose your model type (ML-approach)\n",
        "                                               objective_metric_name = 'validation:rmse',          # Evaluation Matrix for model comparison \n",
        "                                               objective_type = 'Minimize',                        # What you want? ---> minimize / maximize matrix result as best model (here, minimize will give best accuracy (due to error matrix))\n",
        "                                               max_jobs = 20,                                      # total models to train --> More model=>more time=> expensive \n",
        "                                               max_parallel_jobs = 3,                              # The number of models to train in parallel, (more parallel operation--> powerful instance type(cpu) => More Money to spend)\n",
        "                                               hyperparameter_ranges = {\n",
        "                                                    'max_depth': IntegerParameter(3, 12),        # visit -->  https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost_hyperparameters.html\n",
        "                                                    'eta'      : ContinuousParameter(0.05, 0.5), \n",
        "                                                    'min_child_weight': IntegerParameter(2, 8),\n",
        "                                                    'subsample': ContinuousParameter(0.5, 0.9),\n",
        "                                                    'gamma': ContinuousParameter(0, 10),\n",
        "                                               })"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHH4FRGZoDVs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# give some more info to sagemaker about our input's data structure and arrangement\n",
        "s3_input_train = sagemaker.s3_input(s3_data= train_location, content_type='csv')\n",
        "s3_input_validation = sagemaker.s3_input(s3_data= val_location, content_type='csv')\n",
        "\n",
        "xgb.fit({'train':s3_input_train,'validation': s3_input_validation})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JM8hxoBOMAr6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# wait for model response (after all you gave 20 model execution job!)\n",
        "xgb_hyperparameter_tuner.wait()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZuFagfPMAO4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#pick best model for me for highest accuracy\n",
        "xgb_hyperparameter_tuner.best_training_job()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZXsK17vMBeI",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJfG2n5QL_q5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# replace our previous xgb with best one.\n",
        "xgb_attached = sagemaker.estimator.Estimator.attach(xgb_hyperparameter_tuner.best_training_job())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fr_EtAQUDXrM",
        "colab_type": "text"
      },
      "source": [
        "## Test Model\n",
        "Testing need special requirement of sagemaker-batch transform. It works in background so we need to wait.\n",
        "\n",
        "So, first let's construct transform object and give batch transform work.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YeUMCepHoDT0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xgb_transformer = xgb_attached.transformer(instance_count =1, instance_type = 'ml.m4.xlarge')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxfJWfd1oDQs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xgb_transformer.transform(test_location,content_type='text/csv',split_type='Line')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJXJx29AoDCQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xgb_transformer.wait()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QV0QZe-oFdZG",
        "colab_type": "text"
      },
      "source": [
        "Our Output will saved automatically, and its better idea to save it locally so we can use it even after terminate/stop instance. After all, its a matter of budget and application."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72YF0ABXFTVU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!aws s3 cp --recursive $xgb_transformer.output_path $data_dir"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXx09MqeGTA8",
        "colab_type": "text"
      },
      "source": [
        "## Ouput visualization (scatter plot)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CP4VMuG-FTR6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y_pred = pd.read_csv(os.path.join(data_dir, 'test.csv.out'), header=None)\n",
        "plt.scatter(Y_test, Y_pred)\n",
        "plt.xlabel(\"Median Price\")\n",
        "plt.ylabel(\"Predicted Price\")\n",
        "plt.title(\"Median Price vs Predicted Price\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLniLdj9GtLN",
        "colab_type": "text"
      },
      "source": [
        "### Important Observation:\n",
        "The sagemaker does not have lot of disk space regards to notebook space. After the ML modeling,training and testing it will fully used some time. And, give some errors. Which is not easy to diagnosed. So, you must remove data_dir and executed file\n",
        "\n",
        "- You can do it from terminate notebook stance also.\n",
        "\n",
        "***Keep in mind*, IT WILL LOSS YOUR ALL DATA**, even given **INPUT DATA**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEUH0aZ-FTPr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# remove all the files from data_dir\n",
        "!rm $data_dir/*\n",
        "\n",
        "# remove directory\n",
        "!rmdir $data_dir"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fAwYFU2aH6LC",
        "colab_type": "text"
      },
      "source": [
        "### Summary \n",
        "Let's revise \n",
        "The flow of notebook  ...\n",
        "1. Download or otherwise retrieve the data.\n",
        "2. Process / Prepare the data.\n",
        "3. Upload the processed data to S3.\n",
        "4. Train a chosen model.\n",
        "5. Test the trained model (typically using a batch transform job).\n",
        "6. Deploy the trained model.\n",
        "7. Use the deployed model.\n",
        "\n",
        "Thats all, please visit [image_folder]() to see aws working screenshots."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqrUT1T-FTMU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# \"Keep Learning, Enjoy Empowering\" @dave117"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}