{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/vedantdave77/project.Orca/blob/master/ML%20deployment-%20AWS.SageMaker/Boston_House(AWS_SageMaker_Custom_Batch_Transform).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eIWbGtVPjtnQ"
   },
   "source": [
    "# AWS SAGEMAKER \n",
    "\n",
    "Hello, I am [Vedant_Dave](https://vedantdave77.github.io/), a inspirational machine learning practitioner.\n",
    "## Intro To Topic\n",
    "Today, I am going to deploy Boston Housing data Project using AWS Sagemaker's High level API called - \"Python SDK\". \n",
    "> This API has facility to train and deploy model in cloud directly from innner Jupyter notebook creation. So, I will use simple Machine learning workflow as usaual. \n",
    "\n",
    "> Data loading --> Data Preparation --> Model Training --> HP Tuning --> Deployment in AWS. (Hopefully, try to make Web Application). \n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "First of all, I will use SageMaker's batch transform feature, which  is a high-performance and high-throughput method for transforming data and generating inferences. \n",
    "\n",
    "- I personally think, It's ideal for scenarios where you're dealing with large batches of data, don't need sub-second latency, or need to both preprocess and transform the training data. \n",
    "\n",
    "- My main focus is to deploy model, so on analytic point of view, I tried to use Sagemaker's ML library and find median housing price for specific housing requrements in certain areas. \n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DB3yDsFIJB_W"
   },
   "source": [
    "## Set Environment (lib & SageMaker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gslpcnrwJiMC"
   },
   "outputs": [],
   "source": [
    "# Setting-up Notebook in relevant environment.\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "import sklearn.model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cOtUDtqJmNcg"
   },
   "outputs": [],
   "source": [
    "# set sagemaker in env.\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.amazon.amazon_estimator import image_url\n",
    "from sagemaker.predictor import csv_serializer\n",
    "\n",
    "# Object to represent current active session of sagemaker - contains some useful info. for future usage.\n",
    "session = sagemaker.Session()\n",
    "\n",
    "# object shows IAM role - will help us to assign training job to sagemaker.\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gP7UA5RHoG1c"
   },
   "source": [
    "## Download Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tGz26OMNoDnN"
   },
   "outputs": [],
   "source": [
    "boston = load_boston()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IQ1Ldf2HoXgX"
   },
   "source": [
    "## Data preparation and splitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 132
    },
    "colab_type": "code",
    "id": "E5HXQWXdoDkW",
    "outputId": "0537510b-a48e-4560-d81b-16e64e6edfca"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-da5df2199d7d>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    X_bos_pd = pd.DataFrame(boston.data, columns=boston, feature_names)\u001b[0m\n\u001b[0m                                                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m positional argument follows keyword argument\n"
     ]
    }
   ],
   "source": [
    "# prepare data for python notebook\n",
    "X_bos_pd = pd.DataFrame(boston.data, columns=boston, feature_names)\n",
    "y_bos_pd = pd.DataFrame(bosotn.target)\n",
    "\n",
    "# splitting into train and test\n",
    "x_train,S_test,Y_train,Y_test = sklearn.model_selection.train_test_split(X_bos_pd, Y_bos_pd, test_size =0.33)\n",
    "\n",
    "# further splitting of train to train(2/3) and validation(1/3)\n",
    "X_train, X_val, Y_train, Y_val = sklearn.model_selection.train_test_split(X_train, Y_train, test_size=0.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6p9mDXm4qQPo"
   },
   "source": [
    "## Uploading dataa files to S3.\n",
    "\n",
    "Keep in mind that, \n",
    "\n",
    "- When a training job is constructed using SageMaker, a container is executed which performs the training operation.\n",
    "- This container is given access to data that is stored in S3. This means that we need to upload the data we want to use for training to S3. \n",
    "- In addition, when we perform a batch transform job, SageMaker expects the input data to be stored on S3. We can use the SageMaker API to do this and hide some of the details, but first data saved locally and then uploaded to S3 container.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Yc2SI2v4xy92"
   },
   "outputs": [],
   "source": [
    "# define & ensure data dictionary...\n",
    "data_dir = '.../data/boston'\n",
    "if not os.path.exists(data_dir):\n",
    "  os.makedirs(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yYQAaia5oDiK"
   },
   "outputs": [],
   "source": [
    "# In data_dir, I amd creating csv file format for all data, and in validation and train set target data comes in first columns.\n",
    "\n",
    "X_test.to_csv(os.path.join(data_dir,'test.csv'),header = False, index = False)\n",
    " \n",
    "pd.concat([Y_val,X_val], axis =1).to_csv(os.path.join(data_dir, 'validation.csv'),header=False, index= False)\n",
    "pd.concat([Y_train,X_train],axis =1).to_csv(os.path.join(data_dir,'train.csv'),header=Flase,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4w9BxAIs13oi"
   },
   "source": [
    "### Upload to S3 - data storage.\n",
    "Its good prectice to give prefix to your S3 bucket, so you can easily get idea about specific container for relevant project. \n",
    "- Here, I am giving name as \"dataset_name-algorithm_name-API_level\".\n",
    "\n",
    "I will use xgboost algorithm, which is one of the modern approach for supervised learning. It boost our algorithm gradient and give high accuracy result with good F1 score Matrix. \n",
    "\n",
    "> For more info, visit [XGBoost](https://xgboost.readthedocs.io/en/latest/) official documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rq5Uhd5doDel"
   },
   "outputs": [],
   "source": [
    "prefix = 'boston-xgboost-HL'\n",
    "\n",
    "test_location = session.upload_data(os.path.join(data_dir,'test.csv'),key_prefix = prefix)\n",
    "val_location = session.upload_data(os.path.join(data_dir,'validation.csv'), key_prefix = prefix)\n",
    "train_location = session.upload_data(os.path.join(data_dir,'train.csv'),key_prefix = prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pXKWD1V_36wO"
   },
   "source": [
    "## Train and construct the XGBoost Model\n",
    "\n",
    "### setup the training job\n",
    "\n",
    "For setup training job I need to know the exact information about my sagemaker, s3 container and other general information regarding instance. \n",
    "\n",
    "- I am using this [API](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_CreateTrainingJob.html) provided by sagemaker for reference to create this training job.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kUsidrTfubim"
   },
   "outputs": [],
   "source": [
    "# We will need to know the name of the container that we want to use for training. SageMaker provides  a nice utility method to construct this for us.\n",
    "container = get_image_uri(session.boto_region_name, 'xgboost')\n",
    "\n",
    "# specify parameters for tranining job\n",
    "training_params = {}\n",
    "\n",
    "# specify training role (IAM) of this session [same as sagemaker role]\n",
    "training_params['RoleArn'] = role\n",
    "\n",
    "# specify training algorithm and container for job\n",
    "training_params['AlgorithmSpecification'] = {\n",
    "    \"TrainingImage\": container,\n",
    "    \"TrainingInputMode\": \"File\"\n",
    "}\n",
    "\n",
    "# specify output (model artifacts-model change) space [in s3]\n",
    "training_params['OutputDataConfig'] = {\n",
    "    \"S3OutputPath\": \"s3://\" + session.default_bucket() + \"/\" + prefix + \"/output\"\n",
    "}\n",
    "\n",
    "# specify computer capability provided to instance and stopping condition in case or error.\n",
    "training_params['ResourceConfig'] = {\n",
    "    \"InstanceCount\": 1,\n",
    "    \"InstanceType\": \"ml.m4.xlarge\",\n",
    "    \"VolumeSizeInGB\": 5\n",
    "}\n",
    "    \n",
    "training_params['StoppingCondition'] = {        # Error condition stopping\n",
    "    \"MaxRuntimeInSeconds\": 86400\n",
    "}\n",
    "\n",
    "# XGBoost model hyper parameters.\n",
    "training_params['HyperParameters'] = {\n",
    "    \"max_depth\": \"5\",\n",
    "    \"eta\": \"0.2\",\n",
    "    \"gamma\": \"4\",\n",
    "    \"min_child_weight\": \"6\",\n",
    "    \"subsample\": \"0.8\",\n",
    "    \"objective\": \"reg:linear\",\n",
    "    \"early_stopping_rounds\": \"10\",\n",
    "    \"num_round\": \"200\"\n",
    "}\n",
    "\n",
    "# define the data path (from where and what kind of data sagemaker will retrive)\n",
    "training_params['InputDataConfig'] = [\n",
    "    {\n",
    "        \"ChannelName\": \"train\",                                                 # which data \n",
    "        \"DataSource\": {            \n",
    "            \"S3DataSource\": {\n",
    "                \"S3DataType\": \"S3Prefix\",                                       # data identification\n",
    "                \"S3Uri\": train_location,\n",
    "                \"S3DataDistributionType\": \"FullyReplicated\"\n",
    "            }\n",
    "        },\n",
    "        \"ContentType\": \"csv\",                                                   # what kind of data \n",
    "        \"CompressionType\": \"None\"                                               # large data may be compressed\n",
    "    },\n",
    "    {\n",
    "        \"ChannelName\": \"validation\",                                            # same as above for validation\n",
    "        \"DataSource\": {\n",
    "            \"S3DataSource\": {\n",
    "                \"S3DataType\": \"S3Prefix\",\n",
    "                \"S3Uri\": val_location,\n",
    "                \"S3DataDistributionType\": \"FullyReplicated\"\n",
    "            }\n",
    "        },\n",
    "        \"ContentType\": \"csv\",\n",
    "        \"CompressionType\": \"None\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "esaG5-n5CE_B"
   },
   "source": [
    "### Execute the training job \n",
    "Give command to execute after knowing above data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LoUyeZk_ubfU"
   },
   "outputs": [],
   "source": [
    "training_job_name = \"boston-xgboost-\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "training_params['TrainingJobName'] = training_job_name\n",
    "\n",
    "# And now we ask SageMaker to create (and execute) the training job\n",
    "training_job = session.sagemaker_client.create_training_job(**training_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P9dX8dwBubdH"
   },
   "outputs": [],
   "source": [
    "# Its lengthy job so we need to wait until gettingthe anwer.\n",
    "session.logs_for_job(training_job_name, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xlGQ8NV8l7ev"
   },
   "source": [
    "### Build the model\n",
    "The above job give us lot of information and its hard to understand according to data structure so its better to understand it from sagemaaker ownself. \n",
    "\n",
    "- Then we will make model (not a ml model but model for sagemaker information collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_AIT2-63ubay"
   },
   "outputs": [],
   "source": [
    "training_job_info = session.sagemaker_client.describe_training_job(TrainingJobName=training_job_name)\n",
    "\n",
    "model_artifacts = training_job_info['ModelArtifacts']['S3ModelArtifacts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iMsbCYPjubX5"
   },
   "outputs": [],
   "source": [
    "# give model a unique name by (training job name - mode - so, helpful to distinguish each and every job seperately)\n",
    "model_name = training_job_name + \"-model\"\n",
    "\n",
    "# specify information for sagemaker to understand which container need to use for interference and wher it is situated.\n",
    "primary_container = {\n",
    "    \"Image\": container,\n",
    "    \"ModelDataUrl\": model_artifacts\n",
    "}\n",
    "\n",
    "# And lastly we construct the SageMaker model\n",
    "model_info = session.sagemaker_client.create_model(\n",
    "                                ModelName = model_name,\n",
    "                                ExecutionRoleArn = role,\n",
    "                                PrimaryContainer = primary_container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fap5zF75p0TW"
   },
   "source": [
    "### Testing the Model.\n",
    "\n",
    "We already have trained model now and validation also do its best job to make model more generalize and model will not overfit. \n",
    "\n",
    "Now, we will use batch transform (sagemaker testing method) and for that we will first define training parameters, sagemake information model and information saving data structure with s3.\n",
    "> I will take help from this [API](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_CreateTransformJob.html) to build it up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nUDQTbn-ubVa"
   },
   "outputs": [],
   "source": [
    "# define job name and timing specification\n",
    "transform_job_name = 'boston-xgboost-batch-transform-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "\n",
    "# define datastructure for batch transfer job \n",
    "transform_request = \\\n",
    "{\n",
    "    \"TransformJobName\": transform_job_name,\n",
    "    \n",
    "    # specify model name.\n",
    "    \"ModelName\": model_name,\n",
    "    \n",
    "    # specify no. of instance connect with job (for ensurity with job status)\n",
    "    \"MaxConcurrentTransforms\": 1,\n",
    "    \n",
    "    # specify max split chunk limit so in backgroud job each chunk shold be within this range\n",
    "    \"MaxPayloadInMB\": 6,\n",
    "    \n",
    "    # specify chunk to give multiple input sample (sometime we only need single)\n",
    "    \"BatchStrategy\": \"MultiRecord\",\n",
    "    \n",
    "    # specify output storage (in s3 container)\n",
    "    \"TransformOutput\": {\n",
    "        \"S3OutputPath\": \"s3://{}/{}/batch-bransform/\".format(session.default_bucket(),prefix)\n",
    "    },\n",
    "    \n",
    "    # we are using chunk so need to define how and where this file should be appeared.\n",
    "    \"TransformInput\": {\n",
    "        \"ContentType\": \"text/csv\",\n",
    "        \"SplitType\": \"Line\",\n",
    "        \"DataSource\": {\n",
    "            \"S3DataSource\": {\n",
    "                \"S3DataType\": \"S3Prefix\",\n",
    "                \"S3Uri\": test_location,\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    # And lastly we tell SageMaker what sort of compute instance we want to use\n",
    "            \"InstanceType\": \"ml.m4.xlarge\",\n",
    "            \"InstanceCount\": 1\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QMaXE6k3tQLG"
   },
   "source": [
    "### Execute batch transefer job.\n",
    "As above with training, after creating batch transfer job we should define the execution cmmand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a1NEVgbiubQj"
   },
   "outputs": [],
   "source": [
    "transform_response = session.sagemaker_client.create_transform_job(**transform_request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cOUysVh8ubI-"
   },
   "outputs": [],
   "source": [
    "transform_desc = session.wait_for_transform_job(transform_job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s2s9ExoJ_AEs"
   },
   "outputs": [],
   "source": [
    "Y_pred = pd.read_csv(os.path.join(data_dir, 'test.csv.out'), header=None)\n",
    "plt.scatter(Y_test, Y_pred)\n",
    "plt.xlabel(\"Median Price\")\n",
    "plt.ylabel(\"Predicted Price\")\n",
    "plt.title(\"Median Price vs Predicted Price\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SgDJC-yhtk1p"
   },
   "source": [
    "### Result visuzlization\n",
    "Now  Analyze the result by comaring them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wqcGRceVubED"
   },
   "outputs": [],
   "source": [
    "transform_output = \"s3://{}/{}/batch-bransform/\".format(session.default_bucket(),prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S4eRpjzSua8f"
   },
   "outputs": [],
   "source": [
    "!aws s3 cp --recursive $transform_output $data_dir    # take data locally to use it after."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j2B8yGhVt3cB"
   },
   "source": [
    "### Clean up the disk and directory \n",
    "Sometime, when we use deep network then disk will be full and give error for the next operation. Which shold be hard to diagnosed. So, better to clean up the space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "df0AHLmNua3w"
   },
   "outputs": [],
   "source": [
    "# remove all of the files contained in the data_dir directory\n",
    "!rm $data_dir/*\n",
    "\n",
    "# delete the directory itself\n",
    "!rmdir $data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lqrUT1T-FTMU"
   },
   "outputs": [],
   "source": [
    "# \"Keep Learning, Enjoy Empowering\" @dave117"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Boston_House(AWS-SageMaker.Custom_Batch-Transform).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
