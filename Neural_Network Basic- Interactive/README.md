This Part contains following files
---

[1] [First code of deeplearning](https://github.com/vedantdave77/project.Orca/blob/master/Neural_Network%20Basic-%20Interactive/First_Code_of_DeepLearning.ipynb) : have basic information of using pytorch mathematic formulas and information to implement weight, input and sigmoid function for output.

[2] [Multilayer_Perceptron.ipynb] (https://github.com/vedantdave77/project.Orca/blob/master/Neural_Network%20Basic-%20Interactive/Multilayer_Perceptron.ipynb) : Perceptron theorm for neural network to implement each layer output.

[3] [Hyper-Parameter_effect_on_nn.ipynb](https://github.com/vedantdave77/project.Orca/blob/master/Neural_Network%20Basic-%20Interactive/Hyper-Parameter_effect_on_nn.ipynb): Implement simple algorithm to calculate training loss and try to get best answer by tuning parameters.

[4] [BackPropagation Implementation (Manual Logic).ipynb](https://github.com/vedantdave77/project.Orca/blob/master/Neural_Network%20Basic-%20Interactive/BackPropagation%20Implementation%20(Manual%20Logic).ipynb) : Use back propagation to update weigths and parameters for improving accuracy, and reduce trainng loss over time.

[5] [Predicting_Model_&_Improvement(Validation_DropOut).ipynb](https://github.com/vedantdave77/project.Orca/blob/master/Neural_Network%20Basic-%20Interactive/Predicting_Model_%26_Improvement(Validation_DropOut).ipynb): Fashion - Its Fashion MNIST dataset project, where I implment validation approach to generalize network before checking for accuracy with testing dataset. This improves my result of tesing accuracy over 10 percent. I learn about effect drop out and how it is effective to reduce overfiting scenario. I also took reference from 2012 - UoT's research paper on drop out effect and imporvement in deep learning.

---
Thank you - #keep learning, Enjoy Empowering. @dave117
