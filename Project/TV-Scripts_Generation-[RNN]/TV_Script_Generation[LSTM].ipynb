{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TV_Script-Generation[LSTM].ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "19xUQZpFvp2-j_bhU5RFL3Kno2-8CMOFD",
      "authorship_tag": "ABX9TyM8gL1w2W8AqN4ycmmATV6i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vedantdave77/project.Orca/blob/master/Project/TV-Scripts_Generation-%5BRNN%5D/TV_Script_Generation%5BLSTM%5D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-AtJ1WXWVPXz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "d5bcca55-a6c9-419e-de49-a2af3782f090"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76FS_6uGvaYt",
        "colab_type": "text"
      },
      "source": [
        "# TV Script Generation using RNN. \n",
        "\n",
        "AIM:\n",
        "\n",
        "My main aim is to work with kaggle dataset [\"seinfeld-chronic\"](https://www.kaggle.com/thec03u5/seinfeld-chronicles#scripts.csv) dataset. Which is taken from 9 different sensors.\n",
        "\n",
        "A dataset for textual analysis on arguably the best written comedy television show ever. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbfpqJGBwrko",
        "colab_type": "text"
      },
      "source": [
        "## Import Libraries\n",
        "\n",
        "Import required libraries, I will use PyTorch for deep learning. I will use gpu for training. The current gpu provided by Colab is Tesla - T4. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNHgCYTiwq5C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "70b88896-b8c1-460b-8647-cad16a883a69"
      },
      "source": [
        "import os\n",
        "import pickle\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot\n",
        "%matplotlib inline\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from collections import Counter\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# check gpu for the later use \n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "if not train_on_gpu:\n",
        "  print('No gpu available, process takes more time. Use less iterations.')\n",
        "else:\n",
        "  print('GPU available : |Name| : {}'.format(torch.cuda.get_device_name(0)))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU available : |Name| : Tesla T4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMwHMTd9vaVQ",
        "colab_type": "text"
      },
      "source": [
        "## Data Exploration \n",
        "\n",
        "The data file is Seindeld movie script, which is in form of text. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CN6LuTWxvaA-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "outputId": "462c031f-ef83-4ba8-81dd-f93319e3a82a"
      },
      "source": [
        "# Get and Read the Data\n",
        "input_file = '/content/Seinfeld_Scripts.txt'\n",
        "\n",
        "with open(input_file, 'r', encoding='utf8') as file:\n",
        "  input_data = file.read()\n",
        "\n",
        "text = input_data\n",
        "# can not read whole dataset at a time so, try to show sample \n",
        "print('Text Exploration')\n",
        "print('-----------------')\n",
        "print('No. of unique words {}'.format(len({word: None for word in text.split()})))\n",
        "\n",
        "\n",
        "line_range = (0,10)\n",
        "lines = text.split('\\n')\n",
        "print('Number of Lines : {}'.format(len(lines)))\n",
        "\n",
        "word_count_line = [len(line.split()) for line in lines]\n",
        "print('average words per line {}'.format(np.average(word_count_line)))\n",
        "\n",
        "print('====================================================================')\n",
        "print('The lines {} to {}:'.format(*line_range))\n",
        "print('\\n'.join(text.split('\\n')[line_range[0]:line_range[1]]))\n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Text Exploration\n",
            "-----------------\n",
            "No. of unique words 46367\n",
            "Number of Lines : 109233\n",
            "average words per line 5.544240293684143\n",
            "====================================================================\n",
            "The lines 0 to 10:\n",
            "jerry: do you know what this is all about? do you know, why were here? to be out, this is out...and out is one of the single most enjoyable experiences of life. people...did you ever hear people talking about we should go out? this is what theyre talking about...this whole thing, were all out now, no one is home. not one person here is home, were all out! there are people trying to find us, they dont know where we are. (on an imaginary phone) did you ring?, i cant find him. where did he go? he didnt tell me where he was going. he must have gone out. you wanna go out you get ready, you pick out the clothes, right? you take the shower, you get all ready, get the cash, get your friends, the car, the spot, the reservation...then youre standing around, what do you do? you go we gotta be getting back. once youre out, you wanna get back! you wanna go to sleep, you wanna get up, you wanna go out again tomorrow, right? where ever you are in life, its my feeling, youve gotta go. \n",
            "\n",
            "jerry: (pointing at georges shirt) see, to me, that button is in the worst possible spot. the second button literally makes or breaks the shirt, look at it. its too high! its in no-mans-land. you look like you live with your mother. \n",
            "\n",
            "george: are you through? \n",
            "\n",
            "jerry: you do of course try on, when you buy? \n",
            "\n",
            "george: yes, it was purple, i liked it, i dont actually recall considering the buttons. \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mb3_eEYZ2IKd",
        "colab_type": "text"
      },
      "source": [
        "## Data Preprocess\n",
        "\n",
        "Preprocess data for converting  text to lookup table and convert special characters to tokens. \n",
        "\n",
        "lookup table will convert data to **index : word** format. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNDGPtuYldlV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create lookup table (generate vocab dictionary word : int and int :word)\n",
        "def create_lookup_tables(text):\n",
        "  word_counts = Counter(text)\n",
        "  # sorting word from most to least words \n",
        "  sorted_vocab = sorted(word_counts,key=word_counts.get, reverse = True)\n",
        "  # create desired dictionaries\n",
        "  int_to_vocab = {index:word for index,word in enumerate(sorted_vocab)}\n",
        "  vocab_to_int = {word:index for index,word in int_to_vocab.items()}\n",
        "  return (vocab_to_int,int_to_vocab)\n",
        "\n",
        "# still, each line has the special characters and required to tranfer as words \n",
        "def token_lookup():\n",
        "  tokens = dict()\n",
        "  tokens['.'] = '<PERIOD>'\n",
        "  tokens[','] = '<COMMA>'\n",
        "  tokens['\"'] = '<QUOTATION_MARK>'\n",
        "  tokens[';'] = '<SEMICOLON>'\n",
        "  tokens['!'] = '<EXCLAMATION_MARK>'\n",
        "  tokens['?'] = '<QUESTION_MARK>'\n",
        "  tokens['('] = '<LEFT_PAREN>'\n",
        "  tokens[')'] = '<RIGHT_PAREN>'\n",
        "  tokens['?'] = '<QUESTION_MARK>'\n",
        "  tokens['-'] = '<DASH>'\n",
        "  tokens['\\n'] = '<NEW_LINE>'\n",
        "  return tokens\n",
        "\n",
        "special_words = {'PADDING': '<PAD>'}"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnOEduCVBJ_z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# preprocess data and save it for later use \n",
        "text = input_data\n",
        "text = text[81:]\n",
        "token_dict = token_lookup()\n",
        "for key,token in token_dict.items():\n",
        "  text = text.replace(key, '{}'.format(token))\n",
        "\n",
        "text = text.lower()\n",
        "text = text. split()\n",
        "\n",
        "vocab_to_int, int_to_vocab = create_lookup_tables(text + list(special_words.values()))\n",
        "int_text = [vocab_to_int[word] for word in text]\n",
        "pickle.dump((int_text, vocab_to_int, int_to_vocab, token_dict ), open('preprocess.p','wb'))"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ad4q7BP-CiZh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load saved processed dataset  \n",
        "int_text, vocab_to_int, int_to_vocab, token_dict=pickle.load(open('preprocess.p','rb'))"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lBK4FQ--zHK",
        "colab_type": "text"
      },
      "source": [
        "Generate small batches of words for deep learning model. Its convenient to use batch training. \n",
        "\n",
        "Then, I will generate dataloader by converting data to tensor dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpKj_KoAF95p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# generate dataloader with batches.\n",
        "\n",
        "def batch_data(words, sequence_length, batch_size):\n",
        "  n_batches = len(words)//batch_size\n",
        "  words = words[:n_batches*batch_size]       # words related to full batches \n",
        "  y_len = len(words) - sequence_length       # decide no. of rotation (50 word - 5 seq_length = 45 y_len, means 45 times it will generate loop steps)\n",
        "  x,y = [], []\n",
        "  for idx in range(0,y_len):\n",
        "    idx_end = sequence_length + idx\n",
        "    x_batch = words[idx:idx_end]\n",
        "    x.append(x_batch)\n",
        "    batch_y = words[idx_end]\n",
        "    y.append(batch_y)\n",
        "\n",
        "  data = TensorDataset(torch.from_numpy(np.asarray(x)),torch.from_numpy(np.asarray(y)))\n",
        "  data_loader = DataLoader(data, shuffle= False, batch_size = batch_size)\n",
        "  return data_loader \n"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anXlE2jd_6Ny",
        "colab_type": "text"
      },
      "source": [
        "## Model Architecture.\n",
        "\n",
        "Model architecture consist embedding layers, hidden layers and output layer. \n",
        "\n",
        "Here, for RNN I am going to use LSTM cell. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UK4deDDJ1HK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define RNN Architecture\n",
        "\n",
        "class RNN(nn.Module):\n",
        "  def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout= 0.5, lr=0.001):\n",
        "    super(RNN,self).__init__()\n",
        "    self.output_size = output_size\n",
        "    self.n_layers = n_layers\n",
        "    self.hidden_dim = hidden_dim\n",
        "\n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "    self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=dropout, batch_first=True)\n",
        "    self.fc = nn.Linear(hidden_dim, output_size)\n",
        "\n",
        "  def forward(self,nn_input, hidden):\n",
        "    batch_size = nn_input.size(0)\n",
        "    embeds = self.embedding(nn_input)\n",
        "    lstm_out, hidden = self.lstm(embeds, hidden)\n",
        "    lstm_out = lstm_out.contiguous().view(-1,self.hidden_dim)\n",
        "    out = self.fc(lstm_out)\n",
        "    out = out.view(batch_size, -1, self.output_size)\n",
        "    out = out[:, -1]\n",
        "    return out, hidden\n",
        "\n",
        "  def init_hidden(self, batch_size):\n",
        "    weight = next(self.parameters()).data\n",
        "    if (train_on_gpu):\n",
        "      hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(), weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
        "    else:\n",
        "      hidden = (weight.new(self.n_layers,batch_size, self.hidden_dim).zero_(), weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
        "    return hidden\n"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpwYhP6_J1N_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generate forward backward propagation for training loop\n",
        "\n",
        "def forward_back_prop(rnn, optimizer, criterion, input, target, hidden):\n",
        "  if(train_on_gpu):\n",
        "    rnn.cuda()\n",
        "  \n",
        "  h = tuple([each.data for each in hidden])\n",
        "  rnn.zero_grad()\n",
        "  if(train_on_gpu):\n",
        "    inputs, target = input.cuda(),target.cuda()\n",
        "\n",
        "  output,h = rnn(inputs,h)\n",
        "  loss = criterion(output,target)\n",
        "  loss.backward()\n",
        "  nn.utils.clip_grad_norm_(rnn.parameters(),5)                    # prevent exploding gradinet with gradient clipping method.\n",
        "  optimizer.step()\n",
        "  return loss.item(), h"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NDgsciRJ1LH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generate Training Loop\n",
        "def TV_script_RNN(rnn, batch_size, optimizer, criterion, n_epochs, show_every = 100):\n",
        "  batch_losses = []\n",
        "  rnn.train()\n",
        "  print('Training for the {} epochs ... '.format(n_epochs))\n",
        "\n",
        "  for epoch in range(1, n_epochs+1):\n",
        "    hidden = rnn.init_hidden(batch_size)\n",
        "    for batch_i, (inputs,labels) in enumerate(train_loader, 1):\n",
        "      n_batches = len(train_loader.dataset)//batch_size\n",
        "      if(batch_i > n_batches):\n",
        "        break\n",
        "      loss, hidden = forward_back_prop(rnn,optimizer,criterion,inputs,labels, hidden)\n",
        "      batch_losses.append(loss)\n",
        "      # print required updatae status\n",
        "      if batch_i % show_every == 0:\n",
        "        print('Epoch : {:>4}/{:<4} and Loss: {}\\n'.format(epoch, n_epochs, np.average(batch_losses)))\n",
        "  return rnn\n"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8Euxc4HJ1Eb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "81120d4e-0058-4fc0-8363-98cdf53a9133"
      },
      "source": [
        "# Define Hyper Parameters\n",
        "sequence_length = 10\n",
        "batch_size = 128\n",
        "\n",
        "train_loader = batch_data(int_text, sequence_length, batch_size)\n",
        "\n",
        "# Training Parameters\n",
        "num_epochs = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Model Parameters\n",
        "vocab_size = len(vocab_to_int)\n",
        "print(vocab_size)\n",
        "output_size = vocab_size\n",
        "embedding_dim = 200\n",
        "hidden_dim = 250\n",
        "n_layers = 2         \n",
        "show_every = 2000"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "46713\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXtBfh44TZSN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        },
        "outputId": "e7ee2ac4-b00d-460c-9871-6d549725bd13"
      },
      "source": [
        "# Let's Train the model\n",
        "rnn = RNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout= 0.5)\n",
        "if train_on_gpu:\n",
        "  rnn.cuda()\n",
        "\n",
        "optimizer = torch.optim.Adam(rnn.parameters(),lr=learning_rate)\n",
        "criterion =nn.CrossEntropyLoss()\n",
        "\n",
        "# training the model\n",
        "TV_script_RNN_Model_1 = TV_script_RNN(rnn, batch_size, optimizer, criterion, num_epochs, show_every)\n",
        "\n",
        "saved_model = os.path.splitext(os.path.basename('RNN_Model_1'))[0] + '.pt'\n",
        "torch.save(TV_script_RNN,saved_model)\n",
        "print('Model Trained and Saved')\n",
        "\n",
        "# Model is already trained previously, so I called it from my drive"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training for the 10 epochs ... \n",
            "Epoch :    1/10   and Loss: 7.066025070190429\n",
            "\n",
            "Epoch :    1/10   and Loss: 6.893861082911491\n",
            "\n",
            "Epoch :    2/10   and Loss: 6.538230449838057\n",
            "\n",
            "Epoch :    2/10   and Loss: 6.362951337731308\n",
            "\n",
            "Epoch :    3/10   and Loss: 6.134565952415033\n",
            "\n",
            "Epoch :    3/10   and Loss: 6.008999293644042\n",
            "\n",
            "Epoch :    4/10   and Loss: 5.841501305661429\n",
            "\n",
            "Epoch :    4/10   and Loss: 5.746286462610275\n",
            "\n",
            "Epoch :    5/10   and Loss: 5.6169854751280575\n",
            "\n",
            "Epoch :    5/10   and Loss: 5.539699204554733\n",
            "\n",
            "Epoch :    6/10   and Loss: 5.434922536902028\n",
            "\n",
            "Epoch :    6/10   and Loss: 5.370780675148231\n",
            "\n",
            "Epoch :    7/10   and Loss: 5.28334653884506\n",
            "\n",
            "Epoch :    7/10   and Loss: 5.229114390406217\n",
            "\n",
            "Epoch :    8/10   and Loss: 5.15482465223314\n",
            "\n",
            "Epoch :    8/10   and Loss: 5.107828242674056\n",
            "\n",
            "Epoch :    9/10   and Loss: 5.0437797542377165\n",
            "\n",
            "Epoch :    9/10   and Loss: 5.002669688033556\n",
            "\n",
            "Epoch :   10/10   and Loss: 4.9462711167116185\n",
            "\n",
            "Epoch :   10/10   and Loss: 4.909681207590511\n",
            "\n",
            "Model Trained and Saved\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYTcJ8tHVIiz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "91d5f98f-1aa0-4604-ebf1-28214ce624a2"
      },
      "source": [
        "saved_model = os.path.splitext(os.path.basename('/content/drive/My Drive/RNN_Model_1'))[0] + '.pt'\n",
        "torch.save(TV_script_RNN,saved_model)\n",
        "print('Model Trained and Saved')"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model Trained and Saved\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEGAwiLmVR3Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load model\n",
        "load_model = os.path.splitext(os.path.basename('/content/drive/My Drive/RNN_Model_1'))[0] + '.pt'\n",
        "TV_script_RNN_Model = torch.load(load_model)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVvI5IE_sYW-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TV_script_RNN_Model_1.load_state_dict(torch.load('/content/drive/My Drive/RNN_Model_1')['state_dict'])\n",
        "print(TV_script_RNN_Model_1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6r_yUdNRQnC",
        "colab_type": "text"
      },
      "source": [
        "## Generate TV Script\n",
        "\n",
        "With the network trained and saved, I will use it to generate a new, \"fake\" Seinfeld TV script in this section."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-R-Jkv8Q-wx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# generate function for generating text\n",
        "def generate(rnn, prime_id, int_to_vocab, token_dict, pad_value, prediction_length = 100):\n",
        "  rnn.eval()\n",
        "\n",
        "  current_seq = np.full((1,sequence_length),pad_value)\n",
        "  current_seq[-1][-1] = prime_id\n",
        "  predicted = [int_to_vocab[prime_id]]\n",
        "\n",
        "  for _ in range(prediction_length):\n",
        "    if train_on_gpu:\n",
        "      current_seq = torch.LongTensor(current_seq).cuda()\n",
        "    else:\n",
        "      current_seq = torch.LongTensor(current_seq)\n",
        "    \n",
        "    # initiate hidden state\n",
        "    hidden = rnn.init_hidden(current_seq.size(0))\n",
        "\n",
        "    # get the output of rnn \n",
        "    output, _ = rnn(current_seq, hidden)\n",
        "\n",
        "    # get next word probability \n",
        "    p = F.softmax(output, dim=1).data\n",
        "    if train_on_gpu:\n",
        "      p = p.cpu()\n",
        "  \n",
        "    # Use top k sample for the next index word\n",
        "    top_k = 5\n",
        "    p, top_i = p.topk(top_k)\n",
        "    top_i =  top_i.numpy().squeeze()\n",
        "\n",
        "    # select the likely next word index with some of the element \n",
        "    p = p.numpy().squeeze()\n",
        "    word_i = np.random.choice(top_i, p=p/p.sum())\n",
        "\n",
        "    #  retrieve that word from dictionary\n",
        "    word = int_to_vocab[word_i]\n",
        "    predicted.append(word)\n",
        "\n",
        "    # generate newxt word for thesequence and continue it \n",
        "    current_seq = np.roll(current_seq.cpu(), -1,1)\n",
        "    current_seq[-1][-1] = word_i\n",
        "\n",
        "  gen_sentences = ' '.join(predicted)\n",
        "\n",
        "  # replace the punctuation\n",
        "  for key,token in token_dict.items():\n",
        "    ending = ' ' if key in ['\\n','(', '\"'] else ''\n",
        "    gen_sentences = gen_sentences.replace(token.lower(), key)\n",
        "  gen_sentences = gen_sentences.replace('\\n ', '\\n')\n",
        "  gen_sentences = gen_sentences.replace('( ','(')\n",
        "  \n",
        "  return gen_sentences\n"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nlXEMgIScJa5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "98e1f42d-a072-4ef6-d44c-d5c552453a4d"
      },
      "source": [
        "# Generate New Script\n",
        "gen_length = 400         # prediction_length\n",
        "prime_word = 'jerry'\n",
        "pad_word = special_words['PADDING']\n",
        "\n",
        "generated_script = generate(TV_script_RNN_Model_1, vocab_to_int[prime_word + ':'], int_to_vocab, token_dict, vocab_to_int[pad_word], gen_length)\n",
        "print(generated_script)"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "jerry: show's on, moops? you know what? what was it? \n",
            "\n",
            "george: well, i think it's better than a woman on the contrary, \n",
            "\n",
            "jerry: you know you're going to be hungry accountable. and the other way of year. \n",
            "\n",
            "elaine: what happened? \n",
            "\n",
            "jerry: i can't believe this was going on. \n",
            "\n",
            "george: i think i was a little bit nicer. \n",
            "\n",
            "george: oh, i can't. it's the moops. \n",
            "\n",
            "jerry: so you don't think so. \n",
            "\n",
            "kramer: well, that's the most stand. \n",
            "\n",
            "elaine: you don't know how you were getting any more? and i can get uromycitisis poisoning for you. \n",
            "\n",
            "george: what are we gonna do? you got the whole one that blocked me in a lady. \n",
            "\n",
            "jerry: i thought you can do that. \n",
            "\n",
            "george: what did he say? \n",
            "\n",
            "elaine: it's a big adjustment. \n",
            "\n",
            "elaine: i don't know. \n",
            "\n",
            "jerry: you know what the hell was it? \n",
            "\n",
            "george: it's the most stand. \n",
            "\n",
            "george: so i can tell you, how did you get the hell to paris? \n",
            "\n",
            "kramer: oh, it's not an accident. \n",
            "\n",
            "elaine: what is that? \n",
            "\n",
            "elaine: oh, it's a good time ago. \n",
            "\n",
            "jerry: what is that a balk?! or the incident department? the buttons war. the state of the garden is the only time, it's not a lovely getaway. it's modeled and chagrined, mortified and stupefied. and then they have been dating. \n",
            "\n",
            "kruger: i don't have any ideas. \n",
            "\n",
            "jerry: what is that? \n",
            "\n",
            "kramer: it's not complicated. \n",
            "\n",
            "elaine: i cheated to the bathroom in the building. \n",
            "\n",
            "elaine: well, it's not a fault. \n",
            "\n",
            "george: you can't tell anybody. i think it's a great idea. \n",
            "\n",
            "elaine: oh, my god! \n",
            "\n",
            "jerry: how did it happen? \n",
            "\n",
            "george: i don't know. \n",
            "\n",
            "elaine: i think we have a good idea. \n",
            "\n",
            "george: you know what? i was just wondering if i was in snitzer's bakery for ruining that law. \n",
            "\n",
            "rivera: and i was just reeling for a private jet? but the whole thing was a lot of sense. \n",
            "\n",
            "jerry: what do we want you for? \n",
            "\n",
            "kramer: yeah, i can't believe you had to worry about. you know, i think this whole is in a parking lot of prostitutes in a bubble. \n",
            "\n",
            "prostitute: \n",
            "\n",
            "jerry: oy. \n",
            "\n",
            "jerry: so i think it's a big idea. \n",
            "\n",
            "elaine: oh, it's a good time, i don't think so. \n",
            "\n",
            "elaine: i don't think so. \n",
            "\n",
            "george: what do you think i could do it. \n",
            "\n",
            "elaine: you know, it's a good idea. it's just a good time exciting session. \n",
            "\n",
            "george: you know, the whole\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZRz6DE_ZXoM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}