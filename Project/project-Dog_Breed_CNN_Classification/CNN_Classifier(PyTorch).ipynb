{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN_Dog_Breed_Recognizer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP+ektlXVdBPHmz8gCg5z5t",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vedantdave77/project.Orca/blob/master/Project/project-Dog_Breed_CNN_Classification/CNN_Classifier(PyTorch).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9UX-6DFXWV8",
        "colab_type": "text"
      },
      "source": [
        "# CNN From Scratch \n",
        "\n",
        "---\n",
        "\n",
        "### Embeding Google Drive for Data Access\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xyiUQkS3SUL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "e5a3c9c5-c270-4722-88d4-4085f3ff2693"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvFxB-eQKT-R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set parameters which I usually used during project \n",
        "norm_mean = [0.485, 0.456, 0.406]\n",
        "norm_std = [0.229, 0.224, 0.225]\n",
        "img_short_side_resize = 256\n",
        "img_input_size = 224\n",
        "shuffle = True\n",
        "num_workers = 16\n",
        "batch_size = 64"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZZd8GP7Abbk",
        "colab_type": "text"
      },
      "source": [
        "## Data : [Access/ Transform/ Load]\n",
        "\n",
        "Reference: \n",
        "1. [Data Loader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)\n",
        "\n",
        "2. [Customize Data Set Operation](https://pytorch.org/docs/stable/torchvision/datasets.html)\n",
        "\n",
        "3. [Data Transform](https://pytorch.org/docs/stable/torchvision/transforms.html?highlight=transform)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Eprz38e6vwF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import required libraries \n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from PIL import Image\n",
        "from PIL import ImageFile\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True                                          # cutting image (specially, short of edge)\n",
        "\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import datasets \n",
        "\n",
        "# define data_transformation and batch_size \n",
        "transform_train = transforms.Compose([\n",
        "                                      transforms.Resize(img_short_side_resize),\n",
        "                                      transforms.ColorJitter(brightness = 0.2, contrast = 0.2, saturation =0.2, hue = 0.1),\n",
        "                                      transforms.RandomHorizontalFlip(),\n",
        "                                      transforms.RandomResizedCrop(img_input_size, scale=(0.08,1), ratio = (1,1)),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize(mean = norm_mean, std = norm_std)\n",
        "                                       ])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "                                     transforms.Resize(img_input_size),\n",
        "                                     transforms.FiveCrop(img_input_size),\n",
        "                                     transforms.Lambda(lambda crops: torch.stack([\n",
        "                                                                                  transforms.Compose([\n",
        "                                                                                                      transforms.ToTensor(),\n",
        "                                                                                                      transforms.Normalize(mean = norm_mean, std = norm_std)])(crop) for crop in crops\n",
        "                                                                                  ]))\n",
        "\n",
        "                                    ])\n",
        "\n",
        "# load data (define datasets)\n",
        "train_data = datasets.ImageFolder(\"/content/drive/My Drive/Data /dogImages/train/\",transform_train)\n",
        "valid_data = datasets.ImageFolder(\"/content/drive/My Drive/Data /dogImages/valid/\",transform_test)\n",
        "test_data  = datasets.ImageFolder(\"/content/drive/My Drive/Data /dogImages/test/\",transform_test)\n",
        "\n",
        "# separate imput and labels(classes)\n",
        "data = {\"train\" : train_data, \"valid\" : valid_data, \"test\" : test_data}\n",
        "n_classes = len(train_data.classes)\n",
        "\n",
        "# create loaders (train, valid, test)\n",
        "train_loader = torch.utils.data.DataLoader(data[\"train\"], batch_size = batch_size, num_workers = num_workers, shuffle = shuffle, pin_memory = True)\n",
        "valid_loader = torch.utils.data.DataLoader(data[\"valid\"], batch_size = int(np.floor(batch_size/5)), num_workers=0, shuffle = shuffle, pin_memory = True) \n",
        "test_loader = torch.utils.data.DataLoader(data[\"test\"], batch_size = int(np.floor(batch_size/5)), num_workers=0, shuffle = shuffle, pin_memory = True)\n",
        "\n",
        "# loader dictionary\n",
        "loaders_dict = {\"train\" : train_loader, \"valid\" : valid_loader, \"test\" : test_loader}"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhDkeBbHaVbu",
        "colab_type": "text"
      },
      "source": [
        "## Model Architecture :\n",
        "### Create CNN Classifier \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQv6r6ZHIYwI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# generate the CNN Architecture from Scratch\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self,n_classes,layer1_depth = 32):\n",
        "        super(CNN, self).__init__()\n",
        "        \n",
        "        # define layer wise depth\n",
        "        layer2_depth = layer1_depth *2                                         # 32 --> 64\n",
        "        layer3_depth = layer2_depth *2                                         # 64 --> 128\n",
        "\n",
        "        # define Max-pooling layer\n",
        "        self.pool = nn.MaxPool2d(2,2)\n",
        "\n",
        "        # convolution set 1 \n",
        "        self.conv1_1 = nn.Conv2d(3, layer1_depth, 3, stride=1, padding=1)\n",
        "        self.conv1_2 = nn.Conv2d(layer1_depth, layer1_depth, 3, stride=1, padding=1)\n",
        "        self.conv1_3 = nn.Conv2d(layer1_depth, layer1_depth, 3, stride=1, padding=1)\n",
        "        self.bn1_1 = nn.BatchNorm2d(layer1_depth)\n",
        "        self.bn1_2 = nn.BatchNorm2d(layer1_depth)\n",
        "        self.bn1_3 = nn.BatchNorm2d(layer1_depth)\n",
        "\n",
        "        # convolution set 2 \n",
        "        self.conv2_1 = nn.Conv2d(layer1_depth, layer2_depth, 3, stride=1, padding=1)\n",
        "        self.conv2_2 = nn.Conv2d(layer2_depth, layer2_depth, 3, stride=1, padding=1)\n",
        "        self.conv2_3 = nn.Conv2d(layer2_depth, layer2_depth, 3, stride=1, padding=1)\n",
        "        self.bn2_1 = nn.BatchNorm2d(layer2_depth)\n",
        "        self.bn2_2 = nn.BatchNorm2d(layer2_depth)\n",
        "        self.bn2_3 = nn.BatchNorm2d(layer2_depth)\n",
        "\n",
        "        # convolution set 3 \n",
        "        self.conv3_1 = nn.Conv2d(layer2_depth, layer3_depth, 3, stride=1, padding=1)\n",
        "        self.conv3_2 = nn.Conv2d(layer3_depth, layer3_depth, 3, stride=1, padding=1)\n",
        "        self.conv3_3 = nn.Conv2d(layer3_depth, layer3_depth, 3, stride=1, padding=1)\n",
        "        self.bn3_1 = nn.BatchNorm2d(layer3_depth)\n",
        "        self.bn3_2 = nn.BatchNorm2d(layer3_depth)\n",
        "        self.bn3_3 = nn.BatchNorm2d(layer3_depth)\n",
        "\n",
        "        # output \n",
        "        self.output = nn.Linear(layer3_depth,n_classes)                         # 128 ---> 133\n",
        "\n",
        "        # Initialize weight\n",
        "        nn.init.kaiming_normal_(self.conv1_1.weight, nonlinearity='relu')\n",
        "        nn.init.kaiming_normal_(self.conv1_2.weight, nonlinearity='relu')\n",
        "        nn.init.kaiming_normal_(self.conv1_3.weight, nonlinearity='relu')\n",
        "        nn.init.kaiming_normal_(self.conv2_1.weight, nonlinearity='relu')\n",
        "        nn.init.kaiming_normal_(self.conv2_2.weight, nonlinearity='relu')\n",
        "        nn.init.kaiming_normal_(self.conv2_3.weight, nonlinearity='relu')\n",
        "        nn.init.kaiming_normal_(self.conv3_1.weight, nonlinearity='relu')\n",
        "        nn.init.kaiming_normal_(self.conv3_2.weight, nonlinearity='relu') \n",
        "        nn.init.kaiming_normal_(self.conv3_3.weight, nonlinearity='relu')       \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "        # Conv Flow 1\n",
        "        x = F.relu(self.bn1_1(self.conv1_1(x)))\n",
        "        x = F.relu(self.bn1_2(self.conv1_2(x)))\n",
        "        x = F.relu(self.bn1_3(self.conv1_3(x)))\n",
        "        x = self.pool(x)\n",
        "\n",
        "        # Conv Flow 2\n",
        "        x = F.relu(self.bn2_1(self.conv2_1(x)))\n",
        "        x = F.relu(self.bn2_2(self.conv2_2(x)))\n",
        "        x = F.relu(self.bn2_3(self.conv2_3(x)))\n",
        "        x = self.pool(x)\n",
        "\n",
        "        # Conv Flow 3\n",
        "        x = F.relu(self.bn3_1(self.conv3_1(x)))\n",
        "        x = F.relu(self.bn3_2(self.conv3_2(x)))\n",
        "        x = F.relu(self.bn3_3(self.conv3_3(x)))\n",
        "        x = self.pool(x)\n",
        "\n",
        "        # fuse the dimension (height=2, width =3)\n",
        "        x = x.view(x.size(0),x.size(1),-1)\n",
        "        x = x.max(2)[0]\n",
        "\n",
        "        # output \n",
        "        x = self.output(x)\n",
        "        \n",
        "        return x"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kXE60g40WDL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "42c2d359-41c0-4283-8e87-1c340cccf41f"
      },
      "source": [
        "Conv_model = CNN(n_classes)\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "if not use_cuda:\n",
        "    print('CUDA is not available.  Training on CPU ...')\n",
        "    device = \"cpu\"\n",
        "else:\n",
        "    print('CUDA is available!  Training on GPU ...')\n",
        "    device = torch.device(\"cuda:0\")\n",
        "    print(\"Using\",torch.cuda.get_device_name(device))\n",
        "    \n",
        "\n",
        "if use_cuda:\n",
        "    Conv_model.cuda()           "
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CUDA is available!  Training on GPU ...\n",
            "Using Tesla K80\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apH396TAH3w2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Now for the learning rate. Explanation below. \n",
        "learning_rates = 5e-4 * np.logspace(0,1.5,9)                                    # 9 values between log( 0 to 1.5 ) * 5^-4\n",
        "learning_rate = learning_rates[2]"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FRsY0860owJ",
        "colab_type": "text"
      },
      "source": [
        "## Specify loss function and optimizer \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3Gmwm7G0vNk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau \n",
        "\n",
        "# loss function \n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# optimizer \n",
        "optimizer = optim.Adam(Conv_model.parameters(),learning_rate)\n",
        "\n",
        "# learning_rate shedular \n",
        "scheduler = ReduceLROnPlateau(optimizer,'min',verbose=True)\n"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qY0920vQ14Hz",
        "colab_type": "text"
      },
      "source": [
        "## Train and Validate Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvfQGkwd13dl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time \n",
        "def train_epoch(model,train_loader,optimizer,criterion,device):\n",
        "    train_loss = 0.0\n",
        "    model.train()                                                               # define training job\n",
        "    for batch_idx, (data,target) in enumerate(train_loader):                                                                     \n",
        "        data, target = data.to(device),target.to(device)                        # move to CUDA\n",
        "        optimizer.zero_grad()                                                   # reset gradient \n",
        "        output = model(data)                                                    # run model and get output \n",
        "        loss = criterion(output, target)                                        # calculate loss\n",
        "        train_loss += loss.item() * data.size(0)                                # calculate gradients\n",
        "        loss.backward()                                                         # update each layer para value\n",
        "        optimizer.step()\n",
        "    train_loss = train_loss/len(train_loader.dataset)\n",
        "    return model, train_loss"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9Z69MRn3mHt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def valid_epoch(model,valid_loader,criterion,device,fivecrop): \n",
        "    valid_loss = 0.0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for data, target in valid_loader:\n",
        "            data, target = data.to(device), target.to(device)                   # tranfer to CUDA Tesla P 100\n",
        "            if fivecrop == \"mean\":\n",
        "                bs, ncrops, c, h, w = data.size()                               # dimenstion due to 5 crop method              \n",
        "                output = model(data.view(-1,c,h,w))\n",
        "                output = output.view(bs, ncrops, -1).mean(1)\n",
        "            elif fivecrop == \"max\":\n",
        "                bs, ncrops, c,h,w = data.size()\n",
        "                output = model(data.view(-1,c,h,w))\n",
        "                output = output.view(bs, ncrops, -1).max(1)[0] \n",
        "            else:\n",
        "                output = model(data)\n",
        "            \n",
        "            loss = criterion(output, target)                                    # update losses\n",
        "            valid_loss += loss.item() * data.size(0)\n",
        "    valid_loss = valid_loss/len(valid_loader.dataset)\n",
        "    return valid_loss    "
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbS5RIpj5uQE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "22e2af49-f70b-49f5-a022-927fad768dda"
      },
      "source": [
        "def train(n_epochs, loaders_dict, model,optimizer, criterion, device, path_model, fivecrop = None, lr_scheduler = None):\n",
        "    valid_loss_min = np.Inf\n",
        "    train_loss = []\n",
        "    valid_loss = []\n",
        "    # time everything \n",
        "    time_start = time.time()\n",
        "    for epoch in range(1, n_epochs+1): \n",
        "        time_start_epoch = time.time()\n",
        "        \n",
        "        model, train_loss_epoch = train_epoch(model,loaders_dict[\"train\"],optimizer,criterion,device)\n",
        "        train_loss.append(train_loss_epoch)\n",
        "\n",
        "        # validate this epoch \n",
        "        valid_loss_epoch = valid_epoch(model,loaders_dict[\"valid\"],criterion, device, fivecrop)\n",
        "\n",
        "        # call learning rate scheduler \n",
        "        if lr_scheduler is not None:\n",
        "            lr_scheduler.step(valid_loss_epoch)\n",
        "        valid_loss.append(valid_loss_epoch)\n",
        "        \n",
        "        # save model for new lowest validation loss \n",
        "        if valid_loss_epoch <= valid_loss_min:\n",
        "            torch.save(model.state_dict(),path_model)\n",
        "            valid_loss_min = valid_loss_epoch\n",
        "\n",
        "        print(\"Epoch {} done in {:.2f} seconds. \\t | Training Loss: {:.3f} \\t | Validation Loss: {:.3f}\".format(\n",
        "            epoch, time.time() - time_start_epoch, train_loss_epoch, valid_loss_epoch))\n",
        "    print(f\"{n_epochs} epochs ready in {(time.time() - time_start):.3f} seconds. Minimum validation loss: {valid_loss_min:.3f}\")\n",
        "    model.load_state_dict(torch.load(path_model))\n",
        "    return model\n",
        "        \n",
        "# give real training action \n",
        "Conv_model_1 = train(100, loaders_dict, Conv_model, optimizer, criterion, device, 'Conv_model.pt', fivecrop = \"mean\", lr_scheduler = scheduler)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 done in 206.86 seconds. \t | Training Loss: 4.577 \t | Validation Loss: 4.606\n",
            "Epoch 2 done in 202.94 seconds. \t | Training Loss: 4.499 \t | Validation Loss: 4.576\n",
            "Epoch 3 done in 207.09 seconds. \t | Training Loss: 4.454 \t | Validation Loss: 4.425\n",
            "Epoch 4 done in 200.14 seconds. \t | Training Loss: 4.393 \t | Validation Loss: 4.345\n",
            "Epoch 5 done in 208.64 seconds. \t | Training Loss: 4.350 \t | Validation Loss: 4.314\n",
            "Epoch 6 done in 205.22 seconds. \t | Training Loss: 4.290 \t | Validation Loss: 4.342\n",
            "Epoch 7 done in 203.23 seconds. \t | Training Loss: 4.228 \t | Validation Loss: 4.176\n",
            "Epoch 8 done in 199.71 seconds. \t | Training Loss: 4.167 \t | Validation Loss: 4.248\n",
            "Epoch 9 done in 203.20 seconds. \t | Training Loss: 4.106 \t | Validation Loss: 4.048\n",
            "Epoch 10 done in 206.10 seconds. \t | Training Loss: 4.034 \t | Validation Loss: 4.178\n",
            "Epoch 11 done in 200.50 seconds. \t | Training Loss: 3.955 \t | Validation Loss: 3.974\n",
            "Epoch 12 done in 204.41 seconds. \t | Training Loss: 3.859 \t | Validation Loss: 3.824\n",
            "Epoch 13 done in 202.42 seconds. \t | Training Loss: 3.790 \t | Validation Loss: 3.842\n",
            "Epoch 14 done in 208.53 seconds. \t | Training Loss: 3.759 \t | Validation Loss: 3.689\n",
            "Epoch 15 done in 215.22 seconds. \t | Training Loss: 3.657 \t | Validation Loss: 3.624\n",
            "Epoch 16 done in 207.65 seconds. \t | Training Loss: 3.604 \t | Validation Loss: 3.702\n",
            "Epoch 17 done in 209.26 seconds. \t | Training Loss: 3.508 \t | Validation Loss: 3.462\n",
            "Epoch 18 done in 209.23 seconds. \t | Training Loss: 3.471 \t | Validation Loss: 3.336\n",
            "Epoch 19 done in 211.08 seconds. \t | Training Loss: 3.395 \t | Validation Loss: 3.500\n",
            "Epoch 20 done in 209.93 seconds. \t | Training Loss: 3.325 \t | Validation Loss: 3.120\n",
            "Epoch 21 done in 203.73 seconds. \t | Training Loss: 3.249 \t | Validation Loss: 3.081\n",
            "Epoch 22 done in 205.48 seconds. \t | Training Loss: 3.225 \t | Validation Loss: 3.031\n",
            "Epoch 23 done in 202.95 seconds. \t | Training Loss: 3.141 \t | Validation Loss: 3.036\n",
            "Epoch 24 done in 206.40 seconds. \t | Training Loss: 3.075 \t | Validation Loss: 3.004\n",
            "Epoch 25 done in 204.07 seconds. \t | Training Loss: 3.021 \t | Validation Loss: 3.052\n",
            "Epoch 26 done in 203.06 seconds. \t | Training Loss: 2.961 \t | Validation Loss: 2.997\n",
            "Epoch 27 done in 204.56 seconds. \t | Training Loss: 2.930 \t | Validation Loss: 2.667\n",
            "Epoch 28 done in 206.96 seconds. \t | Training Loss: 2.854 \t | Validation Loss: 2.671\n",
            "Epoch 29 done in 207.71 seconds. \t | Training Loss: 2.800 \t | Validation Loss: 2.523\n",
            "Epoch 30 done in 208.02 seconds. \t | Training Loss: 2.747 \t | Validation Loss: 2.672\n",
            "Epoch 31 done in 205.58 seconds. \t | Training Loss: 2.705 \t | Validation Loss: 2.611\n",
            "Epoch 32 done in 205.41 seconds. \t | Training Loss: 2.657 \t | Validation Loss: 2.560\n",
            "Epoch 33 done in 206.09 seconds. \t | Training Loss: 2.616 \t | Validation Loss: 2.441\n",
            "Epoch 34 done in 205.30 seconds. \t | Training Loss: 2.546 \t | Validation Loss: 2.392\n",
            "Epoch 35 done in 210.81 seconds. \t | Training Loss: 2.529 \t | Validation Loss: 2.396\n",
            "Epoch 36 done in 205.13 seconds. \t | Training Loss: 2.467 \t | Validation Loss: 2.377\n",
            "Epoch 37 done in 206.54 seconds. \t | Training Loss: 2.456 \t | Validation Loss: 2.333\n",
            "Epoch 38 done in 204.18 seconds. \t | Training Loss: 2.423 \t | Validation Loss: 2.348\n",
            "Epoch 39 done in 203.72 seconds. \t | Training Loss: 2.380 \t | Validation Loss: 2.150\n",
            "Epoch 40 done in 206.64 seconds. \t | Training Loss: 2.326 \t | Validation Loss: 2.052\n",
            "Epoch 41 done in 207.91 seconds. \t | Training Loss: 2.274 \t | Validation Loss: 2.188\n",
            "Epoch 42 done in 208.09 seconds. \t | Training Loss: 2.253 \t | Validation Loss: 2.041\n",
            "Epoch 43 done in 203.51 seconds. \t | Training Loss: 2.232 \t | Validation Loss: 2.113\n",
            "Epoch 44 done in 203.41 seconds. \t | Training Loss: 2.195 \t | Validation Loss: 2.106\n",
            "Epoch 45 done in 202.49 seconds. \t | Training Loss: 2.172 \t | Validation Loss: 2.215\n",
            "Epoch 46 done in 203.06 seconds. \t | Training Loss: 2.131 \t | Validation Loss: 2.072\n",
            "Epoch 47 done in 210.21 seconds. \t | Training Loss: 2.105 \t | Validation Loss: 1.889\n",
            "Epoch 48 done in 211.37 seconds. \t | Training Loss: 2.065 \t | Validation Loss: 1.807\n",
            "Epoch 49 done in 204.42 seconds. \t | Training Loss: 2.059 \t | Validation Loss: 1.960\n",
            "Epoch 50 done in 212.40 seconds. \t | Training Loss: 2.012 \t | Validation Loss: 2.072\n",
            "Epoch 51 done in 211.25 seconds. \t | Training Loss: 1.976 \t | Validation Loss: 1.888\n",
            "Epoch 52 done in 213.67 seconds. \t | Training Loss: 2.011 \t | Validation Loss: 1.913\n",
            "Epoch 53 done in 208.70 seconds. \t | Training Loss: 1.986 \t | Validation Loss: 1.828\n",
            "Epoch 54 done in 210.92 seconds. \t | Training Loss: 1.948 \t | Validation Loss: 1.818\n",
            "Epoch 55 done in 215.05 seconds. \t | Training Loss: 1.891 \t | Validation Loss: 1.724\n",
            "Epoch 56 done in 209.23 seconds. \t | Training Loss: 1.916 \t | Validation Loss: 1.824\n",
            "Epoch 57 done in 208.55 seconds. \t | Training Loss: 1.886 \t | Validation Loss: 1.783\n",
            "Epoch 58 done in 213.68 seconds. \t | Training Loss: 1.829 \t | Validation Loss: 1.764\n",
            "Epoch 59 done in 216.87 seconds. \t | Training Loss: 1.801 \t | Validation Loss: 1.803\n",
            "Epoch 60 done in 208.81 seconds. \t | Training Loss: 1.786 \t | Validation Loss: 1.725\n",
            "Epoch 61 done in 207.14 seconds. \t | Training Loss: 1.768 \t | Validation Loss: 1.738\n",
            "Epoch 62 done in 211.08 seconds. \t | Training Loss: 1.786 \t | Validation Loss: 1.787\n",
            "Epoch 63 done in 214.01 seconds. \t | Training Loss: 1.744 \t | Validation Loss: 1.620\n",
            "Epoch 64 done in 209.09 seconds. \t | Training Loss: 1.708 \t | Validation Loss: 1.594\n",
            "Epoch 65 done in 213.50 seconds. \t | Training Loss: 1.716 \t | Validation Loss: 1.758\n",
            "Epoch 66 done in 208.37 seconds. \t | Training Loss: 1.712 \t | Validation Loss: 1.672\n",
            "Epoch 67 done in 208.95 seconds. \t | Training Loss: 1.677 \t | Validation Loss: 1.632\n",
            "Epoch 68 done in 206.77 seconds. \t | Training Loss: 1.667 \t | Validation Loss: 1.655\n",
            "Epoch 69 done in 206.78 seconds. \t | Training Loss: 1.671 \t | Validation Loss: 1.627\n",
            "Epoch 70 done in 206.70 seconds. \t | Training Loss: 1.656 \t | Validation Loss: 1.762\n",
            "Epoch 71 done in 208.11 seconds. \t | Training Loss: 1.619 \t | Validation Loss: 1.610\n",
            "Epoch 72 done in 207.64 seconds. \t | Training Loss: 1.625 \t | Validation Loss: 1.547\n",
            "Epoch 73 done in 207.30 seconds. \t | Training Loss: 1.646 \t | Validation Loss: 1.558\n",
            "Epoch 74 done in 207.06 seconds. \t | Training Loss: 1.576 \t | Validation Loss: 1.686\n",
            "Epoch 75 done in 208.51 seconds. \t | Training Loss: 1.597 \t | Validation Loss: 1.487\n",
            "Epoch 76 done in 212.71 seconds. \t | Training Loss: 1.589 \t | Validation Loss: 1.520\n",
            "Epoch 77 done in 212.39 seconds. \t | Training Loss: 1.566 \t | Validation Loss: 1.619\n",
            "Epoch 78 done in 209.21 seconds. \t | Training Loss: 1.541 \t | Validation Loss: 1.490\n",
            "Epoch 79 done in 208.20 seconds. \t | Training Loss: 1.516 \t | Validation Loss: 1.754\n",
            "Epoch 80 done in 207.70 seconds. \t | Training Loss: 1.547 \t | Validation Loss: 1.556\n",
            "Epoch 81 done in 208.06 seconds. \t | Training Loss: 1.455 \t | Validation Loss: 1.462\n",
            "Epoch 82 done in 211.76 seconds. \t | Training Loss: 1.518 \t | Validation Loss: 1.506\n",
            "Epoch 83 done in 210.20 seconds. \t | Training Loss: 1.507 \t | Validation Loss: 1.464\n",
            "Epoch 84 done in 211.02 seconds. \t | Training Loss: 1.506 \t | Validation Loss: 1.560\n",
            "Epoch 85 done in 210.53 seconds. \t | Training Loss: 1.496 \t | Validation Loss: 1.619\n",
            "Epoch 86 done in 208.37 seconds. \t | Training Loss: 1.469 \t | Validation Loss: 1.567\n",
            "Epoch 87 done in 209.52 seconds. \t | Training Loss: 1.451 \t | Validation Loss: 1.538\n",
            "Epoch 88 done in 208.56 seconds. \t | Training Loss: 1.431 \t | Validation Loss: 1.448\n",
            "Epoch 89 done in 213.64 seconds. \t | Training Loss: 1.438 \t | Validation Loss: 1.474\n",
            "Epoch 90 done in 212.50 seconds. \t | Training Loss: 1.400 \t | Validation Loss: 1.521\n",
            "Epoch 91 done in 207.11 seconds. \t | Training Loss: 1.407 \t | Validation Loss: 1.489\n",
            "Epoch 92 done in 202.56 seconds. \t | Training Loss: 1.382 \t | Validation Loss: 1.540\n",
            "Epoch 93 done in 206.43 seconds. \t | Training Loss: 1.400 \t | Validation Loss: 1.594\n",
            "Epoch 94 done in 206.89 seconds. \t | Training Loss: 1.398 \t | Validation Loss: 1.525\n",
            "Epoch 95 done in 209.79 seconds. \t | Training Loss: 1.358 \t | Validation Loss: 1.500\n",
            "Epoch 96 done in 211.34 seconds. \t | Training Loss: 1.342 \t | Validation Loss: 1.530\n",
            "Epoch 97 done in 203.22 seconds. \t | Training Loss: 1.321 \t | Validation Loss: 1.482\n",
            "Epoch 98 done in 202.55 seconds. \t | Training Loss: 1.334 \t | Validation Loss: 1.522\n",
            "Epoch 99 done in 210.17 seconds. \t | Training Loss: 1.320 \t | Validation Loss: 1.441\n",
            "Epoch 100 done in 206.65 seconds. \t | Training Loss: 1.331 \t | Validation Loss: 1.418\n",
            "100 epochs ready in 20769.461 seconds. Minimum validation loss: 1.418\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42jSF1hk_h_C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "234a760c-08bc-4bf9-8e2f-5cd36d9570e1"
      },
      "source": [
        "# load best validation accuracy model\n",
        "Conv_model_1.load_state_dict(torch.load(\"Conv_model.pt\"))"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHLjp8u_CYKE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(loaders, model, criterion, device):\n",
        "    # monitor test loss and accuracy\n",
        "    test_loss = 0.\n",
        "    correct = 0.\n",
        "    total = 0.\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (data, target) in enumerate(loaders['test']):\n",
        "            # move to GPU\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            bs, ncrops, c, h, w = data.size()\n",
        "            # forward pass: compute predicted outputs by passing inputs to the model\n",
        "            output = model(data.view(-1, c, h, w)) # fuse batch size and ncrops\n",
        "            output = output.view(bs, ncrops, -1).mean(1)        \n",
        "            # calculate the loss\n",
        "            loss = criterion(output, target)\n",
        "            # update average test loss \n",
        "            test_loss = test_loss + ((1 / (batch_idx + 1)) * (loss.data - test_loss))\n",
        "            # convert output probabilities to predicted class\n",
        "            pred = output.data.max(1, keepdim=True)[1]\n",
        "            # compare predictions to true label\n",
        "            correct += np.sum(np.squeeze(pred.eq(target.data.view_as(pred))).cpu().numpy())\n",
        "            total += data.size(0)            \n",
        "    print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
        "    print('\\nTest Accuracy: %2d%% (%2d/%2d)' % (\n",
        "        100. * correct / total, correct, total))"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JjgST3MP6pyn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "34a22437-3e26-430b-a694-f21cfc32a1b4"
      },
      "source": [
        "test(loaders_dict, Conv_model, criterion, device)"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 5.919703\n",
            "\n",
            "\n",
            "Test Accuracy:  0% ( 5/836)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPpYXZRsCYGq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip freeze > requirements.txt"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkB0qYc-d8_1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAAAMZlQCYE_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-1QuUy2CYC1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGLEZfL5CYAU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzDF0TpaCX9o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# keep learning, Enjoy Empowering"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3mWZfGbxmSZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}