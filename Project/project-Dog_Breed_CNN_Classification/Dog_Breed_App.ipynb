{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Dog_Breed_App.ipynb",
      "provenance": [],
      "mount_file_id": "166ft0-K3ieogZPzJKdwOYPljLvjH04OW",
      "authorship_tag": "ABX9TyM3KVENZDBoQkzruTifJwbw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "cc9483070800444e99e723ab095ee9eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_dd7f6333611f4a0bb7472ca34e5fb4e3",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e56ae8f593884940836cb23325bb14d7",
              "IPY_MODEL_00cac0ab62124fce8ea4bbee267f2a66"
            ]
          }
        },
        "dd7f6333611f4a0bb7472ca34e5fb4e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e56ae8f593884940836cb23325bb14d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_c293253e21304afbb668b83d4a61f8e0",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 102502400,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 102502400,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5dec79b6d22e44c2b0f1187c1a52654b"
          }
        },
        "00cac0ab62124fce8ea4bbee267f2a66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_25681340e2a646f8a4d83b2563402b4b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 97.8M/97.8M [00:01&lt;00:00, 55.4MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_197d94d74e484359898a365959a73e19"
          }
        },
        "c293253e21304afbb668b83d4a61f8e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5dec79b6d22e44c2b0f1187c1a52654b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "25681340e2a646f8a4d83b2563402b4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "197d94d74e484359898a365959a73e19": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vedantdave77/project.Orca/blob/master/Project/project-Dog_Breed_CNN_Classification/Dog_Breed_App.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9tsEVMGnpbV",
        "colab_type": "text"
      },
      "source": [
        "Previously, I implement my agorithm and it has very poor accuracy, so I will use transfer learning to get best result. and Deploy App flow. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7uN9morBiYWw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "74e9f1d7-be74-4503-c1c6-25b14b0bf3fb"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XuxCeh414K3T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from glob import glob\n",
        "\n",
        "# load filenames for human and dog images\n",
        "human_files = np.array(glob(\"/content/drive/My Drive/Data /lfw/*/*\"))\n",
        "dog_files = np.array(glob(\"/content/drive/My Drive/Data /dogImages/*/*/*\"))\n",
        "\n",
        "human_files_short = human_files[:100]                            # pick first 100 images for testing\n",
        "dog_files_short = dog_files[:100]                                # pick first 100 images for testing pretrained detector "
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wbp8ZeFewbEY",
        "colab_type": "text"
      },
      "source": [
        "### Set Requried Parameters "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXc0yRkfoYoG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set parameters which I usually used during project \n",
        "norm_mean = [0.485, 0.456, 0.406]\n",
        "norm_std = [0.229, 0.224, 0.225]\n",
        "img_short_side_resize = 256\n",
        "img_input_size = 224\n",
        "shuffle = True\n",
        "num_workers = 16\n",
        "batch_size = 64"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIKvh7qewfvj",
        "colab_type": "text"
      },
      "source": [
        "### Data Loader [Access / Load/ Tranfer]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22bFiRKxoNsg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import required libraries \n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from PIL import Image\n",
        "from PIL import ImageFile\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True                                          # cutting image (specially, short of edge)\n",
        "\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import datasets \n",
        "\n",
        "# define data_transformation and batch_size \n",
        "transform_train = transforms.Compose([\n",
        "                                      transforms.Resize(img_short_side_resize),\n",
        "                                      transforms.ColorJitter(brightness = 0.2, contrast = 0.2, saturation =0.2, hue = 0.1),\n",
        "                                      transforms.RandomHorizontalFlip(),\n",
        "                                      transforms.RandomResizedCrop(img_input_size, scale=(0.08,1), ratio = (1,1)),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize(mean = norm_mean, std = norm_std)\n",
        "                                       ])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "                                     transforms.Resize(img_input_size),\n",
        "                                     transforms.FiveCrop(img_input_size),\n",
        "                                     transforms.Lambda(lambda crops: torch.stack([\n",
        "                                                                                  transforms.Compose([\n",
        "                                                                                                      transforms.ToTensor(),\n",
        "                                                                                                      transforms.Normalize(mean = norm_mean, std = norm_std)])(crop) for crop in crops\n",
        "                                                                                  ]))\n",
        "\n",
        "                                    ])\n",
        "\n",
        "# load data (define datasets)\n",
        "train_data = datasets.ImageFolder(\"/content/drive/My Drive/Data /dogImages/train/\",transform_train)\n",
        "valid_data = datasets.ImageFolder(\"/content/drive/My Drive/Data /dogImages/valid/\",transform_test)\n",
        "test_data  = datasets.ImageFolder(\"/content/drive/My Drive/Data /dogImages/test/\",transform_test)\n",
        "\n",
        "# separate imput and labels(classes)\n",
        "data = {\"train\" : train_data, \"valid\" : valid_data, \"test\" : test_data}\n",
        "n_classes = len(train_data.classes)\n",
        "\n",
        "# create loaders (train, valid, test)\n",
        "train_loader = torch.utils.data.DataLoader(data[\"train\"], batch_size = batch_size, num_workers = num_workers, shuffle = shuffle, pin_memory = True)\n",
        "valid_loader = torch.utils.data.DataLoader(data[\"valid\"], batch_size = int(np.floor(batch_size/5)), num_workers=0, shuffle = shuffle, pin_memory = True) \n",
        "test_loader = torch.utils.data.DataLoader(data[\"test\"], batch_size = int(np.floor(batch_size/5)), num_workers=0, shuffle = shuffle, pin_memory = True)\n",
        "\n",
        "# loader dictionary\n",
        "loaders_dict = {\"train\" : train_loader, \"valid\" : valid_loader, \"test\" : test_loader}"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWAcuxMzplI5",
        "colab_type": "text"
      },
      "source": [
        "## Model Architecture (pretrained Transfer Learning Model) \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gn_yJuNaoVmr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83,
          "referenced_widgets": [
            "cc9483070800444e99e723ab095ee9eb",
            "dd7f6333611f4a0bb7472ca34e5fb4e3",
            "e56ae8f593884940836cb23325bb14d7",
            "00cac0ab62124fce8ea4bbee267f2a66",
            "c293253e21304afbb668b83d4a61f8e0",
            "5dec79b6d22e44c2b0f1187c1a52654b",
            "25681340e2a646f8a4d83b2563402b4b",
            "197d94d74e484359898a365959a73e19"
          ]
        },
        "outputId": "fc0ec877-6476-4cac-f45c-8818cafb1aac"
      },
      "source": [
        "import torchvision.models as models \n",
        "import torch.nn as nn\n",
        "\n",
        "# specify model \n",
        "model_tl = models.resnet50(pretrained = True)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /root/.cache/torch/checkpoints/resnet50-19c8e357.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cc9483070800444e99e723ab095ee9eb",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=102502400.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFIusJzYstLG",
        "colab_type": "text"
      },
      "source": [
        "### Adjust parameters "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPHkD-r_qdUk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for param in model_tl.parameters():\n",
        "    param.requires_grad = False                                                 # pretrained model does not need backpropagation. "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7HjPfcCsyPw",
        "colab_type": "text"
      },
      "source": [
        "### CUDA checking "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0G91mc3ormcd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2ed73ef2-e369-463a-87c0-c3aa037fbc98"
      },
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "if not use_cuda:\n",
        "    print('CUDA is not available.  Training on CPU ...')\n",
        "    device = \"cpu\"\n",
        "else:\n",
        "    print('CUDA is available!  Training on GPU ...')\n",
        "    device = torch.device(\"cuda:0\")\n",
        "    print(\"Using\",torch.cuda.get_device_name(device))\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CUDA is not available.  Training on CPU ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVV7TsNas2Bs",
        "colab_type": "text"
      },
      "source": [
        "### Model Customization  [Model is already defined]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rk0kmbyPqm-m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# modifying last layer as per custome requirements\n",
        "model_tl.fc = nn.Linear(model_tl.fc.in_features,n_classes)\n",
        "\n",
        "# initializer the weight for new layers\n",
        "nn.init.kaiming_normal_(model_tl.fc.weight, nonlinearity = 'relu')\n",
        "\n",
        "# transfer data and model to CUDA (Tesla P4)\n",
        "model_tl = model_tl.to(device)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aKD8827tBps",
        "colab_type": "text"
      },
      "source": [
        "### Specify Loss and Optimizer "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gaBoywIJsB5n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "criterion_tl = nn.CrossEntropyLoss()\n",
        "optimizer_tl = optim.Adam(model_tl.parameters(),3e-4)\n",
        "scheduler_tl = ReduceLROnPlateau(optimizer_tl,'min',verbose=True, factor = 0.5, patience =7)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECNXikRLt6K5",
        "colab_type": "text"
      },
      "source": [
        "## Train and Validation of Model\n",
        "\n",
        "---\n",
        "### Model training and training loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-dKbNzPvMYl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time \n",
        "def train_epoch(model,train_loader,optimizer,criterion,device):\n",
        "    train_loss = 0.0\n",
        "    model.train()                                                               # define training job\n",
        "    for batch_idx, (data,target) in enumerate(train_loader):                                                                     \n",
        "        data, target = data.to(device),target.to(device)                        # move to CUDA\n",
        "        optimizer.zero_grad()                                                   # reset gradient \n",
        "        output = model(data)                                                    # run model and get output \n",
        "        loss = criterion(output, target)                                        # calculate loss\n",
        "        train_loss += loss.item() * data.size(0)                                # calculate gradients\n",
        "        loss.backward()                                                         # update each layer para value\n",
        "        optimizer.step()\n",
        "    train_loss = train_loss/len(train_loader.dataset)\n",
        "    return model, train_loss"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKLpm70Kw4eG",
        "colab_type": "text"
      },
      "source": [
        "### Validation epoch loss "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63-jMhDSvkgy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def valid_epoch(model,valid_loader,criterion,device,fivecrop): \n",
        "    valid_loss = 0.0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for data, target in valid_loader:\n",
        "            data, target = data.to(device), target.to(device)                   # tranfer to CUDA Tesla P 100\n",
        "            if fivecrop == \"mean\":\n",
        "                bs, ncrops, c, h, w = data.size()                               # dimenstion due to 5 crop method              \n",
        "                output = model(data.view(-1,c,h,w))\n",
        "                output = output.view(bs, ncrops, -1).mean(1)\n",
        "            elif fivecrop == \"max\":\n",
        "                bs, ncrops, c,h,w = data.size()\n",
        "                output = model(data.view(-1,c,h,w))\n",
        "                output = output.view(bs, ncrops, -1).max(1)[0] \n",
        "            else:\n",
        "                output = model(data)\n",
        "            \n",
        "            loss = criterion(output, target)                                    # update losses\n",
        "            valid_loss += loss.item() * data.size(0)\n",
        "    valid_loss = valid_loss/len(valid_loader.dataset)\n",
        "    return valid_loss    "
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mk2DMr88u8EL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(n_epochs, loaders_dict, model,optimizer, criterion, device, path_model, fivecrop = None, lr_scheduler = None):\n",
        "    valid_loss_min = np.Inf\n",
        "    train_loss = []\n",
        "    valid_loss = []\n",
        "    # time everything \n",
        "    time_start = time.time()\n",
        "    for epoch in range(1, n_epochs+1): \n",
        "        time_start_epoch = time.time()\n",
        "        \n",
        "        model, train_loss_epoch = train_epoch(model,loaders_dict[\"train\"],optimizer,criterion,device)\n",
        "        train_loss.append(train_loss_epoch)\n",
        "\n",
        "        # validate this epoch \n",
        "        valid_loss_epoch = valid_epoch(model,loaders_dict[\"valid\"],criterion, device, fivecrop)\n",
        "\n",
        "        # call learning rate scheduler \n",
        "        if lr_scheduler is not None:\n",
        "            lr_scheduler.step(valid_loss_epoch)\n",
        "        valid_loss.append(valid_loss_epoch)\n",
        "        \n",
        "        # save model for new lowest validation loss \n",
        "        if valid_loss_epoch <= valid_loss_min:\n",
        "            torch.save(model.state_dict(),path_model)\n",
        "            valid_loss_min = valid_loss_epoch\n",
        "\n",
        "        print(\"Epoch {} done in {:.2f} seconds. \\t | Training Loss: {:.3f} \\t | Validation Loss: {:.3f}\".format(\n",
        "            epoch, time.time() - time_start_epoch, train_loss_epoch, valid_loss_epoch))\n",
        "    print(f\"{n_epochs} epochs ready in {(time.time() - time_start):.3f} seconds. Minimum validation loss: {valid_loss_min:.3f}\")\n",
        "    model.load_state_dict(torch.load(path_model))\n",
        "    return model\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "devbxP98xKuH",
        "colab_type": "text"
      },
      "source": [
        "### Model_Training Job Execution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHFK4GLWtmj3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "43e98d9d-b6d5-4096-d22e-8231262a5e20"
      },
      "source": [
        "model_tl = train(100, loaders_dict, model_tl, optimizer_tl, criterion_tl, device, 'model_xlearning.pt', fivecrop = 'mean', lr_scheduler = scheduler_tl)\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 done in 998.36 seconds. \t | Training Loss: 4.053 \t | Validation Loss: 2.705\n",
            "Epoch 2 done in 147.71 seconds. \t | Training Loss: 2.517 \t | Validation Loss: 1.590\n",
            "Epoch 3 done in 148.60 seconds. \t | Training Loss: 1.864 \t | Validation Loss: 1.129\n",
            "Epoch 4 done in 149.03 seconds. \t | Training Loss: 1.524 \t | Validation Loss: 0.900\n",
            "Epoch 5 done in 148.49 seconds. \t | Training Loss: 1.332 \t | Validation Loss: 0.772\n",
            "Epoch 6 done in 148.10 seconds. \t | Training Loss: 1.212 \t | Validation Loss: 0.687\n",
            "Epoch 7 done in 148.21 seconds. \t | Training Loss: 1.108 \t | Validation Loss: 0.642\n",
            "Epoch 8 done in 147.74 seconds. \t | Training Loss: 1.063 \t | Validation Loss: 0.606\n",
            "Epoch 9 done in 147.65 seconds. \t | Training Loss: 1.006 \t | Validation Loss: 0.548\n",
            "Epoch 10 done in 154.56 seconds. \t | Training Loss: 0.988 \t | Validation Loss: 0.543\n",
            "Epoch 11 done in 152.65 seconds. \t | Training Loss: 0.923 \t | Validation Loss: 0.522\n",
            "Epoch 12 done in 153.34 seconds. \t | Training Loss: 0.914 \t | Validation Loss: 0.490\n",
            "Epoch 13 done in 147.79 seconds. \t | Training Loss: 0.892 \t | Validation Loss: 0.490\n",
            "Epoch 14 done in 148.93 seconds. \t | Training Loss: 0.847 \t | Validation Loss: 0.468\n",
            "Epoch 15 done in 147.91 seconds. \t | Training Loss: 0.840 \t | Validation Loss: 0.462\n",
            "Epoch 16 done in 148.17 seconds. \t | Training Loss: 0.819 \t | Validation Loss: 0.439\n",
            "Epoch 17 done in 149.49 seconds. \t | Training Loss: 0.808 \t | Validation Loss: 0.428\n",
            "Epoch 18 done in 149.08 seconds. \t | Training Loss: 0.784 \t | Validation Loss: 0.436\n",
            "Epoch 19 done in 148.65 seconds. \t | Training Loss: 0.777 \t | Validation Loss: 0.428\n",
            "Epoch 20 done in 149.23 seconds. \t | Training Loss: 0.756 \t | Validation Loss: 0.423\n",
            "Epoch 21 done in 149.71 seconds. \t | Training Loss: 0.753 \t | Validation Loss: 0.415\n",
            "Epoch 22 done in 158.37 seconds. \t | Training Loss: 0.734 \t | Validation Loss: 0.411\n",
            "Epoch 23 done in 147.74 seconds. \t | Training Loss: 0.733 \t | Validation Loss: 0.414\n",
            "Epoch 24 done in 150.60 seconds. \t | Training Loss: 0.732 \t | Validation Loss: 0.403\n",
            "Epoch 25 done in 147.96 seconds. \t | Training Loss: 0.715 \t | Validation Loss: 0.397\n",
            "Epoch 26 done in 148.17 seconds. \t | Training Loss: 0.704 \t | Validation Loss: 0.411\n",
            "Epoch 27 done in 147.72 seconds. \t | Training Loss: 0.702 \t | Validation Loss: 0.386\n",
            "Epoch 28 done in 148.41 seconds. \t | Training Loss: 0.701 \t | Validation Loss: 0.392\n",
            "Epoch 29 done in 147.05 seconds. \t | Training Loss: 0.668 \t | Validation Loss: 0.392\n",
            "Epoch 30 done in 147.60 seconds. \t | Training Loss: 0.666 \t | Validation Loss: 0.390\n",
            "Epoch 31 done in 149.69 seconds. \t | Training Loss: 0.684 \t | Validation Loss: 0.384\n",
            "Epoch 32 done in 150.63 seconds. \t | Training Loss: 0.684 \t | Validation Loss: 0.373\n",
            "Epoch 33 done in 151.69 seconds. \t | Training Loss: 0.671 \t | Validation Loss: 0.385\n",
            "Epoch 34 done in 152.47 seconds. \t | Training Loss: 0.656 \t | Validation Loss: 0.378\n",
            "Epoch 35 done in 152.79 seconds. \t | Training Loss: 0.670 \t | Validation Loss: 0.377\n",
            "Epoch 36 done in 152.89 seconds. \t | Training Loss: 0.658 \t | Validation Loss: 0.375\n",
            "Epoch 37 done in 150.80 seconds. \t | Training Loss: 0.634 \t | Validation Loss: 0.377\n",
            "Epoch 38 done in 150.57 seconds. \t | Training Loss: 0.658 \t | Validation Loss: 0.390\n",
            "Epoch 39 done in 149.89 seconds. \t | Training Loss: 0.626 \t | Validation Loss: 0.381\n",
            "Epoch 40 done in 150.60 seconds. \t | Training Loss: 0.617 \t | Validation Loss: 0.370\n",
            "Epoch 41 done in 149.66 seconds. \t | Training Loss: 0.622 \t | Validation Loss: 0.372\n",
            "Epoch 42 done in 151.12 seconds. \t | Training Loss: 0.630 \t | Validation Loss: 0.390\n",
            "Epoch 43 done in 150.37 seconds. \t | Training Loss: 0.607 \t | Validation Loss: 0.390\n",
            "Epoch 44 done in 150.86 seconds. \t | Training Loss: 0.626 \t | Validation Loss: 0.363\n",
            "Epoch 45 done in 150.56 seconds. \t | Training Loss: 0.638 \t | Validation Loss: 0.364\n",
            "Epoch 46 done in 149.94 seconds. \t | Training Loss: 0.595 \t | Validation Loss: 0.375\n",
            "Epoch 47 done in 150.12 seconds. \t | Training Loss: 0.609 \t | Validation Loss: 0.359\n",
            "Epoch 48 done in 150.84 seconds. \t | Training Loss: 0.590 \t | Validation Loss: 0.369\n",
            "Epoch 49 done in 151.05 seconds. \t | Training Loss: 0.587 \t | Validation Loss: 0.368\n",
            "Epoch 50 done in 150.01 seconds. \t | Training Loss: 0.597 \t | Validation Loss: 0.379\n",
            "Epoch 51 done in 152.97 seconds. \t | Training Loss: 0.608 \t | Validation Loss: 0.364\n",
            "Epoch 52 done in 162.41 seconds. \t | Training Loss: 0.592 \t | Validation Loss: 0.375\n",
            "Epoch 53 done in 160.73 seconds. \t | Training Loss: 0.593 \t | Validation Loss: 0.380\n",
            "Epoch 54 done in 158.77 seconds. \t | Training Loss: 0.578 \t | Validation Loss: 0.379\n",
            "Epoch    55: reducing learning rate of group 0 to 1.5000e-04.\n",
            "Epoch 55 done in 158.25 seconds. \t | Training Loss: 0.572 \t | Validation Loss: 0.372\n",
            "Epoch 56 done in 158.86 seconds. \t | Training Loss: 0.561 \t | Validation Loss: 0.350\n",
            "Epoch 57 done in 156.93 seconds. \t | Training Loss: 0.559 \t | Validation Loss: 0.348\n",
            "Epoch 58 done in 158.08 seconds. \t | Training Loss: 0.548 \t | Validation Loss: 0.345\n",
            "Epoch 59 done in 158.50 seconds. \t | Training Loss: 0.549 \t | Validation Loss: 0.354\n",
            "Epoch 60 done in 159.31 seconds. \t | Training Loss: 0.561 \t | Validation Loss: 0.346\n",
            "Epoch 61 done in 159.68 seconds. \t | Training Loss: 0.548 \t | Validation Loss: 0.342\n",
            "Epoch 62 done in 166.61 seconds. \t | Training Loss: 0.549 \t | Validation Loss: 0.355\n",
            "Epoch 63 done in 166.57 seconds. \t | Training Loss: 0.556 \t | Validation Loss: 0.344\n",
            "Epoch 64 done in 165.12 seconds. \t | Training Loss: 0.544 \t | Validation Loss: 0.343\n",
            "Epoch 65 done in 153.29 seconds. \t | Training Loss: 0.554 \t | Validation Loss: 0.355\n",
            "Epoch 66 done in 156.68 seconds. \t | Training Loss: 0.541 \t | Validation Loss: 0.336\n",
            "Epoch 67 done in 157.95 seconds. \t | Training Loss: 0.536 \t | Validation Loss: 0.347\n",
            "Epoch 68 done in 157.64 seconds. \t | Training Loss: 0.526 \t | Validation Loss: 0.348\n",
            "Epoch 69 done in 158.32 seconds. \t | Training Loss: 0.548 \t | Validation Loss: 0.350\n",
            "Epoch 70 done in 158.52 seconds. \t | Training Loss: 0.528 \t | Validation Loss: 0.348\n",
            "Epoch 71 done in 157.01 seconds. \t | Training Loss: 0.564 \t | Validation Loss: 0.349\n",
            "Epoch 72 done in 156.89 seconds. \t | Training Loss: 0.547 \t | Validation Loss: 0.346\n",
            "Epoch 73 done in 157.36 seconds. \t | Training Loss: 0.525 \t | Validation Loss: 0.356\n",
            "Epoch    74: reducing learning rate of group 0 to 7.5000e-05.\n",
            "Epoch 74 done in 157.68 seconds. \t | Training Loss: 0.531 \t | Validation Loss: 0.345\n",
            "Epoch 75 done in 157.41 seconds. \t | Training Loss: 0.517 \t | Validation Loss: 0.349\n",
            "Epoch 76 done in 163.03 seconds. \t | Training Loss: 0.543 \t | Validation Loss: 0.347\n",
            "Epoch 77 done in 167.24 seconds. \t | Training Loss: 0.523 \t | Validation Loss: 0.355\n",
            "Epoch 78 done in 162.45 seconds. \t | Training Loss: 0.522 \t | Validation Loss: 0.346\n",
            "Epoch 79 done in 156.79 seconds. \t | Training Loss: 0.530 \t | Validation Loss: 0.343\n",
            "Epoch 80 done in 154.50 seconds. \t | Training Loss: 0.532 \t | Validation Loss: 0.344\n",
            "Epoch 81 done in 158.00 seconds. \t | Training Loss: 0.526 \t | Validation Loss: 0.345\n",
            "Epoch    82: reducing learning rate of group 0 to 3.7500e-05.\n",
            "Epoch 82 done in 163.37 seconds. \t | Training Loss: 0.522 \t | Validation Loss: 0.341\n",
            "Epoch 83 done in 160.56 seconds. \t | Training Loss: 0.508 \t | Validation Loss: 0.341\n",
            "Epoch 84 done in 160.77 seconds. \t | Training Loss: 0.491 \t | Validation Loss: 0.343\n",
            "Epoch 85 done in 160.23 seconds. \t | Training Loss: 0.520 \t | Validation Loss: 0.338\n",
            "Epoch 86 done in 160.63 seconds. \t | Training Loss: 0.516 \t | Validation Loss: 0.341\n",
            "Epoch 87 done in 159.02 seconds. \t | Training Loss: 0.513 \t | Validation Loss: 0.337\n",
            "Epoch 88 done in 159.98 seconds. \t | Training Loss: 0.518 \t | Validation Loss: 0.340\n",
            "Epoch 89 done in 156.58 seconds. \t | Training Loss: 0.510 \t | Validation Loss: 0.338\n",
            "Epoch    90: reducing learning rate of group 0 to 1.8750e-05.\n",
            "Epoch 90 done in 158.20 seconds. \t | Training Loss: 0.523 \t | Validation Loss: 0.339\n",
            "Epoch 91 done in 160.81 seconds. \t | Training Loss: 0.482 \t | Validation Loss: 0.339\n",
            "Epoch 92 done in 161.92 seconds. \t | Training Loss: 0.504 \t | Validation Loss: 0.338\n",
            "Epoch 93 done in 159.05 seconds. \t | Training Loss: 0.494 \t | Validation Loss: 0.338\n",
            "Epoch 94 done in 159.41 seconds. \t | Training Loss: 0.521 \t | Validation Loss: 0.340\n",
            "Epoch 95 done in 159.26 seconds. \t | Training Loss: 0.527 \t | Validation Loss: 0.333\n",
            "Epoch 96 done in 160.79 seconds. \t | Training Loss: 0.513 \t | Validation Loss: 0.343\n",
            "Epoch 97 done in 162.29 seconds. \t | Training Loss: 0.515 \t | Validation Loss: 0.336\n",
            "Epoch 98 done in 160.06 seconds. \t | Training Loss: 0.519 \t | Validation Loss: 0.337\n",
            "Epoch 99 done in 159.70 seconds. \t | Training Loss: 0.495 \t | Validation Loss: 0.336\n",
            "Epoch 100 done in 159.38 seconds. \t | Training Loss: 0.496 \t | Validation Loss: 0.337\n",
            "100 epochs ready in 16319.182 seconds. Minimum validation loss: 0.333\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXE7wxYJ3y80",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "outputId": "c9b572ea-8e21-44fd-eef3-e7513917f255"
      },
      "source": [
        "# load best model\n",
        "model_tl.load_state_dict(torch.load('model_xlearning.pt'))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-3aa2d6e0f7b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# load best model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel_tl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model_xlearning.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    591\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_legacy_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    594\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_legacy_load\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    771\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    772\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 773\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m     \u001b[0mdeserialized_storage_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mpersistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m    727\u001b[0m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_torch_load_uninitialized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mdeserialized_objects\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mroot_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrestore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeserialized_objects\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mroot_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mview_metadata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdefault_restore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_package_registry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_cuda_deserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_cuda_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_torch_load_uninitialized\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0mstorage_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mvalidate_cuda_device\u001b[0;34m(location)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m         raise RuntimeError('Attempting to deserialize object on a CUDA '\n\u001b[0m\u001b[1;32m    139\u001b[0m                            \u001b[0;34m'device but torch.cuda.is_available() is False. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m                            \u001b[0;34m'If you are running on a CPU-only machine, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwtkoyL5vd5B",
        "colab_type": "text"
      },
      "source": [
        "### Model Testing "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-AaHKZnEvcaV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(loaders, model, criterion, device):\n",
        "    # monitor test loss and accuracy\n",
        "    test_loss = 0.\n",
        "    correct = 0.\n",
        "    total = 0.\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (data, target) in enumerate(loaders['test']):\n",
        "            # move to GPU\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            bs, ncrops, c, h, w = data.size()\n",
        "            # forward pass: compute predicted outputs by passing inputs to the model\n",
        "            output = model(data.view(-1, c, h, w)) # fuse batch size and ncrops\n",
        "            output = output.view(bs, ncrops, -1).mean(1)        \n",
        "            # calculate the loss\n",
        "            loss = criterion(output, target)\n",
        "            # update average test loss \n",
        "            test_loss = test_loss + ((1 / (batch_idx + 1)) * (loss.data - test_loss))\n",
        "            # convert output probabilities to predicted class\n",
        "            pred = output.data.max(1, keepdim=True)[1]\n",
        "            # compare predictions to true label\n",
        "            correct += np.sum(np.squeeze(pred.eq(target.data.view_as(pred))).cpu().numpy())\n",
        "            total += data.size(0)            \n",
        "    print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
        "    print('\\nTest Accuracy: %2d%% (%2d/%2d)' % (\n",
        "        100. * correct / total, correct, total))"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ThOvrdcuuDQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test(loaders_dict, model_tl, criterion_tl,device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWa3GPpRv_L7",
        "colab_type": "text"
      },
      "source": [
        "### Predictive Model\n",
        "\n",
        "create pipeline for data prediction\n",
        "> data -- tranfer -- model prediction -- evaluation score -- output "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9DF3klFv-PE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "outputId": "766ce0f5-0a7d-40b7-adbb-cfeff26c9573"
      },
      "source": [
        "class_names = [item[4:].replace(\"_\",\" \") for item in data[\"train\"].classes]\n",
        "\n",
        "def predict_breed(img_path,model,device):\n",
        "    image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "    in_tranform = transforms.Compose([\n",
        "                                      transforms.Resize(img_short_size_resize),\n",
        "                                      transforms.FiveCrop(img_input_size),\n",
        "                                      transforms.Lambda(lambda crops: torch.stack([transforms.Compose([\n",
        "                                                                                                       transforms.ToTensor(),\n",
        "                                                                                                       transforms.Normalize(mean=norm_mean, std = norm_std)])(crop) for crop in crops]))\n",
        "                                     ])\n",
        "    \n",
        "    scores = model(in_transform(image).to(device)).mean(0)\n",
        "    output = torch.argmax(score)  \n",
        "    return output.to(\"cpu\").item(),  F.softmax(score,dim=0).to(\"cpu\").data.numpy()\n",
        "\n",
        "test_image = dog_files_short[0]\n",
        "pred,probs = predict_breed(test_image,model_tl, device)\n",
        "show_image_from_path(test_image,title=class_names[pred])"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-d3cf62e2f30d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mtest_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdog_files_short\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_breed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_image\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_tl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mshow_image_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_image\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-44-d3cf62e2f30d>\u001b[0m in \u001b[0;36mpredict_breed\u001b[0;34m(img_path, model, device)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     in_tranform = transforms.Compose([\n\u001b[0;32m----> 7\u001b[0;31m                                       \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_short_size_resize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m                                       \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFiveCrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_input_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                                       transforms.Lambda(lambda crops: torch.stack([transforms.Compose([\n",
            "\u001b[0;31mNameError\u001b[0m: name 'img_short_size_resize' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FotfED_FwHiv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip freeze > requirements.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVh2RjCNwHff",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSt14EC4wHdT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}