{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dQB6Vx2g1kjn"
   },
   "source": [
    "# Sentimental_Analysis using AWS sagemaker - XGBoost algorithm\n",
    "[Vedant Dave](https://vedantdave77.github.io/) | Vedantdave77@gmail.com | [LinkedIn](https://www.linkedin.com/in/vedant-dave117/)\n",
    "\n",
    "Hello, I am Vedant Dave, a machine learning practitioner and a data enthusiast. -@dave117\n",
    "\n",
    "## Intro:\n",
    "In this notebook, I am going to analyze IMDB dataset. Its one of the best dagtaset of NLP research. You can search about IMDB on IMDB.com to get an idea about the company portfolio and their workprofile. Well, my main purpose is to work on Batch_Transform and updating model for better performance. MY previous result was 85.12 % accurate with batch_transform (sagemaker - python sdk) and I improved to 87.28% with sagemaker auto Hyper_parameter tuning. \n",
    "\n",
    "Why?\n",
    "\n",
    "- For most of online websites and ecommerce/ digital communication companies, sentimental analysis is one of the major field to improve customer satisfaction, which leads to business growth. \n",
    "\n",
    "- My Major goal is to analyze (preparation of text data and implement a AWS model with sagemaker (batch-transform method). I am also going to make deployment using lambda function (with another notebook). The data storage will be S3 data storage. \n",
    "\n",
    "- The credit for this notebook goes to Udacity, from which I took an intuition, but the code modification, improvement and procedure explaination done by me. So, for any specific issue with notebook you can connect with me on above contact ID, and I request you to use right side of google tab for searching about more explaination. Thank you. \n",
    "\n",
    "---\n",
    "\n",
    "Project ML Flow: **Standford Data API -- S3 -- SageMaker -- Lambda -- WebApp(html file)** \n",
    "\n",
    "---\n",
    "\n",
    "So, let's start...\n",
    "\n",
    "### Download data from [IMDB dataset](http://ai.stanford.edu/~amaas/data/sentiment/)\n",
    "Current format of data is One file, for project we need to seperate them in train, validation and test datasets. The labels are also in pos/ neg form so, for project, its better to covert them in 0 and 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 212
    },
    "colab_type": "code",
    "id": "ppskPPw6UlL0",
    "outputId": "94db716d-fec3-4320-c843-636fa4f478c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-06-13 01:00:32--  http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
      "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
      "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 84125825 (80M) [application/x-gzip]\n",
      "Saving to: ‘../data/aclImdb_v1.tar.gz’\n",
      "\n",
      "../data/aclImdb_v1. 100%[===================>]  80.23M  45.3MB/s    in 1.8s    \n",
      "\n",
      "2020-06-13 01:00:34 (45.3 MB/s) - ‘../data/aclImdb_v1.tar.gz’ saved [84125825/84125825]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%mkdir ../data                                                                                         # create directory\n",
    "!wget -O ../data/aclImdb_v1.tar.gz http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz      # download data with !wget --> its gnu fun. helps to download http://* data\n",
    "!tar -zxf ../data/aclImdb_v1.tar.gz -C ../data                                                          # extract .tar file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B9APSOWD5fXU"
   },
   "source": [
    "### Data Preparation\n",
    "\n",
    "The complex problem in ML is to clean data, and make them ready for analysis. Here, please observe above review. We downloaded from web in html form. That's why you can see html format <br> ... </br> there. So, first we need to remove them. More over, some words are repetative, meaning less and with similar meaning. So, first we will remove all these obsecles. The step is called data preprocessing, also know as data cleaning, dfata wrangling, data manipulation. So, I am going to use NLTK library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wcQ9psFh5d_i"
   },
   "outputs": [],
   "source": [
    "import os                                                                       # provide operating system accordingly ...\n",
    "import glob                                                                     # glob is path name matcher, start each file with .*\n",
    "\n",
    "def read_data(data_dir='../data/aclImdb'):\n",
    "    data = {}\n",
    "    labels = {}\n",
    "    \n",
    "    for data_type in ['train', 'test']:\n",
    "        data[data_type] = {}\n",
    "        labels[data_type] = {}\n",
    "        \n",
    "        for sentiment in ['pos', 'neg']:\n",
    "            data[data_type][sentiment] = []\n",
    "            labels[data_type][sentiment] = []\n",
    "            \n",
    "            path = os.path.join(data_dir, data_type, sentiment, '*.txt')\n",
    "            files = glob.glob(path)\n",
    "            \n",
    "            for f in files:\n",
    "                with open(f) as review:\n",
    "                    data[data_type][sentiment].append(review.read())\n",
    "                    # Here we represent a positive review by '1' and a negative review by '0'\n",
    "                    labels[data_type][sentiment].append(1 if sentiment == 'pos' else 0)\n",
    "                    \n",
    "            assert len(data[data_type][sentiment]) == len(labels[data_type][sentiment]), \\\n",
    "                    \"{}/{} data size does not match labels size\".format(data_type, sentiment)\n",
    "                \n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n8T3xzaL5d8O"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total IMDB reviews : train = 12500 pos/ 12500 neg, test = 12500 pos / 12500 neg\n"
     ]
    }
   ],
   "source": [
    "data,labels = read_data()\n",
    "print(\"Total IMDB reviews : train = {} pos/ {} neg, test = {} pos / {} neg\".\n",
    "      format(len(data['train']['pos']),len(data['train']['neg']),\n",
    "             len(data['test']['pos']),len(data['test']['neg'])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8tF6poUq5d6d"
   },
   "outputs": [],
   "source": [
    "# Now, lets conmbine pos and neg dataset and shuffle them for making training and testing dataset.\n",
    "# WHY?  --> because, form above function we get four sets separated by pos, neg in train and test set... (look and understand)\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "def prepare_imdb_data(data,lables):\n",
    "    data_train = data['train']['pos'] + data['train']['neg']\n",
    "    data_test = data['test']['pos'] + data['test']['neg']\n",
    "    labels_train = labels['train']['pos'] + labels['train']['neg']              # Awesome mistake +++ cost me 8 days (and 50+ hr sagemaker cost)\n",
    "    labels_test = labels['test']['pos'] + labels['test']['neg']\n",
    "\n",
    "    # shuffle reviews and correspoing labels within training and test dataset\n",
    "    data_train, labels_train = shuffle(data_train,labels_train)                 # this helps us to shuffle through whole training ...\n",
    "    data_test, labels_test = shuffle(data_test,labels_test)\n",
    "\n",
    "    # return a datasets for future processes.\n",
    "    return data_train, data_test, labels_train, labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "72F89VYF5d3-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDB total reviews (full dataset) :train = 25000, test - 25000\n"
     ]
    }
   ],
   "source": [
    "train_X,test_X,train_y,test_y = prepare_imdb_data(data,labels)\n",
    "print('IMDB total reviews (full dataset) :train = {}, test - {}'.format(len(train_X),len(test_X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hz3K6bEW5d1f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'If the answer to this question is yes, then you should enjoy this excellent movie. I\\'ve just seen it a couple of hours ago here in Paris (where the action of the movie takes place)and I can still feel the huge trauma I received in the back of my eyes...What a visual shock ! I\\'ve never seen such a beautiful black&white photo and such a drastic change in the way of doing animated movies. I strongly believe there will a before and after \"Renaissance\", similarly to what we saw with Pixar movies or the Akira and GhostInTheShell experiences. This is a real breakthrough in the small world of animated movies and I hope this french initiative (a small unknown french studio with a few young folks who had a dream named \"Renaissance\"...) will receive the success and recognition it deserves. Vive la France !'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X[100]                                                                    #  first 100 reviews (.txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qTH0SAFrARcS"
   },
   "source": [
    "### Data Preprocessing\n",
    "The complex problem in ML is to clean data, and make them ready for analysis. Here, please observe above review. We downloaded from web in html form. That's why you can see html format **(<!br>  \\</!br>)** there. So, first we need to remove them. More over, some words are repetative, meaning less and with similar meaning. So, first we will remove all these obsecles. The step is called data preprocessing, also know as data cleaning, dfata wrangling, data manipulation. So, I am going to use NLTK library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "whAo52i55dzJ",
    "outputId": "7f6544a3-24e1-4505-9908-5db0b5f6972d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"stopwords\")                                # does not work\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *                            # does not work\n",
    "\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8fkaxgV35dwf"
   },
   "outputs": [],
   "source": [
    "import re                                                     # import request.\n",
    "from bs4 import BeautifulSoup                                 # python library for html and css parsing(remove format stye...) \n",
    "\n",
    "def review_to_words(review):\n",
    "    text = BeautifulSoup(review, \"html.parser\").get_text()                      # remove html tags\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\",\" \",text.lower())                             # conver to lowercase (all a to z, A to Z, 0 to 9)\n",
    "    words = text.split()                                                        # split string into words\n",
    "    words = [w for w in words if  w not in stopwords.words(\"english\")]          # remove stopwords\n",
    "    words = [PorterStemmer().stem(w) for w in words]                           # stem --> nlp library for stemmers (prular words, languages, similarity etc...)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D8OSjE7C5dtR"
   },
   "outputs": [],
   "source": [
    "import pickle                                                                   # for serializing/deserializing python input (here,pickle--> converts datastructure to byte stram)\n",
    "cache_dir = os.path.join(\"../cache\", \"sentiment_analysis\")                      # define storage path\n",
    "os.makedirs(cache_dir, exist_ok=True)                                           # ensure about directory\n",
    "\n",
    "def preprocess_data(data_train, data_test, labels_train, labels_test,\n",
    "                    cache_dir = cache_dir, cache_file =\"preprocessed_data.pkl\"):\n",
    "  \n",
    "    cache_data = None                                          # initialize cach data\n",
    "    if cache_file is not None:                                 # comp saved cache data for future purpose so, the operation will be faster \n",
    "        try: \n",
    "            with open(os.path.join(cache_dir, cache_file), \"rb\") as f:           # read bite form pickle file\n",
    "                cache_data = pickle.load(f)\n",
    "            print(\"Read preprocessed data from cache file :\" , cache_file)\n",
    "        except:\n",
    "            pass                       \n",
    "\n",
    "    if cache_data is None:\n",
    "        words_train = [review_to_words(review) for review in data_train]    # generate list [] from available dict. \"data_train\"\n",
    "        words_test = [review_to_words(review) for review in data_test]     # ... same \n",
    "\n",
    "        if cache_file is not None:\n",
    "            cache_data = dict(words_train = words_train,words_test = words_test, labels_train = labels_train,labels_test=labels_test)\n",
    "        with open(os.path.join(cache_dir, cache_file),\"wb\") as f:\n",
    "            pickle.dump(cache_data,f)\n",
    "            print(\"Wrote preprocessed data to cache file: \", cache_file)\n",
    "    else: \n",
    "        print(\"Getting from cache data ...\")\n",
    "        words_train,words_test,labels_train,labels_test = (cache_data['words_train'],cache_data['words_test'],cache_data['labels_train'],cache_data['labels_test'])\n",
    "      \n",
    "    return words_train,words_test,labels_train,labels_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explainations of preprocess operation:** \n",
    "\n",
    "Here, We had two options, \n",
    "> first one **load data directly from cache_file,which generated previously. If does not exist, then** and then move to cache_data....\n",
    ">> (A)  Now, first **check the data existance as cache_data**, if it is there in **empty cache_data, then generate train and test list** for cache_data  and also write operation (dump) to **fill the cache_file.** So, for future purpose our data will be taken from cache_file.<br>\n",
    ">> (B) But, **if cache_data is already exists, then better to load data** from it,to save time.\n",
    "\n",
    "Still, in case of confusion!, its better to make a flow diagram on paper ownself. ;) :).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q6m_r9Ubz-BU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote preprocessed data to cache file:  preprocessed_data.pkl\n"
     ]
    }
   ],
   "source": [
    "# get preprocessed data\n",
    "train_X,test_X,train_y,test_y = preprocess_data(train_X,test_X,train_y,test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fix8k_YI0h4_"
   },
   "source": [
    "### Extract Bag of words features \n",
    "\n",
    "\n",
    "The bag-of-words model is a simplifying representation used in natural language processing and information retrieval (IR). In this model, a text (such as a sentence or a document) is represented as the bag (multiset) of its words, disregarding grammar and even word order but keeping multiplicity.\n",
    "\n",
    "\n",
    "As an example : \n",
    "\n",
    "> Sentence : I like data science, it is the exploration behind data. Please, give me an opportunity to work with data science\n",
    ">> Bag of words = {\"I\" = 1, \"like\":1, \"data\" :3, science\" :2 , \"it\":1, ... ,\"with\" : 1};"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sIygdc2fz9-C"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.externals import joblib                                            # joblib is advanced pickle version used for storing numpy arrays (bite-pyhton_moduel-bite\n",
    "\n",
    "def extract_BoW_features(words_train,words_test,vocabulary_size = 5000,\n",
    "                         cache_dir = cache_dir, cache_file = 'bow_features.pkl'):\n",
    "    cache_data = None\n",
    "    if cache_file is not None:\n",
    "        try:\n",
    "            with open(os.path.join(cache_dir, cache_file), 'rb') as f:\n",
    "                cache_data = joblib.load(f)\n",
    "            print(\"Read features from cache file: \", cache_file)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    if cache_data is None:\n",
    "        vectorizer = CountVectorizer(max_features = vocabulary_size ,\n",
    "                                     preprocessor = lambda x: x,tokenizer = lambda x:x)\n",
    "        features_train = vectorizer.fit_transform(words_train).toarray()\n",
    "        features_test = vectorizer.transform(words_test).toarray()\n",
    "\n",
    "        if cache_file is not None:\n",
    "            vocabulary = vectorizer.vocabulary_\n",
    "            cache_data = dict(features_train = features_train,features_test=features_test,\n",
    "                            vocabulary = vocabulary)\n",
    "            with open(os.path.join(cache_dir, cache_file),'wb') as f:\n",
    "                joblib.dump(cache_data,f)\n",
    "            print(\"wrote features to cache file:\",cache_file)\n",
    "    else:\n",
    "        features_train, features_test,vocabulary = (cache_data['features_train'],cache_data['features_test'],\n",
    "                                                    cache_data['vocabulary'])\n",
    "\n",
    "    return features_train, features_test, vocabulary\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gMP5eYvRz98O"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrote features to cache file: bow_features.pkl\n"
     ]
    }
   ],
   "source": [
    "train_X,test_X,vocabulary = extract_BoW_features(train_X,test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W1vzcPtmJ8d1"
   },
   "source": [
    "### Classification using  XGBoost Algorithm\n",
    "SageMaker has predefined XGBoost Algirthm for classificatio task. But for better accuracy and avoid overfitting I want to use validation dataset. For that, first we will give first 10000 review to validation and then give data to XGBoost in panda dataframe format. The data is stored in S3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y0eP9fpBz94U"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "val_X = pd.DataFrame(train_X[:10000])\n",
    "train_X = pd.DataFrame(train_X[10000:])\n",
    "\n",
    "val_y = pd.DataFrame(train_y[:10000])\n",
    "train_y = pd.DataFrame(train_y[10000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7_PmpEvrJFJV"
   },
   "outputs": [],
   "source": [
    "# generate local dictionary where our data is stored for use.\n",
    "data_dir = '../data/xgboost'\n",
    "if not os.path.exists(data_dir):                                                # ensure about dir. (resolve bug)\n",
    "    os.makedirs(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NdWcpT9MJGE9"
   },
   "outputs": [],
   "source": [
    "# save data to dictionary (take some time - 1 min) , check your local instance to see the directory\n",
    "\n",
    "pd.DataFrame(test_X).to_csv(os.path.join(data_dir, 'test.csv'), header=False, index=False)                         # test.csv \n",
    "\n",
    "pd.concat([val_y, val_X], axis=1).to_csv(os.path.join(data_dir, 'validation.csv'), header=False, index=False)   # validation.csv\n",
    "\n",
    "pd.concat([train_y, train_X], axis=1).to_csv(os.path.join(data_dir, 'train.csv'), header=False, index=False)    # test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sUyJKTpSJGBk"
   },
   "outputs": [],
   "source": [
    "# initialize memory storage (so, set a bit of memory to None)\n",
    "train_X = val_X = train_y = val_y = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JTBGZKQwjk9_"
   },
   "source": [
    "Another, important point is, you can not erase such memory every time because when you train deep algorithms, where data will use again and again.\n",
    "\n",
    "---\n",
    "\n",
    "### Uploading Training/validation to S3 \n",
    "\n",
    "#### Flow :=> Local_Dir    -TO-   S3(data)    -TO-     Sagemaker(training)    -TO-    S3(result)    -TO-   Local_Dir \n",
    "\n",
    "here, Local_Dir is our notebook instance, not your machine space. Check Jupyter Notebook instance main folde, for that.\n",
    "\n",
    "Here, I am going to use sagemaker's high level functionality so, all the background work will be done by sagemaker ownself, and I just need to provide resources, commands and requirements to sagemaker. This is regid but quicker approach.\n",
    "\n",
    "There is posibility of Low level functionality, which give us chance to provide flexiblility to model, but when you need to do some research around your result. Well, here in future I will use auto Hyper parameter tuning, to get best answer (with high accuracy) for our dataset problem. So, its nice to use highlevel features.\n",
    "\n",
    "Let's start real work with SAGEMAKER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bAgduC9cJF_r"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'upload_data' method will be deprecated in favor of 'S3Uploader' class (https://sagemaker.readthedocs.io/en/stable/s3.html#sagemaker.s3.S3Uploader) in SageMaker Python SDK v2.\n",
      "'upload_data' method will be deprecated in favor of 'S3Uploader' class (https://sagemaker.readthedocs.io/en/stable/s3.html#sagemaker.s3.S3Uploader) in SageMaker Python SDK v2.\n",
      "'upload_data' method will be deprecated in favor of 'S3Uploader' class (https://sagemaker.readthedocs.io/en/stable/s3.html#sagemaker.s3.S3Uploader) in SageMaker Python SDK v2.\n"
     ]
    }
   ],
   "source": [
    "import sagemaker                                                                # call sagemaker\n",
    "session = sagemaker.Session()                                                   # create  session for sagemaker \n",
    "prefix = 'sentiment-xgboost-update'                                                    # prefix will be used for unique name identification (in near future)\n",
    "\n",
    "# set specific location on S3 for easy access \n",
    "test_location = session.upload_data(os.path.join(data_dir,'test.csv'),key_prefix= prefix)           # upload test data \n",
    "val_location = session.upload_data(os.path.join(data_dir,'validation.csv'),key_prefix = prefix)    # upload validation data\n",
    "train_location = session.upload_data(os.path.join(data_dir, 'train.csv'),key_prefix = prefix)        # upload train data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CiKQEHwDmq8-"
   },
   "source": [
    "### Create XGBoost model tuning requirement\n",
    "\n",
    "As I declared before, I am using high level API, helps me to get answer quickly without more flexibility. But, after auto tuing we will get the best answer. Now, here before training, we need to do some setup. \n",
    "\n",
    "Sagemaker model creation : it's ecosystem has three different objects, which are interactive with eachother. \n",
    "1. Model Artifacts\n",
    "2. Training Code (container)\n",
    "3. Inference Code (container)\n",
    "\n",
    "Model artifact is Model itself. The training code use training data, and create model artifacts. Inference code use the model artifacts to predict new data. \n",
    "\n",
    "Sagemaker use docker containers. So, after all docker container is one kind of package of code with proper sequence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ALl4hkN0mqmc"
   },
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role                                          \n",
    "role = get_execution_role()                                                     # create model execution role = IAM role, for giving permission to specific person or user group (to control unauthorize access)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vb1NhGwQJF8b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'get_image_uri' method will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n",
      "WARNING:root:There is a more up to date SageMaker XGBoost image. To use the newer image, please set 'repo_version'='1.0-1'. For example:\n",
      "\tget_image_uri(region, 'xgboost', '1.0-1').\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "container = get_image_uri(session.boto_region_name,'xgboost')                   # set container for giving private space to model (when you have more than one deploying model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dE_CFWjLJF6W"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Parameter image_name will be renamed to image_uri in SageMaker Python SDK v2.\n"
     ]
    }
   ],
   "source": [
    "# specify model with requried parameters \n",
    "# xgb = None                                                                      # create model\n",
    "xgb = sagemaker.estimator.Estimator(container,                                  # define container (where to take data)\n",
    "                                    role,                                       # define role (who give permission for this)\n",
    "                                    train_instance_count = 1,                   # instance will used for task (more instance, more power, more expense)\n",
    "                                    train_instance_type = 'ml.m4.xlarge',       # power of isntance (more power, more expense, less execution time)\n",
    "                                    output_path = 's3://{}/{}/output'.format(session.default_bucket(),prefix),   # where to save\n",
    "                                    sagemaker_session= session)                 # define session (the current one)\n",
    "\n",
    "\n",
    "\n",
    "xgb.set_hyperparameters(max_depth = 5,                                          # please understand xgboost documents first \n",
    "                        eta =0.2,\n",
    "                        gamma=4,\n",
    "                        min_child_weight=6,\n",
    "                        subsample = 0.8,\n",
    "                        silent= 0,\n",
    "                        objective = 'binary:logistic',\n",
    "                        early_stopping_rounds= 10,\n",
    "                        num_round = 500)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PBTjaWXV-vKr"
   },
   "source": [
    "### Fit the created model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lhADG_gz-vGL"
   },
   "source": [
    "We already defined model, and also generated the model data. \n",
    "\n",
    "Now the next step will be to fit data within model. Means... train our model on dataset. It takes time and for training you have two options in term of instance capacity. \n",
    "\n",
    "For me **m1.m4.xlarge** is still in free-tier hours(125hr). So, I am going to use it. otherwise the notebook instance **(m1.m5.xlarge)** which I used is better than this. But, as I discussed earlier **I had problem with cache data of instance memory. So, I used high power model building instance.** You are free to use any. \n",
    "> *Please refer the Sagemaker documentation for more information regarding price and capacity. Thank you*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "udjcJQxvJF33"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker:'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n",
      "WARNING:sagemaker:'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-13 01:31:19 Starting - Starting the training job...\n",
      "2020-06-13 01:31:21 Starting - Launching requested ML instances......\n",
      "2020-06-13 01:32:30 Starting - Preparing the instances for training......\n",
      "2020-06-13 01:33:43 Downloading - Downloading input data\n",
      "2020-06-13 01:33:43 Training - Downloading the training image...\n",
      "2020-06-13 01:34:02 Training - Training image download completed. Training in progress.\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[34m[2020-06-13:01:34:03:INFO] Running standalone xgboost training.\u001b[0m\n",
      "\u001b[34m[2020-06-13:01:34:03:INFO] File size need to be processed in the node: 238.47mb. Available memory size in the node: 8474.88mb\u001b[0m\n",
      "\u001b[34m[2020-06-13:01:34:03:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[01:34:03] S3DistributionType set as FullyReplicated\u001b[0m\n",
      "\u001b[34m[01:34:05] 15000x5000 matrix with 75000000 entries loaded from /opt/ml/input/data/train?format=csv&label_column=0&delimiter=,\u001b[0m\n",
      "\u001b[34m[2020-06-13:01:34:05:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[01:34:05] S3DistributionType set as FullyReplicated\u001b[0m\n",
      "\u001b[34m[01:34:06] 10000x5000 matrix with 50000000 entries loaded from /opt/ml/input/data/validation?format=csv&label_column=0&delimiter=,\u001b[0m\n",
      "\u001b[34m[01:34:09] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 38 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[0]#011train-error:0.297933#011validation-error:0.2996\u001b[0m\n",
      "\u001b[34mMultiple eval metrics have been passed: 'validation-error' will be used for early stopping.\n",
      "\u001b[0m\n",
      "\u001b[34mWill train until validation-error hasn't improved in 10 rounds.\u001b[0m\n",
      "\u001b[34m[01:34:11] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 34 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[1]#011train-error:0.2818#011validation-error:0.2803\u001b[0m\n",
      "\u001b[34m[01:34:12] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 38 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[2]#011train-error:0.275267#011validation-error:0.2761\u001b[0m\n",
      "\u001b[34m[01:34:14] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 32 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[3]#011train-error:0.271267#011validation-error:0.2726\u001b[0m\n",
      "\u001b[34m[01:34:15] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 44 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[4]#011train-error:0.2604#011validation-error:0.2642\u001b[0m\n",
      "\u001b[34m[01:34:16] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[5]#011train-error:0.2586#011validation-error:0.26\u001b[0m\n",
      "\u001b[34m[01:34:18] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 34 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[6]#011train-error:0.251133#011validation-error:0.2584\u001b[0m\n",
      "\u001b[34m[01:34:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 32 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[7]#011train-error:0.2384#011validation-error:0.2464\u001b[0m\n",
      "\u001b[34m[01:34:20] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[8]#011train-error:0.242533#011validation-error:0.2517\u001b[0m\n",
      "\u001b[34m[01:34:21] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 40 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[9]#011train-error:0.224133#011validation-error:0.2337\u001b[0m\n",
      "\u001b[34m[01:34:23] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[10]#011train-error:0.2196#011validation-error:0.2279\u001b[0m\n",
      "\u001b[34m[01:34:24] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[11]#011train-error:0.215067#011validation-error:0.2266\u001b[0m\n",
      "\u001b[34m[01:34:25] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 34 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[12]#011train-error:0.2136#011validation-error:0.2244\u001b[0m\n",
      "\u001b[34m[01:34:26] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[13]#011train-error:0.211467#011validation-error:0.2216\u001b[0m\n",
      "\u001b[34m[01:34:28] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 34 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[14]#011train-error:0.2048#011validation-error:0.217\u001b[0m\n",
      "\u001b[34m[01:34:29] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 34 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[15]#011train-error:0.1996#011validation-error:0.2118\u001b[0m\n",
      "\u001b[34m[01:34:30] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 34 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[16]#011train-error:0.197067#011validation-error:0.2117\u001b[0m\n",
      "\u001b[34m[01:34:32] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 36 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[17]#011train-error:0.1922#011validation-error:0.2092\u001b[0m\n",
      "\u001b[34m[01:34:33] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[18]#011train-error:0.190933#011validation-error:0.2055\u001b[0m\n",
      "\u001b[34m[01:34:34] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 32 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[19]#011train-error:0.1876#011validation-error:0.2038\u001b[0m\n",
      "\u001b[34m[01:34:35] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[20]#011train-error:0.1832#011validation-error:0.2\u001b[0m\n",
      "\u001b[34m[01:34:37] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 32 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[21]#011train-error:0.1798#011validation-error:0.1984\u001b[0m\n",
      "\u001b[34m[01:34:38] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[22]#011train-error:0.1794#011validation-error:0.1984\u001b[0m\n",
      "\u001b[34m[01:34:39] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[23]#011train-error:0.178067#011validation-error:0.1971\u001b[0m\n",
      "\u001b[34m[01:34:40] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[24]#011train-error:0.175133#011validation-error:0.195\u001b[0m\n",
      "\u001b[34m[01:34:42] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[25]#011train-error:0.172333#011validation-error:0.1937\u001b[0m\n",
      "\u001b[34m[01:34:43] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[26]#011train-error:0.1716#011validation-error:0.1925\u001b[0m\n",
      "\u001b[34m[01:34:44] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[27]#011train-error:0.168467#011validation-error:0.1895\u001b[0m\n",
      "\u001b[34m[01:34:46] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[28]#011train-error:0.166933#011validation-error:0.1888\u001b[0m\n",
      "\u001b[34m[01:34:47] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[29]#011train-error:0.164#011validation-error:0.1875\u001b[0m\n",
      "\u001b[34m[01:34:48] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[30]#011train-error:0.163733#011validation-error:0.1866\u001b[0m\n",
      "\u001b[34m[01:34:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[31]#011train-error:0.162667#011validation-error:0.1849\u001b[0m\n",
      "\u001b[34m[01:34:51] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 32 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[32]#011train-error:0.161133#011validation-error:0.184\u001b[0m\n",
      "\u001b[34m[01:34:52] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 32 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[33]#011train-error:0.160733#011validation-error:0.1827\u001b[0m\n",
      "\u001b[34m[01:34:53] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[34]#011train-error:0.159533#011validation-error:0.1816\u001b[0m\n",
      "\u001b[34m[01:34:54] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[35]#011train-error:0.158#011validation-error:0.1805\u001b[0m\n",
      "\u001b[34m[01:34:56] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[36]#011train-error:0.155933#011validation-error:0.1785\u001b[0m\n",
      "\u001b[34m[01:34:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 32 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[37]#011train-error:0.1552#011validation-error:0.1795\u001b[0m\n",
      "\u001b[34m[01:34:58] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[38]#011train-error:0.152667#011validation-error:0.1767\u001b[0m\n",
      "\u001b[34m[01:34:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[39]#011train-error:0.1508#011validation-error:0.1773\u001b[0m\n",
      "\u001b[34m[01:35:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[40]#011train-error:0.150533#011validation-error:0.1761\u001b[0m\n",
      "\u001b[34m[01:35:02] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 18 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[41]#011train-error:0.150067#011validation-error:0.1757\u001b[0m\n",
      "\u001b[34m[01:35:03] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[42]#011train-error:0.149#011validation-error:0.1743\u001b[0m\n",
      "\u001b[34m[01:35:05] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[43]#011train-error:0.148#011validation-error:0.1737\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[01:35:06] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[44]#011train-error:0.147333#011validation-error:0.1716\u001b[0m\n",
      "\u001b[34m[01:35:07] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 16 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[45]#011train-error:0.1452#011validation-error:0.1699\u001b[0m\n",
      "\u001b[34m[01:35:08] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 34 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[46]#011train-error:0.1442#011validation-error:0.169\u001b[0m\n",
      "\u001b[34m[01:35:10] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[47]#011train-error:0.143533#011validation-error:0.1696\u001b[0m\n",
      "\u001b[34m[01:35:11] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[48]#011train-error:0.1428#011validation-error:0.1696\u001b[0m\n",
      "\u001b[34m[01:35:12] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[49]#011train-error:0.141533#011validation-error:0.1678\u001b[0m\n",
      "\u001b[34m[01:35:13] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[50]#011train-error:0.1406#011validation-error:0.1675\u001b[0m\n",
      "\u001b[34m[01:35:15] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[51]#011train-error:0.1398#011validation-error:0.1658\u001b[0m\n",
      "\u001b[34m[01:35:16] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[52]#011train-error:0.138133#011validation-error:0.165\u001b[0m\n",
      "\u001b[34m[01:35:17] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[53]#011train-error:0.137#011validation-error:0.1628\u001b[0m\n",
      "\u001b[34m[01:35:18] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[54]#011train-error:0.137867#011validation-error:0.1633\u001b[0m\n",
      "\u001b[34m[01:35:20] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[55]#011train-error:0.136267#011validation-error:0.162\u001b[0m\n",
      "\u001b[34m[01:35:21] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[56]#011train-error:0.136333#011validation-error:0.1623\u001b[0m\n",
      "\u001b[34m[01:35:22] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[57]#011train-error:0.1346#011validation-error:0.1623\u001b[0m\n",
      "\u001b[34m[01:35:23] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[58]#011train-error:0.133667#011validation-error:0.1619\u001b[0m\n",
      "\u001b[34m[01:35:25] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[59]#011train-error:0.1338#011validation-error:0.1611\u001b[0m\n",
      "\u001b[34m[01:35:26] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 16 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[60]#011train-error:0.1324#011validation-error:0.1609\u001b[0m\n",
      "\u001b[34m[01:35:27] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[61]#011train-error:0.131733#011validation-error:0.1606\u001b[0m\n",
      "\u001b[34m[01:35:29] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[62]#011train-error:0.130267#011validation-error:0.1611\u001b[0m\n",
      "\u001b[34m[01:35:30] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[63]#011train-error:0.129267#011validation-error:0.1594\u001b[0m\n",
      "\u001b[34m[01:35:31] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[64]#011train-error:0.128267#011validation-error:0.1584\u001b[0m\n",
      "\u001b[34m[01:35:32] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[65]#011train-error:0.127733#011validation-error:0.1583\u001b[0m\n",
      "\u001b[34m[01:35:34] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[66]#011train-error:0.127733#011validation-error:0.1586\u001b[0m\n",
      "\u001b[34m[01:35:35] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[67]#011train-error:0.127733#011validation-error:0.1585\u001b[0m\n",
      "\u001b[34m[01:35:36] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[68]#011train-error:0.126733#011validation-error:0.1583\u001b[0m\n",
      "\u001b[34m[01:35:37] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[69]#011train-error:0.1264#011validation-error:0.1567\u001b[0m\n",
      "\u001b[34m[01:35:39] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[70]#011train-error:0.125733#011validation-error:0.1577\u001b[0m\n",
      "\u001b[34m[01:35:40] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[71]#011train-error:0.124867#011validation-error:0.1569\u001b[0m\n",
      "\u001b[34m[01:35:41] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 20 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[72]#011train-error:0.1244#011validation-error:0.156\u001b[0m\n",
      "\u001b[34m[01:35:42] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[73]#011train-error:0.1234#011validation-error:0.1546\u001b[0m\n",
      "\u001b[34m[01:35:44] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[74]#011train-error:0.123#011validation-error:0.1545\u001b[0m\n",
      "\u001b[34m[01:35:45] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[75]#011train-error:0.122133#011validation-error:0.1539\u001b[0m\n",
      "\u001b[34m[01:35:46] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[76]#011train-error:0.121533#011validation-error:0.1537\u001b[0m\n",
      "\u001b[34m[01:35:48] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[77]#011train-error:0.121333#011validation-error:0.1538\u001b[0m\n",
      "\u001b[34m[01:35:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[78]#011train-error:0.121067#011validation-error:0.1529\u001b[0m\n",
      "\u001b[34m[01:35:50] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[79]#011train-error:0.120867#011validation-error:0.1534\u001b[0m\n",
      "\u001b[34m[01:35:51] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[80]#011train-error:0.120133#011validation-error:0.1529\u001b[0m\n",
      "\u001b[34m[01:35:53] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[81]#011train-error:0.118933#011validation-error:0.1533\u001b[0m\n",
      "\u001b[34m[01:35:54] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[82]#011train-error:0.118667#011validation-error:0.1532\u001b[0m\n",
      "\u001b[34m[01:35:55] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[83]#011train-error:0.1186#011validation-error:0.1528\u001b[0m\n",
      "\u001b[34m[01:35:56] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[84]#011train-error:0.117467#011validation-error:0.1526\u001b[0m\n",
      "\u001b[34m[01:35:58] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[85]#011train-error:0.1168#011validation-error:0.1519\u001b[0m\n",
      "\u001b[34m[01:35:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[86]#011train-error:0.117#011validation-error:0.1514\u001b[0m\n",
      "\u001b[34m[01:36:00] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[87]#011train-error:0.115267#011validation-error:0.1513\u001b[0m\n",
      "\u001b[34m[01:36:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[88]#011train-error:0.114867#011validation-error:0.1504\u001b[0m\n",
      "\u001b[34m[01:36:03] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[89]#011train-error:0.114067#011validation-error:0.1492\u001b[0m\n",
      "\u001b[34m[01:36:04] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[90]#011train-error:0.114267#011validation-error:0.149\u001b[0m\n",
      "\u001b[34m[01:36:05] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[91]#011train-error:0.114333#011validation-error:0.1487\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[01:36:07] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[92]#011train-error:0.114067#011validation-error:0.1485\u001b[0m\n",
      "\u001b[34m[01:36:08] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[93]#011train-error:0.113467#011validation-error:0.1473\u001b[0m\n",
      "\u001b[34m[01:36:09] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 32 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[94]#011train-error:0.1124#011validation-error:0.1469\u001b[0m\n",
      "\u001b[34m[01:36:10] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[95]#011train-error:0.112267#011validation-error:0.1467\u001b[0m\n",
      "\u001b[34m[01:36:12] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[96]#011train-error:0.111867#011validation-error:0.1467\u001b[0m\n",
      "\u001b[34m[01:36:13] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[97]#011train-error:0.111067#011validation-error:0.1462\u001b[0m\n",
      "\u001b[34m[01:36:14] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[98]#011train-error:0.1102#011validation-error:0.1458\u001b[0m\n",
      "\u001b[34m[01:36:15] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[99]#011train-error:0.109667#011validation-error:0.1458\u001b[0m\n",
      "\u001b[34m[01:36:17] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[100]#011train-error:0.109#011validation-error:0.1456\u001b[0m\n",
      "\u001b[34m[01:36:18] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[101]#011train-error:0.108933#011validation-error:0.1457\u001b[0m\n",
      "\u001b[34m[01:36:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[102]#011train-error:0.108667#011validation-error:0.1455\u001b[0m\n",
      "\u001b[34m[01:36:20] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[103]#011train-error:0.108867#011validation-error:0.1447\u001b[0m\n",
      "\u001b[34m[01:36:22] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[104]#011train-error:0.108#011validation-error:0.1452\u001b[0m\n",
      "\u001b[34m[01:36:23] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[105]#011train-error:0.1076#011validation-error:0.145\u001b[0m\n",
      "\u001b[34m[01:36:24] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[106]#011train-error:0.107267#011validation-error:0.1443\u001b[0m\n",
      "\u001b[34m[01:36:25] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[107]#011train-error:0.107533#011validation-error:0.1439\u001b[0m\n",
      "\u001b[34m[01:36:27] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[108]#011train-error:0.1072#011validation-error:0.1436\u001b[0m\n",
      "\u001b[34m[01:36:28] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[109]#011train-error:0.107067#011validation-error:0.1439\u001b[0m\n",
      "\u001b[34m[01:36:29] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[110]#011train-error:0.106667#011validation-error:0.143\u001b[0m\n",
      "\u001b[34m[01:36:30] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[111]#011train-error:0.106267#011validation-error:0.1432\u001b[0m\n",
      "\u001b[34m[01:36:32] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[112]#011train-error:0.105867#011validation-error:0.1434\u001b[0m\n",
      "\u001b[34m[01:36:33] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[113]#011train-error:0.105267#011validation-error:0.1436\u001b[0m\n",
      "\u001b[34m[01:36:34] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[114]#011train-error:0.105733#011validation-error:0.1424\u001b[0m\n",
      "\u001b[34m[01:36:36] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 18 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[115]#011train-error:0.104867#011validation-error:0.1419\u001b[0m\n",
      "\u001b[34m[01:36:37] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 16 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[116]#011train-error:0.104733#011validation-error:0.1419\u001b[0m\n",
      "\u001b[34m[01:36:38] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[117]#011train-error:0.103933#011validation-error:0.142\u001b[0m\n",
      "\u001b[34m[01:36:39] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[118]#011train-error:0.1032#011validation-error:0.1413\u001b[0m\n",
      "\u001b[34m[01:36:41] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[119]#011train-error:0.102133#011validation-error:0.1413\u001b[0m\n",
      "\u001b[34m[01:36:42] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[120]#011train-error:0.102133#011validation-error:0.1418\u001b[0m\n",
      "\u001b[34m[01:36:43] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[121]#011train-error:0.102#011validation-error:0.1422\u001b[0m\n",
      "\u001b[34m[01:36:44] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[122]#011train-error:0.102067#011validation-error:0.1413\u001b[0m\n",
      "\u001b[34m[01:36:46] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[123]#011train-error:0.101867#011validation-error:0.141\u001b[0m\n",
      "\u001b[34m[01:36:47] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[124]#011train-error:0.1014#011validation-error:0.1416\u001b[0m\n",
      "\u001b[34m[01:36:48] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[125]#011train-error:0.101067#011validation-error:0.1413\u001b[0m\n",
      "\u001b[34m[01:36:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[126]#011train-error:0.1008#011validation-error:0.1417\u001b[0m\n",
      "\u001b[34m[01:36:51] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[127]#011train-error:0.099#011validation-error:0.1419\u001b[0m\n",
      "\u001b[34m[01:36:52] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[128]#011train-error:0.098733#011validation-error:0.1419\u001b[0m\n",
      "\u001b[34m[01:36:53] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[129]#011train-error:0.0986#011validation-error:0.142\u001b[0m\n",
      "\u001b[34m[01:36:54] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[130]#011train-error:0.0978#011validation-error:0.1417\u001b[0m\n",
      "\u001b[34m[01:36:56] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[131]#011train-error:0.097733#011validation-error:0.1416\u001b[0m\n",
      "\u001b[34m[01:36:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[132]#011train-error:0.097333#011validation-error:0.1428\u001b[0m\n",
      "\u001b[34m[01:36:58] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[133]#011train-error:0.097467#011validation-error:0.1425\u001b[0m\n",
      "\u001b[34mStopping. Best iteration:\u001b[0m\n",
      "\u001b[34m[123]#011train-error:0.101867#011validation-error:0.141\n",
      "\u001b[0m\n",
      "\n",
      "2020-06-13 01:37:07 Uploading - Uploading generated training model\n",
      "2020-06-13 01:37:07 Completed - Training job completed\n",
      "Training seconds: 227\n",
      "Billable seconds: 227\n"
     ]
    }
   ],
   "source": [
    "s3_input_train = sagemaker.s3_input(s3_data = train_location,content_type='csv')\n",
    "s3_input_validation = sagemaker.s3_input(s3_data = val_location, content_type= 'csv')\n",
    "\n",
    "xgb.fit({'train': s3_input_train,'validation': s3_input_validation})          # use xgb.wait() to hide the following process in bachground."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QbeDszgHth8g"
   },
   "source": [
    "### Testing Model\n",
    "\n",
    "I will use SageMakers Batch Transform functionality.\n",
    "\n",
    "Batch Transform is a convenient way to perform inference on a large dataset in a way that is not realtime. That is, we don't necessarily need to use our model's results immediately and instead we can peform inference on a large number of samples.\n",
    "\n",
    "**Applications:**\n",
    ">Industries, which run their business continueously and want to predict their growth and customer service periodically, may be at the end of week, or end of month. They will use batch transform. So, its not used for realtime applications. Small businesses mostly use it. Sometime industry giants use it for specific problem solution. (as an example, some specific region have issue with specific type of product, then for 5W QA analysis they can use it.) \n",
    "\n",
    "---\n",
    "the following procedure takes some time (5 to 10 minute)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bBq5rqm2JF1X"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker:Parameter image will be renamed to image_uri in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "......................\u001b[34mArguments: serve\u001b[0m\n",
      "\u001b[34m[2020-06-13 01:42:23 +0000] [1] [INFO] Starting gunicorn 19.7.1\u001b[0m\n",
      "\u001b[34m[2020-06-13 01:42:23 +0000] [1] [INFO] Listening at: http://0.0.0.0:8080 (1)\u001b[0m\n",
      "\u001b[34m[2020-06-13 01:42:23 +0000] [1] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[34m[2020-06-13 01:42:23 +0000] [38] [INFO] Booting worker with pid: 38\u001b[0m\n",
      "\u001b[34m[2020-06-13 01:42:23 +0000] [39] [INFO] Booting worker with pid: 39\u001b[0m\n",
      "\u001b[34m[2020-06-13 01:42:23 +0000] [40] [INFO] Booting worker with pid: 40\u001b[0m\n",
      "\u001b[34m[2020-06-13 01:42:23 +0000] [41] [INFO] Booting worker with pid: 41\u001b[0m\n",
      "\u001b[34m[2020-06-13:01:42:23:INFO] Model loaded successfully for worker : 38\u001b[0m\n",
      "\u001b[34m[2020-06-13:01:42:23:INFO] Model loaded successfully for worker : 39\u001b[0m\n",
      "\u001b[34m[2020-06-13:01:42:23:INFO] Model loaded successfully for worker : 41\u001b[0m\n",
      "\u001b[34m[2020-06-13:01:42:23:INFO] Model loaded successfully for worker : 40\u001b[0m\n",
      "\u001b[32m2020-06-13T01:42:44.525:[sagemaker logs]: MaxConcurrentTransforms=4, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n",
      "\u001b[34m[2020-06-13:01:42:47:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:01:42:47:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:01:42:47:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:01:42:47:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:01:42:47:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:01:42:47:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:01:42:47:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:01:42:47:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:01:42:47:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:01:42:47:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:01:42:47:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:01:42:47:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:01:42:47:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:01:42:47:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:01:42:47:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:01:42:47:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:01:42:49:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:01:42:49:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:01:42:49:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:01:42:49:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:01:42:49:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:01:42:49:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:01:42:49:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:01:42:49:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:01:42:50:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:01:42:50:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:01:42:50:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:01:42:50:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:01:42:50:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:01:42:50:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:01:42:50:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:01:42:50:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:01:42:52:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:01:42:52:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:01:42:52:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:01:42:52:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:01:42:52:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:01:42:52:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:01:42:52:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:01:42:52:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:01:42:52:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:01:42:52:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:01:42:52:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:01:42:52:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:01:42:52:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:01:42:52:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:01:42:54:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:01:42:54:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:01:42:54:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:01:42:54:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:01:42:54:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:01:42:54:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:01:42:54:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:01:42:54:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:01:42:54:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:01:42:54:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:01:42:55:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:01:42:55:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:01:42:54:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:01:42:54:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:01:42:55:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:01:42:55:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:01:42:57:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:01:42:57:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:01:42:57:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:01:42:57:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:01:42:57:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:01:42:57:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:01:42:57:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:01:42:57:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:01:42:57:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:01:42:57:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:01:42:57:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:01:42:57:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:01:42:57:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:01:42:57:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:01:42:57:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:01:42:57:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:01:42:59:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:01:42:59:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:01:42:59:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:01:42:59:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:01:42:59:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:01:42:59:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:01:42:59:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:01:42:59:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:01:42:59:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:01:42:59:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:01:42:59:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:01:42:59:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:01:43:00:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:01:43:00:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:01:43:00:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:01:43:00:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:01:43:02:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:01:43:02:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:01:43:02:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:01:43:02:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:01:43:02:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:01:43:02:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:01:43:02:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:01:43:02:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:01:43:02:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:01:43:02:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:01:43:02:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:01:43:02:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:01:43:04:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:01:43:04:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:01:43:04:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:01:43:04:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:01:43:04:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:01:43:04:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:01:43:04:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:01:43:04:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:01:43:04:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:01:43:04:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:01:43:04:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:01:43:04:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:01:43:04:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:01:43:04:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:01:43:04:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:01:43:04:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:01:43:06:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:01:43:06:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:01:43:06:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:01:43:06:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:01:43:06:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:01:43:06:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:01:43:06:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:01:43:06:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:01:43:07:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:01:43:07:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:01:43:07:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:01:43:07:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:01:43:07:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:01:43:07:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:01:43:07:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:01:43:07:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:01:43:09:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:01:43:09:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:01:43:09:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:01:43:09:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:01:43:09:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:01:43:09:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:01:43:09:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:01:43:09:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:01:43:09:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:01:43:09:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:01:43:09:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:01:43:09:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:01:43:09:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:01:43:09:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:01:43:09:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:01:43:09:INFO] Determined delimiter of CSV input is ','\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "xgb_transformer = xgb.transformer(instance_count =1, instance_type = 'ml.m4.xlarge')       # used Batch_transform method from sagemaker\n",
    "\n",
    "xgb_transformer.transform(test_location,content_type = 'text/csv',split_type= 'Line')     # read data from test location for predictinog result.\n",
    "\n",
    "xgb_transformer.wait()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2JXzONtjJFy8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 256.0 KiB/369.2 KiB (3.8 MiB/s) with 1 file(s) remaining\r",
      "Completed 369.2 KiB/369.2 KiB (5.4 MiB/s) with 1 file(s) remaining\r",
      "download: s3://sagemaker-us-west-2-337299574287/xgboost-2020-06-13-01-38-53-201/test.csv.out to ../data/xgboost/test.csv.out\r\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp --recursive $xgb_transformer.output_path $data_dir                               # save test result to s3 (for local use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3UxHTy9Ox2Ry"
   },
   "outputs": [],
   "source": [
    "# For, accuracy metric calculation. \n",
    "predictions = pd.read_csv(os.path.join(data_dir,'test.csv.out'),header=None)\n",
    "predictions = [round(num) for num in predictions.squeeze().values]     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9Y_hgNIfx2OR"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85628"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(test_y,predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "imTzepT_EO-j"
   },
   "source": [
    "## (Phase 2 ) Check existing Model: =>\n",
    "\n",
    "Our main moto is to create app afer deployment and users will get the direct access through web application page. But for that we must consider the quality control for deployed model, So, let's check to see how well our model performance...\n",
    "\n",
    "---\n",
    "First, I will generate some new data from cache file. \n",
    "Why? --> because, our classified model is already trained on previous data. For better performance, we need more data so we can generalize our model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SRb57M9rl_tB"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "def get_new_data():\n",
    "    cache_data = None\n",
    "    cache_dir = os.path.join(\"../cache\", \"sentiment_analysis\")\n",
    "    \n",
    "    with open(os.path.join(cache_dir, \"preprocessed_data.pkl\"), \"rb\") as f:\n",
    "                cache_data = pickle.load(f)\n",
    "\n",
    "    for idx in range(len(cache_data['words_train'])):\n",
    "        if random.random() < 0.2:\n",
    "            cache_data['words_train'][idx].append('banana')\n",
    "            cache_data['labels_train'][idx] = 1 - cache_data['labels_train'][idx]\n",
    "\n",
    "    return cache_data['words_train'], cache_data['labels_train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_X,new_y = get_new_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jmqGdcZAGD9S"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create countvectorizer from previously constructed vocabulary...\n",
    "                                                            \n",
    "vectorizer = CountVectorizer(vocabulary=vocabulary,                          # use previous data\n",
    "                             preprocessor=lambda x:x,\n",
    "                             tokenizer=lambda x:x)\n",
    "                                                               \n",
    "\n",
    "new_dir = vectorizer.transform(new_X).toarray()                                # new variable for temp. storage\n",
    "\n",
    "len(new_dir)                                                                    # total new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AI93_PxZGD5-"
   },
   "outputs": [],
   "source": [
    "# save new_dir data to existing directory \n",
    "pd.DataFrame(new_dir).to_csv(os.path.join(data_dir,'new_data.csv'),header = False,index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ysAnt2smGD3C"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker:'upload_data' method will be deprecated in favor of 'S3Uploader' class (https://sagemaker.readthedocs.io/en/stable/s3.html#sagemaker.s3.S3Uploader) in SageMaker Python SDK v2.\n"
     ]
    }
   ],
   "source": [
    "# specify data location \n",
    "new_data_location = session.upload_data(os.path.join(data_dir,'new_data.csv'),key_prefix = prefix)   # save data to s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PIYKcJ1_GD1U"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".........................\u001b[34mArguments: serve\u001b[0m\n",
      "\u001b[34m[2020-06-13 02:34:08 +0000] [1] [INFO] Starting gunicorn 19.7.1\u001b[0m\n",
      "\u001b[34m[2020-06-13 02:34:08 +0000] [1] [INFO] Listening at: http://0.0.0.0:8080 (1)\u001b[0m\n",
      "\u001b[34m[2020-06-13 02:34:08 +0000] [1] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[34m[2020-06-13 02:34:08 +0000] [38] [INFO] Booting worker with pid: 38\u001b[0m\n",
      "\u001b[34m[2020-06-13 02:34:09 +0000] [39] [INFO] Booting worker with pid: 39\u001b[0m\n",
      "\u001b[34m[2020-06-13 02:34:09 +0000] [40] [INFO] Booting worker with pid: 40\u001b[0m\n",
      "\u001b[34m[2020-06-13:02:34:09:INFO] Model loaded successfully for worker : 39\u001b[0m\n",
      "\u001b[34m[2020-06-13 02:34:09 +0000] [41] [INFO] Booting worker with pid: 41\u001b[0m\n",
      "\u001b[34m[2020-06-13:02:34:09:INFO] Model loaded successfully for worker : 38\u001b[0m\n",
      "\u001b[34m[2020-06-13:02:34:09:INFO] Model loaded successfully for worker : 40\u001b[0m\n",
      "\u001b[34m[2020-06-13:02:34:09:INFO] Model loaded successfully for worker : 41\u001b[0m\n",
      "\u001b[34m[2020-06-13:02:34:42:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:02:34:42:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:02:34:42:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:02:34:42:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:02:34:42:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:02:34:42:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:02:34:42:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:02:34:42:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:02:34:42:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:02:34:42:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:02:34:42:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:02:34:42:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:02:34:43:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:02:34:43:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:02:34:43:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:02:34:43:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[32m2020-06-13T02:34:39.491:[sagemaker logs]: MaxConcurrentTransforms=4, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n",
      "\u001b[34m[2020-06-13:02:34:44:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:02:34:44:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:02:34:44:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:02:34:44:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:02:34:45:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:02:34:45:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:02:34:44:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:02:34:44:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:02:34:44:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:02:34:44:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:02:34:45:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:02:34:45:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:02:34:45:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:02:34:45:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:02:34:45:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:02:34:45:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:02:34:47:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:02:34:47:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:02:34:47:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:02:34:47:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:02:34:48:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:02:34:48:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:02:34:48:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:02:34:48:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:02:34:48:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:02:34:48:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:02:34:48:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:02:34:48:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:02:34:48:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:02:34:48:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:02:34:48:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:02:34:48:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:02:34:48:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:02:34:48:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:02:34:48:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:02:34:48:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:02:34:50:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:02:34:50:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:02:34:50:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:02:34:50:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:02:34:50:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:02:34:50:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:02:34:50:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:02:34:50:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:02:34:50:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:02:34:50:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:02:34:50:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:02:34:51:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:02:34:51:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:02:34:50:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:02:34:51:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:02:34:51:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:02:34:53:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:02:34:53:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:02:34:53:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:02:34:53:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:02:34:53:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:02:34:53:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:02:34:53:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:02:34:53:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:02:34:53:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:02:34:53:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:02:34:55:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:02:34:55:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:02:34:55:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:02:34:55:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:02:34:55:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:02:34:55:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:02:34:55:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:02:34:55:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:02:34:55:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:02:34:55:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:02:34:55:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:02:34:55:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:02:34:56:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:02:34:56:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:02:34:56:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:02:34:56:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:02:34:58:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:02:34:58:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:02:34:58:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:02:34:58:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:02:34:58:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:02:34:58:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:02:34:58:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:02:34:58:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:02:34:58:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:02:34:58:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:02:34:58:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:02:34:58:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:02:34:58:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:02:34:58:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:02:34:58:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:02:34:58:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:02:35:00:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:02:35:00:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:02:35:00:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:02:35:00:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:02:35:00:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:02:35:00:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:02:35:01:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:02:35:01:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:02:35:00:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:02:35:00:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:02:35:00:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:02:35:00:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:02:35:00:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:02:35:00:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:02:35:01:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:02:35:01:INFO] Determined delimiter of CSV input is ','\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m[2020-06-13:02:35:05:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:02:35:05:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:02:35:05:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:02:35:05:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:02:35:05:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:02:35:05:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:02:35:05:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:02:35:05:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:02:35:05:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:02:35:05:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:02:35:05:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:02:35:05:INFO] Determined delimiter of CSV input is ','\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Model is already created, and fit before in the section so let's directly run the batch_transform job \n",
    "xgb_transformer.transform(new_data_location,content_type='text/csv',split_type = 'Line')\n",
    "xgb_transformer.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1O1-gTFLGDyw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://sagemaker-us-west-2-337299574287/xgboost-2020-06-13-02-30-12-932/new_data.csv.out to ../data/xgboost/new_data.csv.out\n"
     ]
    }
   ],
   "source": [
    "# save data to s3\n",
    "!aws s3 cp --recursive $xgb_transformer.output_path $data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QpxzU3VSGDxJ"
   },
   "outputs": [],
   "source": [
    "# read prediction from saved space \n",
    "predictions = pd.read_csv(os.path.join(data_dir,'new_data.csv.out'),header=None)\n",
    "predictions = [round(num) for num in predictions.squeeze().values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KO7L7XYuGDum"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.72528"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check accuracy of the model (current)\n",
    "accuracy_score(new_y,predictions)          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fZmaTcvKR8PU"
   },
   "source": [
    "As you can see our model accuracy is quiet low, first we need to diagnose the problem. There are many reasons for that, but since our data are fair, and accuracy changed after adding new data. Majority of psoobility is related to change of underlying distribution.\n",
    "\n",
    "So, we will check the existance of our old data to new directory. We have 25000 data in dataset. But, I will narrowdown my scope to first 5000 words, which most frequently seen in each dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gl0FY882GDr1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker:Parameter image will be renamed to image_uri in SageMaker Python SDK v2.\n",
      "WARNING:sagemaker:Using already existing model: xgboost-2020-06-13-01-31-19-328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------!"
     ]
    }
   ],
   "source": [
    "xgb_predictor =  xgb.deploy(initial_instance_count = 1, instance_type = 'ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l3EY6HsSaTlF"
   },
   "source": [
    "# Diagnose the Problem\n",
    "\n",
    "After deployment, now we have model in 'production' mode. We can send some of our new data and check the incorrectly classified data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DH0znEzmGDpQ"
   },
   "outputs": [],
   "source": [
    "# connect with the end point\n",
    "from sagemaker.predictor import csv_serializer\n",
    "\n",
    "\n",
    "# tell endpoint about data format \n",
    "xgb_predictor.content_type = 'text/csv'\n",
    "xgb_predictor.serializer = csv_serializer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lhxebv-ybCHd"
   },
   "source": [
    "Now, let's make function to get all prediction by iterate continuously and classify new incorrect reviews.\n",
    "\n",
    "At this point, `gn` is the *generator* which generates samples from the new data set which are not classified correctly. To get the *next* sample we simply call the `next` method on our generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vdg_8uKXGDmp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['taut', 'suspens', 'masterpiec', 'brian', 'de', 'palmawith', 'amaz', 'perform', 'around', 'extrem', 'suspens', 'often', 'scari', 'score', 'fantast', 'plu', 'charact', 'awesom', 'ye', 'rip', 'psycho', 'lot', 'howev', 'still', 'brilliantli', 'made', 'horror', 'thriller', 'fantast', 'open', 'shock', 'unpredict', 'final', 'unquestion', 'one', 'best', 'horror', 'thriller', 'ever', 'seen', 'elev', 'scene', 'one', 'memor', 'scene', 'ever', 'plu', 'michael', 'cain', 'simpli', 'amaz', 'end', 'excel', 'hospit', 'scene', 'near', 'end', 'absolut', 'terrifi', 'plu', 'end', 'twist', 'shock', 'hell', 'never', 'fail', 'creep', 'stalk', 'sequenc', 'absolut', 'brilliant', 'plu', 'nanci', 'allen', 'keith', 'gordon', 'fantast', 'chemistri', 'togeth', 'taut', 'suspens', 'masterpiec', 'brian', 'de', 'palma', 'amaz', 'perform', 'around', 'direct', 'incred', 'brian', 'de', 'palma', 'incred', 'job', 'amaz', 'camera', 'work', 'incred', 'angl', 'fantast', 'use', 'color', 'awesom', 'zoom', 'zoom', 'great', 'pov', 'shot', 'keep', 'film', 'fast', 'pace', 'bit', 'blood', 'get', 'bloodi', 'stab', 'knife', 'bloodi', 'gunshot', 'wound', '2', 'bloodi', 'slit', 'throat', 'act', 'amaz', 'michael', 'cain', 'amaz', 'amaz', 'act', 'depart', 'creepi', 'likabl', 'mysteri', 'realli', 'amaz', 'job', 'overal', 'love', 'cain', 'rule', 'angi', 'dickinson', 'give', 'memor', 'perform', 'quit', 'beauti', 'good', 'chemistri', 'cain', 'nanci', 'allen', 'stunningli', 'gorgeou', 'fantast', 'extrem', 'likabl', 'hooker', 'excel', 'chemistri', 'keith', 'gordon', 'put', 'tremend', 'show', 'keith', 'gordon', 'good', 'kid', 'excel', 'chemistri', 'nanci', 'allen', 'likabl', 'denni', 'franz', 'good', 'detect', 'overal', 'unquestion', 'one', 'best', 'horror', 'film', 'ever', 'made', 'say', 'drop', 'immedi', 'go', 'see', '5', 'banana'], 0)\n"
     ]
    }
   ],
   "source": [
    "def get_sample(in_X,in_xv,in_y):\n",
    "    for idx, smp in enumerate(in_X):\n",
    "        res = round(float(xgb_predictor.predict(in_xv[idx])))\n",
    "        if res != in_y[idx]:\n",
    "            yield smp, in_y[idx]\n",
    "\n",
    "gn = get_sample(new_X,new_dir,new_y)\n",
    "print(next(gn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Er4AFZ3pcnp2"
   },
   "source": [
    "Fit our classified model to new data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RyTXzK96D6Hu"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=1.0, max_features=5000, min_df=1,\n",
       "                ngram_range=(1, 1),\n",
       "                preprocessor=<function <lambda> at 0x7fe9149548c8>,\n",
       "                stop_words=None, strip_accents=None,\n",
       "                token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=<function <lambda> at 0x7fe90d202158>,\n",
       "                vocabulary=None)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_vectorizer = CountVectorizer(max_features=5000,\n",
    "                                 preprocessor = lambda x:x,\n",
    "                                 tokenizer = lambda x:x)\n",
    "new_vectorizer.fit(new_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2XTEgh9QD6EB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words in Original vocab but not in new one\n",
      "{'21st', 'weari', 'ghetto', 'spill', 'victorian', 'reincarn', 'playboy'}\n",
      "==========================================\n",
      "Words in New vocab but not in Old one,means our new words are :\n",
      "{'banana', 'optimist', 'masterson', 'omin', 'orchestr', 'dubiou', 'sophi'}\n"
     ]
    }
   ],
   "source": [
    "original_vocabulary = set(vocabulary.keys())\n",
    "new_vocabulary = set(new_vectorizer.vocabulary_.keys())\n",
    "\n",
    "print(\"Words in Original vocab but not in new one\")\n",
    "print(original_vocabulary - new_vocabulary)\n",
    "print(\"==========================================\")\n",
    "print(\"Words in New vocab but not in Old one,means our new words are :\")      \n",
    "print(new_vocabulary - original_vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6drZSBD-Sl4L"
   },
   "source": [
    "### Build a new Model \n",
    "something has changed about the underlying distribution of the words that our reviews are made up of, we need to create a new model. This way our new model will take into account whatever it is that has changed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UcxRiRRXSm9P"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_xv = new_vectorizer.transform(new_X).toarray()                              # create new vocabulary for model (for , add new data to new_x in future)\n",
    "len(new_xv[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZXFW4PrqSn7A"
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "new_val_X = pd.DataFrame(new_xv[:10000])\n",
    "new_train_X = pd.DataFrame(new_xv[10000:])\n",
    "\n",
    "new_val_y = pd.DataFrame(new_y[:10000])\n",
    "new_train_y = pd.DataFrame(new_y[10000:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To save some memory, we can delete some of our data which does not require in future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IVGcNYbkSn3v"
   },
   "outputs": [],
   "source": [
    "new_X = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7bniR6cLSnt-"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(new_xv).to_csv(os.path.join(data_dir, 'new_data.csv'), header=False, index=False)\n",
    "\n",
    "pd.concat([new_val_y, new_val_X], axis=1).to_csv(os.path.join(data_dir, 'new_validation.csv'), header=False, index=False)\n",
    "pd.concat([new_train_y, new_train_X], axis=1).to_csv(os.path.join(data_dir, 'new_train.csv'), header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NGpyM8D4ghgt"
   },
   "source": [
    "We already saved data to local dictionary, so its time to delete this from our memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r0xei2_RSnqo"
   },
   "outputs": [],
   "source": [
    "new_val_y = new_val_X = new_train_y = new_train_X = new_XV = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jThubTY9gtz0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker:'upload_data' method will be deprecated in favor of 'S3Uploader' class (https://sagemaker.readthedocs.io/en/stable/s3.html#sagemaker.s3.S3Uploader) in SageMaker Python SDK v2.\n",
      "WARNING:sagemaker:'upload_data' method will be deprecated in favor of 'S3Uploader' class (https://sagemaker.readthedocs.io/en/stable/s3.html#sagemaker.s3.S3Uploader) in SageMaker Python SDK v2.\n",
      "WARNING:sagemaker:'upload_data' method will be deprecated in favor of 'S3Uploader' class (https://sagemaker.readthedocs.io/en/stable/s3.html#sagemaker.s3.S3Uploader) in SageMaker Python SDK v2.\n"
     ]
    }
   ],
   "source": [
    "# save all those data to s3\n",
    "new_data_location = session.upload_data(os.path.join(data_dir, 'new_data.csv'), key_prefix=prefix)\n",
    "new_val_location = session.upload_data(os.path.join(data_dir, 'new_validation.csv'), key_prefix=prefix)\n",
    "new_train_location = session.upload_data(os.path.join(data_dir, 'new_train.csv'), key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RkHO3UvLg_Z6"
   },
   "source": [
    "### create new model (xgboost to train this data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QLEPbAUXgtwn"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Parameter image_name will be renamed to image_uri in SageMaker Python SDK v2.\n"
     ]
    }
   ],
   "source": [
    "new_xgb = sagemaker.estimator.Estimator(container, # The location of the container we wish to use\n",
    "                                    role,                                    # our current IAM Role\n",
    "                                    train_instance_count=1,                  # How many compute instances\n",
    "                                    train_instance_type='ml.m4.xlarge',      # What kind of compute instances\n",
    "                                    output_path='s3://{}/{}/output'.format(session.default_bucket(), prefix),\n",
    "                                    sagemaker_session=session)\n",
    "\n",
    "\n",
    "new_xgb.set_hyperparameters(max_depth=5,\n",
    "                        eta=0.2,\n",
    "                        gamma=4,\n",
    "                        min_child_weight=6,\n",
    "                        subsample=0.8,\n",
    "                        silent=0,\n",
    "                        objective='binary:logistic',\n",
    "                        early_stopping_rounds=10,\n",
    "                        num_round=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UfdxKIH9gtvS"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker:'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n",
      "WARNING:sagemaker:'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n"
     ]
    }
   ],
   "source": [
    "# give training command to sagemaker\n",
    "s3_new_input_train = sagemaker.s3_input(s3_data=new_train_location, content_type='csv')\n",
    "s3_new_input_validation = sagemaker.s3_input(s3_data=new_val_location, content_type='csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q1300BMogttQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-13 03:50:58 Starting - Starting the training job...\n",
      "2020-06-13 03:51:00 Starting - Launching requested ML instances......\n",
      "2020-06-13 03:52:08 Starting - Preparing the instances for training......\n",
      "2020-06-13 03:53:07 Downloading - Downloading input data...\n",
      "2020-06-13 03:53:43 Training - Downloading the training image.\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[34m[2020-06-13:03:54:02:INFO] Running standalone xgboost training.\u001b[0m\n",
      "\u001b[34m[2020-06-13:03:54:02:INFO] File size need to be processed in the node: 238.47mb. Available memory size in the node: 8478.1mb\u001b[0m\n",
      "\u001b[34m[2020-06-13:03:54:02:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[03:54:02] S3DistributionType set as FullyReplicated\u001b[0m\n",
      "\u001b[34m[03:54:04] 15000x5000 matrix with 75000000 entries loaded from /opt/ml/input/data/train?format=csv&label_column=0&delimiter=,\u001b[0m\n",
      "\u001b[34m[2020-06-13:03:54:04:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[03:54:04] S3DistributionType set as FullyReplicated\u001b[0m\n",
      "\u001b[34m[03:54:06] 10000x5000 matrix with 50000000 entries loaded from /opt/ml/input/data/validation?format=csv&label_column=0&delimiter=,\u001b[0m\n",
      "\u001b[34m[03:54:09] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 48 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[0]#011train-error:0.3024#011validation-error:0.3044\u001b[0m\n",
      "\u001b[34mMultiple eval metrics have been passed: 'validation-error' will be used for early stopping.\n",
      "\u001b[0m\n",
      "\u001b[34mWill train until validation-error hasn't improved in 10 rounds.\u001b[0m\n",
      "\u001b[34m[03:54:11] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 40 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[1]#011train-error:0.2994#011validation-error:0.3001\u001b[0m\n",
      "\u001b[34m[03:54:12] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 34 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[2]#011train-error:0.2874#011validation-error:0.2908\u001b[0m\n",
      "\u001b[34m[03:54:13] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 38 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[3]#011train-error:0.273467#011validation-error:0.2793\u001b[0m\n",
      "\n",
      "2020-06-13 03:54:02 Training - Training image download completed. Training in progress.\u001b[34m[03:54:15] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 52 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[4]#011train-error:0.273267#011validation-error:0.2769\u001b[0m\n",
      "\u001b[34m[03:54:16] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 34 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[5]#011train-error:0.268133#011validation-error:0.2735\u001b[0m\n",
      "\u001b[34m[03:54:17] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 36 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[6]#011train-error:0.260133#011validation-error:0.2657\u001b[0m\n",
      "\u001b[34m[03:54:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 42 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[7]#011train-error:0.2522#011validation-error:0.256\u001b[0m\n",
      "\u001b[34m[03:54:20] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 34 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[8]#011train-error:0.243267#011validation-error:0.2469\u001b[0m\n",
      "\u001b[34m[03:54:21] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[9]#011train-error:0.241#011validation-error:0.2464\u001b[0m\n",
      "\u001b[34m[03:54:22] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 32 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[10]#011train-error:0.2354#011validation-error:0.2426\u001b[0m\n",
      "\u001b[34m[03:54:24] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[11]#011train-error:0.228533#011validation-error:0.2386\u001b[0m\n",
      "\u001b[34m[03:54:25] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[12]#011train-error:0.225467#011validation-error:0.2337\u001b[0m\n",
      "\u001b[34m[03:54:26] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 32 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[13]#011train-error:0.2238#011validation-error:0.229\u001b[0m\n",
      "\u001b[34m[03:54:27] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 40 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[14]#011train-error:0.2156#011validation-error:0.2221\u001b[0m\n",
      "\u001b[34m[03:54:29] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 46 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[15]#011train-error:0.211533#011validation-error:0.2185\u001b[0m\n",
      "\u001b[34m[03:54:30] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[16]#011train-error:0.208267#011validation-error:0.2165\u001b[0m\n",
      "\u001b[34m[03:54:31] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 32 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[17]#011train-error:0.203333#011validation-error:0.2133\u001b[0m\n",
      "\u001b[34m[03:54:32] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 40 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[18]#011train-error:0.201933#011validation-error:0.2129\u001b[0m\n",
      "\u001b[34m[03:54:34] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 36 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[19]#011train-error:0.202333#011validation-error:0.2127\u001b[0m\n",
      "\u001b[34m[03:54:35] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[20]#011train-error:0.198867#011validation-error:0.209\u001b[0m\n",
      "\u001b[34m[03:54:36] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 38 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[21]#011train-error:0.1944#011validation-error:0.2076\u001b[0m\n",
      "\u001b[34m[03:54:37] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 46 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[22]#011train-error:0.190867#011validation-error:0.2047\u001b[0m\n",
      "\u001b[34m[03:54:39] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 32 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[23]#011train-error:0.189733#011validation-error:0.2038\u001b[0m\n",
      "\u001b[34m[03:54:40] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 36 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[24]#011train-error:0.1872#011validation-error:0.2024\u001b[0m\n",
      "\u001b[34m[03:54:41] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[25]#011train-error:0.186267#011validation-error:0.2007\u001b[0m\n",
      "\u001b[34m[03:54:43] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[26]#011train-error:0.1846#011validation-error:0.1982\u001b[0m\n",
      "\u001b[34m[03:54:44] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[27]#011train-error:0.182267#011validation-error:0.1977\u001b[0m\n",
      "\u001b[34m[03:54:45] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[28]#011train-error:0.1824#011validation-error:0.1974\u001b[0m\n",
      "\u001b[34m[03:54:46] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 38 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[29]#011train-error:0.177067#011validation-error:0.1959\u001b[0m\n",
      "\u001b[34m[03:54:48] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[30]#011train-error:0.1758#011validation-error:0.1947\u001b[0m\n",
      "\u001b[34m[03:54:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[31]#011train-error:0.174333#011validation-error:0.1944\u001b[0m\n",
      "\u001b[34m[03:54:50] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[32]#011train-error:0.172333#011validation-error:0.1942\u001b[0m\n",
      "\u001b[34m[03:54:51] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 32 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[33]#011train-error:0.171333#011validation-error:0.1912\u001b[0m\n",
      "\u001b[34m[03:54:53] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[34]#011train-error:0.171#011validation-error:0.1907\u001b[0m\n",
      "\u001b[34m[03:54:54] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[35]#011train-error:0.169267#011validation-error:0.1903\u001b[0m\n",
      "\u001b[34m[03:54:55] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 38 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[36]#011train-error:0.1684#011validation-error:0.1894\u001b[0m\n",
      "\u001b[34m[03:54:56] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[37]#011train-error:0.166933#011validation-error:0.1884\u001b[0m\n",
      "\u001b[34m[03:54:58] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[38]#011train-error:0.164667#011validation-error:0.1855\u001b[0m\n",
      "\u001b[34m[03:54:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[39]#011train-error:0.163467#011validation-error:0.1862\u001b[0m\n",
      "\u001b[34m[03:55:00] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[40]#011train-error:0.162867#011validation-error:0.1852\u001b[0m\n",
      "\u001b[34m[03:55:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 16 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[41]#011train-error:0.162533#011validation-error:0.1858\u001b[0m\n",
      "\u001b[34m[03:55:03] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 32 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[42]#011train-error:0.161533#011validation-error:0.1854\u001b[0m\n",
      "\u001b[34m[03:55:04] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[43]#011train-error:0.160933#011validation-error:0.1849\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[03:55:05] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 40 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[44]#011train-error:0.159133#011validation-error:0.1852\u001b[0m\n",
      "\u001b[34m[03:55:06] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[45]#011train-error:0.1578#011validation-error:0.1851\u001b[0m\n",
      "\u001b[34m[03:55:08] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[46]#011train-error:0.157333#011validation-error:0.185\u001b[0m\n",
      "\u001b[34m[03:55:09] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[47]#011train-error:0.156533#011validation-error:0.1848\u001b[0m\n",
      "\u001b[34m[03:55:10] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[48]#011train-error:0.155133#011validation-error:0.1832\u001b[0m\n",
      "\u001b[34m[03:55:11] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[49]#011train-error:0.154#011validation-error:0.1831\u001b[0m\n",
      "\u001b[34m[03:55:13] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[50]#011train-error:0.155333#011validation-error:0.1823\u001b[0m\n",
      "\u001b[34m[03:55:14] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[51]#011train-error:0.155#011validation-error:0.1822\u001b[0m\n",
      "\u001b[34m[03:55:15] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[52]#011train-error:0.1538#011validation-error:0.1815\u001b[0m\n",
      "\u001b[34m[03:55:17] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[53]#011train-error:0.153467#011validation-error:0.1817\u001b[0m\n",
      "\u001b[34m[03:55:18] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[54]#011train-error:0.152333#011validation-error:0.1812\u001b[0m\n",
      "\u001b[34m[03:55:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[55]#011train-error:0.150933#011validation-error:0.1801\u001b[0m\n",
      "\u001b[34m[03:55:20] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[56]#011train-error:0.149933#011validation-error:0.1788\u001b[0m\n",
      "\u001b[34m[03:55:22] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[57]#011train-error:0.1504#011validation-error:0.1788\u001b[0m\n",
      "\u001b[34m[03:55:23] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[58]#011train-error:0.1496#011validation-error:0.1801\u001b[0m\n",
      "\u001b[34m[03:55:24] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[59]#011train-error:0.149067#011validation-error:0.1788\u001b[0m\n",
      "\u001b[34m[03:55:25] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[60]#011train-error:0.149133#011validation-error:0.1787\u001b[0m\n",
      "\u001b[34m[03:55:27] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[61]#011train-error:0.148867#011validation-error:0.1794\u001b[0m\n",
      "\u001b[34m[03:55:28] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[62]#011train-error:0.147467#011validation-error:0.1789\u001b[0m\n",
      "\u001b[34m[03:55:29] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[63]#011train-error:0.146733#011validation-error:0.1794\u001b[0m\n",
      "\u001b[34m[03:55:30] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 34 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[64]#011train-error:0.146333#011validation-error:0.179\u001b[0m\n",
      "\u001b[34m[03:55:32] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[65]#011train-error:0.145333#011validation-error:0.1784\u001b[0m\n",
      "\u001b[34m[03:55:33] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[66]#011train-error:0.1452#011validation-error:0.1781\u001b[0m\n",
      "\u001b[34m[03:55:34] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[67]#011train-error:0.145067#011validation-error:0.1783\u001b[0m\n",
      "\u001b[34m[03:55:35] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[68]#011train-error:0.1446#011validation-error:0.1785\u001b[0m\n",
      "\u001b[34m[03:55:37] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[69]#011train-error:0.1446#011validation-error:0.179\u001b[0m\n",
      "\u001b[34m[03:55:38] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[70]#011train-error:0.144067#011validation-error:0.1785\u001b[0m\n",
      "\u001b[34m[03:55:39] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[71]#011train-error:0.143067#011validation-error:0.1776\u001b[0m\n",
      "\u001b[34m[03:55:40] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[72]#011train-error:0.142667#011validation-error:0.1775\u001b[0m\n",
      "\u001b[34m[03:55:42] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[73]#011train-error:0.141467#011validation-error:0.1762\u001b[0m\n",
      "\u001b[34m[03:55:43] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[74]#011train-error:0.1414#011validation-error:0.1764\u001b[0m\n",
      "\u001b[34m[03:55:44] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 34 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[75]#011train-error:0.139933#011validation-error:0.1763\u001b[0m\n",
      "\u001b[34m[03:55:45] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[76]#011train-error:0.139067#011validation-error:0.1768\u001b[0m\n",
      "\u001b[34m[03:55:47] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 34 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[77]#011train-error:0.138533#011validation-error:0.1764\u001b[0m\n",
      "\u001b[34m[03:55:48] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[78]#011train-error:0.137133#011validation-error:0.1761\u001b[0m\n",
      "\u001b[34m[03:55:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 38 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[79]#011train-error:0.1358#011validation-error:0.1772\u001b[0m\n",
      "\u001b[34m[03:55:51] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[80]#011train-error:0.135333#011validation-error:0.1768\u001b[0m\n",
      "\u001b[34m[03:55:52] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[81]#011train-error:0.135267#011validation-error:0.1764\u001b[0m\n",
      "\u001b[34m[03:55:53] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[82]#011train-error:0.134933#011validation-error:0.1756\u001b[0m\n",
      "\u001b[34m[03:55:54] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[83]#011train-error:0.134333#011validation-error:0.1757\u001b[0m\n",
      "\u001b[34m[03:55:56] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[84]#011train-error:0.1336#011validation-error:0.1757\u001b[0m\n",
      "\u001b[34m[03:55:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[85]#011train-error:0.1336#011validation-error:0.1758\u001b[0m\n",
      "\u001b[34m[03:55:58] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[86]#011train-error:0.133333#011validation-error:0.1762\u001b[0m\n",
      "\u001b[34m[03:55:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[87]#011train-error:0.132533#011validation-error:0.1759\u001b[0m\n",
      "\u001b[34m[03:56:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[88]#011train-error:0.1318#011validation-error:0.1752\u001b[0m\n",
      "\u001b[34m[03:56:02] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[89]#011train-error:0.131667#011validation-error:0.1745\u001b[0m\n",
      "\u001b[34m[03:56:03] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[90]#011train-error:0.130933#011validation-error:0.1748\u001b[0m\n",
      "\u001b[34m[03:56:04] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[91]#011train-error:0.1304#011validation-error:0.1748\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[03:56:06] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[92]#011train-error:0.130733#011validation-error:0.1749\u001b[0m\n",
      "\u001b[34m[03:56:07] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[93]#011train-error:0.130267#011validation-error:0.1756\u001b[0m\n",
      "\u001b[34m[03:56:08] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[94]#011train-error:0.129133#011validation-error:0.1744\u001b[0m\n",
      "\u001b[34m[03:56:09] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[95]#011train-error:0.128733#011validation-error:0.174\u001b[0m\n",
      "\u001b[34m[03:56:11] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[96]#011train-error:0.129067#011validation-error:0.1734\u001b[0m\n",
      "\u001b[34m[03:56:12] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[97]#011train-error:0.128533#011validation-error:0.1727\u001b[0m\n",
      "\u001b[34m[03:56:13] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[98]#011train-error:0.129#011validation-error:0.1735\u001b[0m\n",
      "\u001b[34m[03:56:14] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[99]#011train-error:0.129733#011validation-error:0.1741\u001b[0m\n",
      "\n",
      "2020-06-13 03:56:29 Uploading - Uploading generated training model\u001b[34m[03:56:16] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[100]#011train-error:0.128667#011validation-error:0.1747\u001b[0m\n",
      "\u001b[34m[03:56:17] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[101]#011train-error:0.128667#011validation-error:0.1749\u001b[0m\n",
      "\u001b[34m[03:56:18] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[102]#011train-error:0.128067#011validation-error:0.175\u001b[0m\n",
      "\u001b[34m[03:56:20] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[103]#011train-error:0.1274#011validation-error:0.1757\u001b[0m\n",
      "\u001b[34m[03:56:21] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 16 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[104]#011train-error:0.1262#011validation-error:0.1751\u001b[0m\n",
      "\u001b[34m[03:56:22] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[105]#011train-error:0.1258#011validation-error:0.1745\u001b[0m\n",
      "\u001b[34m[03:56:23] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[106]#011train-error:0.125467#011validation-error:0.1749\u001b[0m\n",
      "\u001b[34m[03:56:25] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[107]#011train-error:0.124733#011validation-error:0.1752\u001b[0m\n",
      "\u001b[34mStopping. Best iteration:\u001b[0m\n",
      "\u001b[34m[97]#011train-error:0.128533#011validation-error:0.1727\n",
      "\u001b[0m\n",
      "\n",
      "2020-06-13 03:56:36 Completed - Training job completed\n",
      "Training seconds: 209\n",
      "Billable seconds: 209\n"
     ]
    }
   ],
   "source": [
    "# fit our model \n",
    "new_xgb.fit({'train': s3_new_input_train, 'validation': s3_new_input_validation})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nWaaZT98htcb"
   },
   "source": [
    "### Checking of our model  (SAME PROCEDURE) - as before\n",
    "Here, I am going to use batch transform method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Kmx62I1lgtrT"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker:Parameter image will be renamed to image_uri in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".....................\u001b[34mArguments: serve\u001b[0m\n",
      "\u001b[34m[2020-06-13 04:00:35 +0000] [1] [INFO] Starting gunicorn 19.7.1\u001b[0m\n",
      "\u001b[34m[2020-06-13 04:00:35 +0000] [1] [INFO] Listening at: http://0.0.0.0:8080 (1)\u001b[0m\n",
      "\u001b[34m[2020-06-13 04:00:35 +0000] [1] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[34m[2020-06-13 04:00:35 +0000] [38] [INFO] Booting worker with pid: 38\u001b[0m\n",
      "\u001b[34m[2020-06-13 04:00:35 +0000] [39] [INFO] Booting worker with pid: 39\u001b[0m\n",
      "\u001b[34m[2020-06-13 04:00:35 +0000] [40] [INFO] Booting worker with pid: 40\u001b[0m\n",
      "\u001b[34m[2020-06-13 04:00:35 +0000] [41] [INFO] Booting worker with pid: 41\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:00:36:INFO] Model loaded successfully for worker : 39\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:00:36:INFO] Model loaded successfully for worker : 40\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:00:36:INFO] Model loaded successfully for worker : 38\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:00:36:INFO] Model loaded successfully for worker : 41\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:00:50:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:00:50:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:00:50:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:00:50:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[32m2020-06-13T04:00:48.333:[sagemaker logs]: MaxConcurrentTransforms=4, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:00:51:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:00:51:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:00:51:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:00:51:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:00:51:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:00:51:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:00:51:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:00:51:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:00:52:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:00:52:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:00:52:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:00:52:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:00:53:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:00:53:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:00:53:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:00:53:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:00:53:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:00:53:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:00:53:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:00:53:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:00:55:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:00:55:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:00:55:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:00:55:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:00:55:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:00:55:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:00:55:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:00:55:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:00:55:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:00:55:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:00:55:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:00:55:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:00:55:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:00:55:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:00:55:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:00:55:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:00:57:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:00:57:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:00:57:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:00:57:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:00:57:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:00:57:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:00:57:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:00:57:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:00:57:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:00:57:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:00:57:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:00:57:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:00:57:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:00:57:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:00:57:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:00:57:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:00:59:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:00:59:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:00:59:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:00:59:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:01:00:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:01:00:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:00:59:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:00:59:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:00:59:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:00:59:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:01:00:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:01:00:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:01:02:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:01:02:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:01:02:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:01:02:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:01:02:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:01:02:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:01:02:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:01:02:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:01:04:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:01:04:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:01:04:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:01:04:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:01:05:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:01:05:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:01:04:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:01:04:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:01:04:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:01:04:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:01:05:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:01:05:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:01:05:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:01:05:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:01:05:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:01:05:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:01:07:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:01:07:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:01:07:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:01:07:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:01:07:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:01:07:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:01:07:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:01:07:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:01:07:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:01:07:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:01:07:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:01:07:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:01:07:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:01:07:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:01:07:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:01:07:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:01:09:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:01:09:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:01:09:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:01:09:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:01:09:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:01:09:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:01:10:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:01:10:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:01:09:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:01:09:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:01:09:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:01:09:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:01:09:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:01:09:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:01:10:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:01:10:INFO] Determined delimiter of CSV input is ','\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[2020-06-13:04:01:14:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:01:14:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:01:14:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:01:14:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:01:14:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:01:14:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:01:14:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:01:14:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_xgb_transformer = new_xgb.transformer(instance_count = 1, instance_type = 'ml.m4.xlarge')\n",
    "new_xgb_transformer.transform(new_data_location, content_type='text/csv', split_type='Line')\n",
    "new_xgb_transformer.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fwYkJFm7gtpf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 256.0 KiB/366.6 KiB (3.5 MiB/s) with 1 file(s) remaining\r",
      "Completed 366.6 KiB/366.6 KiB (4.9 MiB/s) with 1 file(s) remaining\r",
      "download: s3://sagemaker-us-west-2-337299574287/xgboost-2020-06-13-03-57-12-423/new_data.csv.out to ../data/xgboost/new_data.csv.out\r\n"
     ]
    }
   ],
   "source": [
    "# save data to local instance\n",
    "!aws s3 cp --recursive $new_xgb_transformer.output_path $data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JmOrHHejgtmW"
   },
   "outputs": [],
   "source": [
    "# see the prediction result of our model\n",
    "predictions = pd.read_csv(os.path.join(data_dir, 'new_data.csv.out'), header=None)\n",
    "predictions = [round(num) for num in predictions.squeeze().values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mo71WIuTgtkK"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8538"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find accuracy score of model\n",
    "accuracy_score(new_y, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I8T211nRiV2W"
   },
   "source": [
    "Check our this accuracy with the previous one, and its a better, Now let's change our deployed model. \n",
    "\n",
    "For that, I am creating new directory as it can directly stored data from cache dataset and its new data, different form original dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gmZcgYi0gtio"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read preprocessed data from cache file: preprocessed_data.pkl\n"
     ]
    }
   ],
   "source": [
    "cache_data = None\n",
    "with open(os.path.join(cache_dir, \"preprocessed_data.pkl\"), \"rb\") as f:\n",
    "            cache_data = pickle.load(f)\n",
    "            print(\"Read preprocessed data from cache file:\", \"preprocessed_data.pkl\")\n",
    "            \n",
    "test_X = cache_data['words_test']\n",
    "test_y = cache_data['labels_test']\n",
    "\n",
    "# data already saved in variable above so better to delete from cache_data helps to free some space. \n",
    "cache_data = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rZzDNklqgtfp"
   },
   "outputs": [],
   "source": [
    "# use batch transform (by transforming test data (only reviews(x), from previously created  vectorizer object)\n",
    "test_X = new_vectorizer.transform(test_X).toarray()                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4MIoHAZegtc0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker:'upload_data' method will be deprecated in favor of 'S3Uploader' class (https://sagemaker.readthedocs.io/en/stable/s3.html#sagemaker.s3.S3Uploader) in SageMaker Python SDK v2.\n"
     ]
    }
   ],
   "source": [
    "pd.DataFrame(test_X).to_csv(os.path.join(data_dir, 'test.csv'), header=False, index=False)       # save data to directory\n",
    "\n",
    "test_location = session.upload_data(os.path.join(data_dir, 'test.csv'), key_prefix=prefix)       # specify test location.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HHdxLuMBgtZ-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "........................\u001b[34mArguments: serve\u001b[0m\n",
      "\u001b[34m[2020-06-13 04:38:15 +0000] [1] [INFO] Starting gunicorn 19.7.1\u001b[0m\n",
      "\u001b[34m[2020-06-13 04:38:15 +0000] [1] [INFO] Listening at: http://0.0.0.0:8080 (1)\u001b[0m\n",
      "\u001b[34m[2020-06-13 04:38:15 +0000] [1] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[34m[2020-06-13 04:38:15 +0000] [43] [INFO] Booting worker with pid: 43\u001b[0m\n",
      "\u001b[34m[2020-06-13 04:38:15 +0000] [44] [INFO] Booting worker with pid: 44\u001b[0m\n",
      "\u001b[34m[2020-06-13 04:38:15 +0000] [45] [INFO] Booting worker with pid: 45\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:38:16:INFO] Model loaded successfully for worker : 43\u001b[0m\n",
      "\u001b[34m[2020-06-13 04:38:16 +0000] [46] [INFO] Booting worker with pid: 46\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:38:16:INFO] Model loaded successfully for worker : 44\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:38:16:INFO] Model loaded successfully for worker : 46\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:38:16:INFO] Model loaded successfully for worker : 45\u001b[0m\n",
      "\u001b[32m2020-06-13T04:38:36.183:[sagemaker logs]: MaxConcurrentTransforms=4, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:38:39:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:38:39:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:38:39:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:38:39:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:38:39:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:38:39:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:38:39:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:38:39:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:38:39:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:38:39:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:38:39:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:38:39:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:38:39:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:38:39:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:38:39:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:38:39:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:38:41:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:38:41:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:38:41:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:38:41:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:38:41:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:38:41:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:38:41:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:38:42:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:38:42:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:38:41:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:38:41:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:38:41:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:38:41:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:38:41:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:38:42:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:38:42:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:38:44:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:38:44:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:38:46:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:38:46:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:38:46:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:38:46:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:38:46:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:38:46:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:38:47:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:38:47:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:38:46:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:38:46:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:38:46:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:38:46:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:38:46:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:38:46:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:38:47:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:38:47:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:38:49:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:38:49:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:38:49:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:38:49:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:38:49:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:38:49:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:38:49:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:38:49:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:38:49:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:38:49:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:38:49:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:38:49:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:38:49:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:38:49:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:38:49:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:38:49:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:38:51:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:38:51:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:38:51:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:38:51:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:38:51:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:38:51:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:38:52:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:38:52:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:38:51:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:38:51:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:38:51:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:38:51:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:38:51:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:38:51:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:38:52:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:38:52:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:38:53:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:38:53:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:38:54:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:38:54:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:38:54:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:38:54:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:38:53:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:38:53:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:38:54:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:38:54:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:38:54:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:38:54:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\n",
      "\u001b[34m[2020-06-13:04:38:56:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:38:56:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:38:56:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:38:56:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:38:56:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:38:56:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:38:56:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:38:56:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:38:58:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:38:58:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:38:58:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:38:58:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:38:58:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:38:58:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:38:58:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:38:58:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:38:58:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:38:58:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:38:58:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:38:58:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:38:59:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:38:59:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:38:59:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:38:59:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:39:01:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:39:01:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:39:01:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:39:01:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:39:01:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:39:01:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:39:01:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:39:01:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:39:01:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-13:04:39:01:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:39:01:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:39:01:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:39:01:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:39:01:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:39:01:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-13:04:39:01:INFO] Determined delimiter of CSV input is ','\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# fit model with new data\n",
    "new_xgb_transformer.transform(test_location, content_type='text/csv', split_type='Line')\n",
    "new_xgb_transformer.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e9mt5i2Aj5O5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 256.0 KiB/367.0 KiB (1.8 MiB/s) with 1 file(s) remaining\r",
      "Completed 367.0 KiB/367.0 KiB (2.6 MiB/s) with 1 file(s) remaining\r",
      "download: s3://sagemaker-us-west-2-337299574287/xgboost-2020-06-13-04-34-38-157/test.csv.out to ../data/xgboost/test.csv.out\r\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp --recursive $new_xgb_transformer.output_path $data_dir               # saved data to local instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g0hvLInXj5MF"
   },
   "outputs": [],
   "source": [
    "predictions = pd.read_csv(os.path.join(data_dir, 'test.csv.out'), header=None) # see predictions s\n",
    "predictions = [round(num) for num in predictions.squeeze().values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GrKHeotzj5Kj"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83756"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(test_y, predictions)                                             # find new accuracy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t1EEU_uhkrFH"
   },
   "source": [
    "Now, our accuracy is 83.75  %\n",
    "\n",
    "---\n",
    "\n",
    "### Updating Model\n",
    "\n",
    "Now,  we have a new model that we'd like to use instead of one that is already deployed. Furthermore, we are assuming that the model that is already deployed is being used in some sort of application. As a result, what we want to do is update the existing endpoint so that it uses our new model. \n",
    "> So, let's generate endpoint first.\n",
    "\n",
    "Then, we can access the model outside of this notebook from endpoint. As we generated our last all model from 'model_name' and then the generated time, we should first create object model for endpoint inside sagemaker. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QC4ZnOUTj5HR"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'xgboost-2020-06-13-03-50-58-517'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_xgb_transformer.model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7dZRfAvxk9Hx"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8IiO9-gFj5D-"
   },
   "outputs": [],
   "source": [
    "from time import gmtime, strftime\n",
    "new_xgb_endpoint_config_name = \"sentiment-update-xgboost-endpoint-config-\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())     # for giving unique name \n",
    "new_xgb_endpoint_config_info = session.sagemaker_client.create_endpoint_config(                                          # please visit previous section's declaration.\n",
    "                            EndpointConfigName = new_xgb_endpoint_config_name,\n",
    "                            ProductionVariants = [{\n",
    "                                \"InstanceType\": \"ml.m4.xlarge\",\n",
    "                                \"InitialVariantWeight\": 1,\n",
    "                                \"InitialInstanceCount\": 1,\n",
    "                                \"ModelName\": new_xgb_transformer.model_name,\n",
    "                                \"VariantName\": \"XGB-Model\"\n",
    "                            }])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EgiPwlEbj5Bl"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'EndpointArn': 'arn:aws:sagemaker:us-west-2:337299574287:endpoint/xgboost-2020-06-13-01-31-19-328',\n",
       " 'ResponseMetadata': {'RequestId': '74b618d7-b117-478b-b9ca-16af21698c28',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '74b618d7-b117-478b-b9ca-16af21698c28',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '99',\n",
       "   'date': 'Sat, 13 Jun 2020 04:27:44 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# update the endpoint... \n",
    "session.sagemaker_client.update_endpoint(EndpointName=xgb_predictor.endpoint, EndpointConfigName=new_xgb_endpoint_config_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rJpcl1T7j4-V"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------!"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'EndpointName': 'xgboost-2020-06-13-01-31-19-328',\n",
       " 'EndpointArn': 'arn:aws:sagemaker:us-west-2:337299574287:endpoint/xgboost-2020-06-13-01-31-19-328',\n",
       " 'EndpointConfigName': 'sentiment-update-xgboost-endpoint-config-2020-06-13-04-27-42',\n",
       " 'ProductionVariants': [{'VariantName': 'XGB-Model',\n",
       "   'DeployedImages': [{'SpecifiedImage': '433757028032.dkr.ecr.us-west-2.amazonaws.com/xgboost:1',\n",
       "     'ResolvedImage': '433757028032.dkr.ecr.us-west-2.amazonaws.com/xgboost@sha256:513e8442b35b9ecc9d326f85659f8e30b10e2cec096b863f04a9738baa9ebb57',\n",
       "     'ResolutionTime': datetime.datetime(2020, 6, 13, 4, 27, 46, 931000, tzinfo=tzlocal())}],\n",
       "   'CurrentWeight': 1.0,\n",
       "   'DesiredWeight': 1.0,\n",
       "   'CurrentInstanceCount': 1,\n",
       "   'DesiredInstanceCount': 1}],\n",
       " 'EndpointStatus': 'InService',\n",
       " 'CreationTime': datetime.datetime(2020, 6, 13, 3, 27, 2, 267000, tzinfo=tzlocal()),\n",
       " 'LastModifiedTime': datetime.datetime(2020, 6, 13, 4, 33, 56, 524000, tzinfo=tzlocal()),\n",
       " 'ResponseMetadata': {'RequestId': 'bd6855c3-7768-472c-b4e9-4247190ffcfc',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'bd6855c3-7768-472c-b4e9-4247190ffcfc',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '742',\n",
       "   'date': 'Sat, 13 Jun 2020 04:34:16 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.wait_for_endpoint(xgb_predictor.endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZnczS3dblIF2"
   },
   "source": [
    "### Delete Endpoint.\n",
    "We are done with the deployed endpoint we need to make sure to shut it down, otherwise we will continue to be charged for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ODY6B6T2j485"
   },
   "outputs": [],
   "source": [
    "xgb_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k28nwMw96MeJ"
   },
   "source": [
    "### Clean up disk and dir (free memory for next prediction)\n",
    "\n",
    "The default notebook instance on SageMaker doesn't have a lot of excess disk space available. As you continue to complete and execute other notebooks you will eventually fill up this disk space, leading to errors which can be difficult to diagnose. \n",
    "\n",
    "Once you are completely finished using a notebook it is a good idea to remove the files that you created along the way. Of course, you can do this from the terminal or from the notebook hub if you would like. The cell below contains some commands to clean up the created files from within the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PQXA_m0mx2Jk"
   },
   "outputs": [],
   "source": [
    "# first delete the files from directory\n",
    "!rm $data_dir/*\n",
    "\n",
    "# delete directory itself\n",
    "!rmdir $data_dir\n",
    "\n",
    "# remove all the files in the cache_dir\n",
    "!rm $cache_dir/*\n",
    "\n",
    "# remove cache_directory itself\n",
    "!rmdir $cache_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j-ye14Eix2GK"
   },
   "outputs": [],
   "source": [
    "# Keep Learning,Enjoy Empowering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Sentimental_Analytics(AWS_SageMaker_Updating_Model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
