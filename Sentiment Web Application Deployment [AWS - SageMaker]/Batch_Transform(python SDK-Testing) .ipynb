{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dQB6Vx2g1kjn"
   },
   "source": [
    "# Sentimental_Analysis using AWS sagemaker - XGBoost algorithm\n",
    "[Vedant Dave](https://vedantdave77.github.io/) | Vedantdave77@gmail.com | [LinkedIn](https://www.linkedin.com/in/vedant-dave117/)\n",
    "\n",
    "Hello, I am Vedant Dave, a machine learning practitioner data enthusiast professional. -@dave117\n",
    "\n",
    "## Intro:\n",
    "In this notebook, I am going to analyze IMDB dataset. Its one of the best dagtaset of NLP research. You can search about IMDB on IMDB.com to get an idea about the company portfolio and their workprofile. Well, my main purpose is to use AWS-Sagemaker's python SDK - xgboost module for Sentiment-Anlysis.\n",
    "\n",
    "Why?\n",
    "\n",
    "- For most of online websites and ecommerce/ digital communication companies, sentimental analysis is one of the major field to improve customer satisfaction, which leads to business growth. \n",
    "\n",
    "- My Major goal is to analyze (preparation of text data and implement a AWS model with sagemaker (batch-transform method). I am also going to make deployment using lambda function (with another notebook). The data storage will be S3 data storage. \n",
    "\n",
    "- The credit for this notebook goes to Udacity, from which I took an intuition, but the code modification, improvement and procedure explaination done by me. So, for any specific issue with notebook you can connect with me on above contact ID, and I request you to use right side of google tab for searching about more explaination. Thank you. \n",
    "\n",
    "---\n",
    "\n",
    "Project ML Flow: **Standford Data API -- S3 -- SageMaker -- Lambda -- WebApp(html file)** \n",
    "\n",
    "---\n",
    "\n",
    "So, let's start...\n",
    "\n",
    "### Download data from [IMDB dataset](http://ai.stanford.edu/~amaas/data/sentiment/)\n",
    "and save it to local directory first. Ihere, local directory always refer to our AWS notebook instance). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 212
    },
    "colab_type": "code",
    "id": "ppskPPw6UlL0",
    "outputId": "94db716d-fec3-4320-c843-636fa4f478c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-06-07 18:09:29--  http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
      "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
      "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 84125825 (80M) [application/x-gzip]\n",
      "Saving to: ‘../data/aclImdb_v1.tar.gz’\n",
      "\n",
      "../data/aclImdb_v1. 100%[===================>]  80.23M  48.0MB/s    in 1.7s    \n",
      "\n",
      "2020-06-07 18:09:31 (48.0 MB/s) - ‘../data/aclImdb_v1.tar.gz’ saved [84125825/84125825]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%mkdir ../data\n",
    "!wget -O ../data/aclImdb_v1.tar.gz http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz # download data from Stanford API\n",
    "!tar -zxf ../data/aclImdb_v1.tar.gz -C ../data                                                    # extract .tar file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B9APSOWD5fXU"
   },
   "source": [
    "### Data Preparation\n",
    "\n",
    "Current format of data is One file, for project we need to seperate them in train, validation and test datasets. The labels are also in pos/ neg form so, for project, its better to covert them in 0 and 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wcQ9psFh5d_i"
   },
   "outputs": [],
   "source": [
    "import os                                                                       # provide operating system accordingly ...\n",
    "import glob                                                                     # glob is path name matcher, start each file with .*\n",
    "\n",
    "def read_data(data_dir='../data/aclImdb'):\n",
    "    data = {}\n",
    "    labels = {}\n",
    "    \n",
    "    for data_type in ['train', 'test']:\n",
    "        data[data_type] = {}\n",
    "        labels[data_type] = {}\n",
    "        \n",
    "        for sentiment in ['pos', 'neg']:\n",
    "            data[data_type][sentiment] = []\n",
    "            labels[data_type][sentiment] = []\n",
    "            \n",
    "            path = os.path.join(data_dir, data_type, sentiment, '*.txt')\n",
    "            files = glob.glob(path)\n",
    "            \n",
    "            for f in files:\n",
    "                with open(f) as review:\n",
    "                    data[data_type][sentiment].append(review.read())\n",
    "                    # Here we represent a positive review by '1' and a negative review by '0'\n",
    "                    labels[data_type][sentiment].append(1 if sentiment == 'pos' else 0)\n",
    "                    \n",
    "            assert len(data[data_type][sentiment]) == len(labels[data_type][sentiment]), \\\n",
    "                    \"{}/{} data size does not match labels size\".format(data_type, sentiment)\n",
    "                \n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n8T3xzaL5d8O"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total IMDB reviews : train = 12500 pos/ 12500 neg, test = 12500 pos / 12500 neg\n"
     ]
    }
   ],
   "source": [
    "data,labels = read_data()\n",
    "print(\"Total IMDB reviews : train = {} pos/ {} neg, test = {} pos / {} neg\".\n",
    "      format(len(data['train']['pos']),len(data['train']['neg']),\n",
    "             len(data['test']['pos']),len(data['test']['neg'])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8tF6poUq5d6d"
   },
   "outputs": [],
   "source": [
    "# Now, lets conmbine pos and neg dataset and shuffle them for making training and testing dataset.\n",
    "# WHY?  --> because, form above function we get four sets separated by pos, neg in train and test set... (look and understand)\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "def prepare_imdb_data(data,lables):\n",
    "    data_train = data['train']['pos'] + data['train']['neg']\n",
    "    data_test = data['test']['pos'] + data['test']['neg']\n",
    "    labels_train = data['train']['pos'] + data['train']['neg']\n",
    "    labels_test = data['test']['pos'] + data['test']['neg']\n",
    "\n",
    "    # shuffle reviews and correspoing labels within training and test dataset\n",
    "    data_train, labels_train = shuffle(data_train,labels_train)                 # this helps us to shuffle through whole training ...\n",
    "    data_test, labels_test = shuffle(data_test,labels_test)\n",
    "\n",
    "    # return a datasets for future processes.\n",
    "    return data_train, data_test, labels_train, labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "72F89VYF5d3-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDB total reviews (full dataset) :train = 25000, test - 25000\n"
     ]
    }
   ],
   "source": [
    "train_X,test_X,train_y,test_y = prepare_imdb_data(data,labels)\n",
    "print('IMDB total reviews (full dataset) :train = {}, test - {}'.format(len(train_X),len(test_X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hz3K6bEW5d1f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'OK, so I just saw the movie, although it appeared last year... I thought that it was generally a decent movie, except for the storyline, which was stupid and horrible... First of all, we never get to know anything about the creatures, why they appeared, wtf are they doing in our world, and really, have they been on Earth before we were or did they just come from space? Secondly, the role of the butcher to maintain order is just so obviously created... Really, how large could the underground for a sub station could have been? There were only so many of those creatures, so I think instead of killing innocent people in vain, they could have just planted some tactical bombs, or maybe clear the are and a Nuke would have done the job. I know it sounds funny and it is, but I do not see the killing of people as being NECESSARY... Thirdly, Leon acts like Superman jumping on the train and fighting Vinnie Jones, who was way taller and bigger in stature. Then again, when he faces the conductor he does nothing and acts as a wimp, watching all the abominations. I mean OK, the conductor had creepy help(lol), but if Leon was so brave he would have gone all the way... I mean he risks his life first, then does nothing exactly when he should have. He could have died as a hero but lives as a coward... this might be the case, but not after showing so much braveness earlier on... Then, the cop thing... come on! This was a city having a subway, I bet there must have been other cops except that lady, other police stations,this was really kind of silly... All in all, great acting by Vinnie Jones, interesting idea up to the reason behind it which is not really built at all... By the way, what did the signs on the chest mean? Vinnie Jones cannot make up for the rest...'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X[100]                                                                    #  first 100 reviews (.txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qTH0SAFrARcS"
   },
   "source": [
    "### Data Preprocessing\n",
    "The complex problem in ML is to clean data, and make them ready for analysis. Here, please observe above review. We downloaded from web in html form. That's why you can see html format <br> ... </br> there. So, first we need to remove them. More over, some words are repetative, meaning less and with similar meaning. So, first we will remove all these obsecles. The step is called data preprocessing, also know as data cleaning, dfata wrangling, data manipulation. So, I am going to use NLTK library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "whAo52i55dzJ",
    "outputId": "7f6544a3-24e1-4505-9908-5db0b5f6972d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"stopwords\")                                # does not work\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *                            # does not work\n",
    "\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8fkaxgV35dwf"
   },
   "outputs": [],
   "source": [
    "import re                                                     # import request.\n",
    "from bs4 import BeautifulSoup                                 # python library for html and css parsing(remove)\n",
    "\n",
    "def review_to_words(review):\n",
    "    text = BeautifulSoup(review, \"html.parser\").get_text()                      # remove html tags\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\",\" \",text.lower())                             # conver to lowercase (all a to z, A to Z, 0 to 9)\n",
    "    words = text.split()                                                        # split string into words\n",
    "    words = [w for w in words if  w not in stopwords.words(\"english\")]          # remove stopwords\n",
    "    words = [PorterStemmer().stem(w) for w in words]                           # stem --> nlp library for stemmers (prular words, languages, similarity etc...)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D8OSjE7C5dtR"
   },
   "outputs": [],
   "source": [
    "import pickle                                                                   # for serializing/deserializing python input (here,pickle--> converts datastructure to byte stram)\n",
    "cache_dir = os.path.join(\"../cache\", \"sentiment_analysis\")                      # define storage path\n",
    "os.makedirs(cache_dir, exist_ok=True)                                           # ensure about directory\n",
    "\n",
    "def preprocess_data(data_train, data_test, labels_train, labels_test,\n",
    "                    cache_dir = cache_dir, cache_file =\"preprocessed_data.pkl\"):\n",
    "  \n",
    "    cache_data = None                                          # initialize cach data\n",
    "    if cache_file is not None:                                 # comp saved cache data for future purpose so, the operation will be faster \n",
    "        try: \n",
    "            with open(os.path.join(cache_dir, cache_file), \"rb\") as f:           # read bite form pickle file\n",
    "                cache_data = pickle.load(f)\n",
    "            print(\"Read preprocessed data from cache file :\" , cache_file)\n",
    "        except:\n",
    "                  pass                       \n",
    "\n",
    "    if cache_data is None:\n",
    "        words_train = [review_to_words(review) for review in data_train]    # generate list [] from available dict. \"data_train\"\n",
    "        words_test = [review_to_words(review) for review in data_test]     # ... same \n",
    "\n",
    "        if cache_file is not None:\n",
    "            cache_data = dict(words_train = words_train,words_test = words_test, labels_train = labels_train,labels_test=labels_test)\n",
    "        with open(os.path.join(cache_dir, cache_file),\"wb\") as f:\n",
    "            pickle.dump(cache_data,f)\n",
    "            print(\"Wrote preprocessed data to cache file: \", cache_file)\n",
    "    else: \n",
    "        words_train,words_test,labels_train,labels_test = (cache_data['words_train'],cache_data['words_test'],cache_data['labels_train'],cache_data['labels_test'])\n",
    "      \n",
    "    return words_train,words_test,labels_train,labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q6m_r9Ubz-BU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote preprocessed data to cache file:  preprocessed_data.pkl\n"
     ]
    }
   ],
   "source": [
    "# get preprocessed data (wait .................................... ! :)\n",
    "train_X,test_X,train_y,test_y = preprocess_data(train_X,test_X,train_y,test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **NOTE :** **Preprocess may take longer time, so until that enjoy this [video](https://www.youtube.com/watch?v=8Mlc4-3tgzc)** , will help in near future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fix8k_YI0h4_"
   },
   "source": [
    "### Extract Bag of words features \n",
    "\n",
    "The bag-of-words model is a simplifying representation used in natural language processing and information retrieval (IR). In this model, a text (such as a sentence or a document) is represented as the bag (multiset) of its words, disregarding grammar and even word order but keeping multiplicity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sIygdc2fz9-C"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.externals import joblib                                            # joblib is advanced pickle version used for storing numpy arrays (bite-pyhton_moduel-bite\n",
    "\n",
    "def extract_BoW_features(words_train,words_test,vocabulary_size = 5000,\n",
    "                         cache_dir = cache_dir, cache_file = 'bow_features.pkl'):\n",
    "    cache_data = None\n",
    "    if cache_file is not None:\n",
    "        try:\n",
    "            with open(os.path.join(cache_dir, cache_file), 'rb') as f:\n",
    "                cache_data = joblib.load(f)\n",
    "            print(\"Read features from cache file: \", cache_file)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    if cache_data is None:\n",
    "        vectorizer = CountVectorizer(max_features = vocabulary_size ,\n",
    "                                     preprocessor = lambda x: x,tokenizer = lambda x:x)\n",
    "        features_train = vectorizer.fit_transform(words_train).toarray()\n",
    "        features_test = vectorizer.transform(words_test).toarray()\n",
    "\n",
    "        if cache_file is not None:\n",
    "            vocabulary = vectorizer.vocabulary_\n",
    "            cache_data = dict(features_train = features_train,features_test=features_test,\n",
    "                            vocabulary = vocabulary)\n",
    "            with open(os.path.join(cache_dir, cache_file),'wb') as f:\n",
    "                joblib.dump(cache_data,f)\n",
    "            print(\"wrote features to cache file:\",cache_file)\n",
    "    else:\n",
    "        features_train, features_test,vocabulary = (cache_data['features_train'],cache_data['features_test'],\n",
    "                                                    cache_data['vocabulary'])\n",
    "\n",
    "    return features_train, features_test, vocabulary\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gMP5eYvRz98O"
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 954. MiB for an array with shape (25000, 5000) and data type int64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-eed00574f22d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_X\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvocabulary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_BoW_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-319a7aaaeb94>\u001b[0m in \u001b[0;36mextract_BoW_features\u001b[0;34m(words_train, words_test, vocabulary_size, cache_dir, cache_file)\u001b[0m\n\u001b[1;32m     17\u001b[0m         vectorizer = CountVectorizer(max_features = vocabulary_size ,\n\u001b[1;32m     18\u001b[0m                                      preprocessor = lambda x: x,tokenizer = lambda x:x)\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mfeatures_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mfeatures_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/scipy/sparse/compressed.py\u001b[0m in \u001b[0;36mtoarray\u001b[0;34m(self, order, out)\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0morder\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m             \u001b[0morder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_swap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1025\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_toarray_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1026\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_contiguous\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_contiguous\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Output array must be C or F contiguous'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m_process_toarray_args\u001b[0;34m(self, order, out)\u001b[0m\n\u001b[1;32m   1187\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1189\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate 954. MiB for an array with shape (25000, 5000) and data type int64"
     ]
    }
   ],
   "source": [
    "train_X,test_X,vocabulary = extract_BoW_features(train_X,test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W1vzcPtmJ8d1"
   },
   "source": [
    "### Classification using  XGBoost Algorithm\n",
    "SageMaker has predefined XGBoost Algirthm for classificatio task. But for better accuracy and avoid overfitting I want to use validation dataset. For that, first we will give first 10000 review to validation and then give data to XGBoost in panda dataframe format. The data is stored in S3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y0eP9fpBz94U"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "val_X = pd.DataFrame(train_X[:10000])\n",
    "train_X = pd.DataFrame(train_X[10000:])\n",
    "\n",
    "val_y = pd.DataFrame(train_y[:10000])\n",
    "train_y = pd.DataFrame(train_y[10000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7_PmpEvrJFJV"
   },
   "outputs": [],
   "source": [
    "# generate local dictionary where our data is stored for use.\n",
    "data_dir = '../data/xgboost'\n",
    "if not os.path.exists(data_dir):                                                # ensure about dir. (resolve bug)\n",
    "    os.makedirs(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NdWcpT9MJGE9"
   },
   "outputs": [],
   "source": [
    "# save data to dictionary \n",
    "pd.DataFrame(test_S).to_csv(os.path.join(data_dir,'test.csv'),header=False,index=False)                         # test.csv \n",
    "\n",
    "pd.DataFrame([val_y,val_X],axis=1).to_csv(os.path.join(data_dir,'validation.csv'),header= False, index= False)   # validation.csv\n",
    "\n",
    "pd.DataFrame([train_y,train_X],axis=1).to_csv(os.path.join(data_dir,'train.csv'),header= False, index = False)   # train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sUyJKTpSJGBk"
   },
   "outputs": [],
   "source": [
    "# initialize memory storage (so, set a bit of memory to None)\n",
    "train_X =- val_X = train_y = val_y = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JTBGZKQwjk9_"
   },
   "source": [
    "### Uploading Training/validation to S3 \n",
    "Flow --> Local_dir --> S3 --> SageMaker --> S3(result) --> Local_dir(result)\n",
    "\n",
    "Here, I am going to use sagemaker's high level features so, all the background work will be done by sagemaker ownself, and I just need to provide resources, commands and requirements to sagemaker. \n",
    "\n",
    "There is posibility of Low level fetaures, which give us chance to provide flexiblility to model, but when you need to do some research around your result. Well, here in future I will use auto Hyper parameter tuning, to get best answer (with high accuracy) for our dataset problem. So, its nice to use highlevel features.\n",
    "\n",
    "Let's start real work with SAGEMAKER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bAgduC9cJF_r"
   },
   "outputs": [],
   "source": [
    "import sagemaker                                                                # call sagemaker\n",
    "session = sagemaker.Session()                                                   # create  session for sagemaker \n",
    "prefix = 'sentiment-xgboost'                                                    # prefix will be used for unique name identification (in near future)\n",
    "\n",
    "# set specific location on S3 for easy access \n",
    "test_location = session.upload_data(os.path.join(data_dir,'test.csv'),key_prefix= prefix)           # upload test data \n",
    "val_location = session.upload_data(os.path.join(data_dir,'validataion.csv'),key_prefix = prefix)    # upload validation data\n",
    "train_location = session.upload_data(os.path.csv(data_dir, 'train.csv'),key_prefix = prefix)        # upload train data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CiKQEHwDmq8-"
   },
   "source": [
    "### Create XGBoost model tuning requirement\n",
    "\n",
    "Will create specific requirement for sagemaker to understand what to do, where to access and How to use model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ALl4hkN0mqmc"
   },
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role                                          \n",
    "role = get_execution_role()                                                     # create model execution role = IAM role, for giving permission to specific person or user group (to control unauthorize access)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vb1NhGwQJF8b"
   },
   "outputs": [],
   "source": [
    "from sagemaker.amazon.amazon_estimater import get_image_uri\n",
    "container = get_image_uri(session.boto_region_name,'xgboost')                   # set container for giving private space to model (when you have more than one deploying model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dE_CFWjLJF6W"
   },
   "outputs": [],
   "source": [
    "# specify model with requried parameters \n",
    "xgb = None                                                                      # create model\n",
    "xgb = sagemaker.estimator.Estimator(container,                                  # define container (where to take data)\n",
    "                                    role,                                       # define role (who give permission for this)\n",
    "                                    train_instance_count = 1,                   # instance will used for task (more instance, more power, more expense)\n",
    "                                    train_instance_type = 'ml.m4.xlarge',       # power of isntance (more power, more expense, less execution time)\n",
    "                                    output_path = 's3://{}/{}/output'.format(session.default_bucket(),prefix),   # where to save\n",
    "                                    sagemaker_session= session)                 # define session (the current one)\n",
    "\n",
    "\n",
    "\n",
    "xgb.set_hyperparamaetrs(max_depth - 5,                                          # understand xgboost documents first \n",
    "                        eta =0.2,\n",
    "                        gamma=4,\n",
    "                        min_child_weight=6,\n",
    "                        subsample = 0.8,\n",
    "                        silent= 0,\n",
    "                        objective = 'binary:logistic',\n",
    "                        early_stopping_rounds= 10,\n",
    "                        num_round = 500)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PBTjaWXV-vKr"
   },
   "source": [
    "### Fit the created model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lhADG_gz-vGL"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "udjcJQxvJF33"
   },
   "outputs": [],
   "source": [
    "s3_input_train = sagemaker.s3_input(s3_train = train_location,content_type='csv')\n",
    "s3_input_validation = sagemaker.s3_input(s3_validation = validation_location, content_type= 'csv')\n",
    "\n",
    "xgb.fit({'train': s3_input_train,'validation': s3_input_validation})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QbeDszgHth8g"
   },
   "source": [
    "### Testing Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bBq5rqm2JF1X"
   },
   "outputs": [],
   "source": [
    "xgb_transformer = xgb.transformer(instance_count =1, instance_type = 'm1.m4.xlarge')       # used Batch_transform method from sagemaker\n",
    "\n",
    "xgb_transformer.transfrorm(test_location,content_type = 'text/csv',split_type= 'Line')     # read data from test location for predictinog result.\n",
    "\n",
    "xgb_transformer.wait()                                                                     # wait for response "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2JXzONtjJFy8"
   },
   "outputs": [],
   "source": [
    "!aws s3 cp --recursive #xgb_transformer.output_path $data_dir                               # save test result to s3 (for local use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3UxHTy9Ox2Ry"
   },
   "outputs": [],
   "source": [
    "predictions = pd.read_csv(os.path.join(data_dir,'test.csv.out'),header=None)\n",
    "predictions = [round(num) for num in predictions.squeeze().values]              # create list of prediction values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9Y_hgNIfx2OR"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(test_y,predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k28nwMw96MeJ"
   },
   "source": [
    "### Clean up disk and dir (free memory for next prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PQXA_m0mx2Jk"
   },
   "outputs": [],
   "source": [
    "# first delete the files from directory\n",
    "!rm $data_dir/*\n",
    "\n",
    "# delete directory itself\n",
    "!rmdir $data_dir\n",
    "\n",
    "# remove all the files in the cache_dir\n",
    "!rm $cache_dir/*\n",
    "\n",
    "# remove cache_directory itself\n",
    "!rmdir $cache_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j-ye14Eix2GK"
   },
   "outputs": [],
   "source": [
    "# Keep Learning,Enjoy Empowering"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Sentimental_Analytics(AWS_SageMaker_BT.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
