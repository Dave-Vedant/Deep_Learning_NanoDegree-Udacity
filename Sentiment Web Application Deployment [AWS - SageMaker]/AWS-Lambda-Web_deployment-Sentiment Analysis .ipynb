{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis Web App\n",
    "\n",
    "[Vedant Dave](https://vedantdave77.github.io/) | Vedantdave77@gmail.com | [LinkedIn](https://www.linkedin.com/in/vedant-dave117/)\n",
    "\n",
    "Hello, I am Vedant Dave, a machine learning practitioner data enthusiast professional. -@dave117\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, I am going to analyze IMDB dataset. Its one of the best dagtaset of NLP research. You can search about IMDB on IMDB.com to get an idea about the company portfolio and their workprofile. Well, my main purpose is to deploy web app using lambda function.\n",
    "\n",
    "Before this I already worked on Batch_transform job using python SDK, sagemaker hyperparameter tuning and model updation using new generated data. \n",
    "\n",
    "In this notebook I will use Amazon's SageMaker service to construct a random tree model to predict the sentiment of a movie review. In addition, we will deploy this model to an endpoint and construct a very simple web app which will interact with our model's deployed endpoint.\n",
    "\n",
    "## General Outline\n",
    "\n",
    "The flow of process is as follow:\n",
    "\n",
    "1. Download or otherwise retrieve the data. \n",
    "2. Process / Prepare the data.................................[Data Preprocessing]\n",
    "3. Upload the processed data to S3..........................[DataBase creation] ---------- AWS-S3\n",
    "4. Train a chosen model.............................................[Development-Training Phase] ----------- AWS-SageMaker\n",
    "5. Test the trained model (a batch transform job)........[Testing Phase]\n",
    "6. Deploy the trained model...........................................[Model Release] ------------ AWS-Lambda\n",
    "7. Use the deployed model..............................................[Model Running Test]------------WEB_API\n",
    "\n",
    "In this notebook we will progress through each of the steps above. We will also see that the final step, using the deployed model, can be quite challenging.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data from [IMDB dataset](http://ai.stanford.edu/~amaas/data/sentiment/)\n",
    "Current format of data is One file, for project we need to seperate them in train, validation and test datasets. The labels are also in pos/ neg form so, for project, its better to covert them in 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-06-14 18:51:43--  http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
      "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
      "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 84125825 (80M) [application/x-gzip]\n",
      "Saving to: â€˜../data/aclImdb_v1.tar.gzâ€™\n",
      "\n",
      "../data/aclImdb_v1. 100%[===================>]  80.23M  46.3MB/s    in 1.7s    \n",
      "\n",
      "2020-06-14 18:51:45 (46.3 MB/s) - â€˜../data/aclImdb_v1.tar.gzâ€™ saved [84125825/84125825]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%mkdir ../data\n",
    "!wget -O ../data/aclImdb_v1.tar.gz http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "!tar -zxf ../data/aclImdb_v1.tar.gz -C ../data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "The complex problem in ML is to clean data, and make them ready for analysis. Here, please observe above review. We downloaded from web in html form. That's why you can see html format <br> ... </br> there. So, first we need to remove them. More over, some words are repetative, meaning less and with similar meaning. So, first we will remove all these obsecles. The step is called data preprocessing, also know as data cleaning, dfata wrangling, data manipulation. So, I am going to use NLTK library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os                                                                       # provide operating system accordingly ...\n",
    "import glob                                                                     # glob is path name matcher, start each file with .*\n",
    "\n",
    "def read_data(data_dir='../data/aclImdb'):\n",
    "    data = {}\n",
    "    labels = {}\n",
    "    \n",
    "    for data_type in ['train', 'test']:\n",
    "        data[data_type] = {}\n",
    "        labels[data_type] = {}\n",
    "        \n",
    "        for sentiment in ['pos', 'neg']:\n",
    "            data[data_type][sentiment] = []\n",
    "            labels[data_type][sentiment] = []\n",
    "            \n",
    "            path = os.path.join(data_dir, data_type, sentiment, '*.txt')\n",
    "            files = glob.glob(path)\n",
    "            \n",
    "            for f in files:\n",
    "                with open(f) as review:\n",
    "                    data[data_type][sentiment].append(review.read())\n",
    "                    # Here we represent a positive review by '1' and a negative review by '0'\n",
    "                    labels[data_type][sentiment].append(1 if sentiment == 'pos' else 0)\n",
    "                    \n",
    "            assert len(data[data_type][sentiment]) == len(labels[data_type][sentiment]), \\\n",
    "                    \"{}/{} data size does not match labels size\".format(data_type, sentiment)\n",
    "                \n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDB reviews: train = 12500 pos / 12500 neg, test = 12500 pos / 12500 neg\n"
     ]
    }
   ],
   "source": [
    "data, labels = read_data()\n",
    "print(\"IMDB reviews: train = {} pos / {} neg, test = {} pos / {} neg\".format(\n",
    "            len(data['train']['pos']), len(data['train']['neg']),\n",
    "            len(labels['test']['pos']), len(labels['test']['neg'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, lets conmbine pos and neg dataset and shuffle them for making training and testing dataset.\n",
    "# WHY?  --> because, form above function we get four sets separated by pos, neg in train and test set... (look and understand)\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "def prepare_imdb_data(data,lables):\n",
    "    data_train = data['train']['pos'] + data['train']['neg']\n",
    "    data_test = data['test']['pos'] + data['test']['neg']\n",
    "    labels_train = labels['train']['pos'] + labels['train']['neg']              # Awesome mistake +++ cost me 8 days (and 50+ hr sagemaker cost)\n",
    "    labels_test = labels['test']['pos'] + labels['test']['neg']\n",
    "\n",
    "    # shuffle reviews and correspoing labels within training and test dataset\n",
    "    data_train, labels_train = shuffle(data_train,labels_train)                 # this helps us to shuffle through whole training ...\n",
    "    data_test, labels_test = shuffle(data_test,labels_test)\n",
    "\n",
    "    # return a datasets for future processes.\n",
    "    return data_train, data_test, labels_train, labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDb reviews (combined): train = 25000, test = 25000\n"
     ]
    }
   ],
   "source": [
    "train_X, test_X, train_y, test_y = prepare_imdb_data(data, labels)\n",
    "print(\"IMDb reviews (combined): train = {}, test = {}\".format(len(train_X), len(test_X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this was a very good movie i wished i could find it in vhs to buy,i really enjoyed this movie i would definaetly recommend this movie to watch i would like to see it again but can never find it in tv, it would be well worth the time to watch it again'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X[100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "The complex problem in ML is to clean data, and make them ready for analysis. Here, please observe above review. We downloaded from web in html form. That's why you can see html format **(<!br>  \\</!br>)** there. So, first we need to remove them. More over, some words are repetative, meaning less and with similar meaning. So, first we will remove all these obsecles. The step is called data preprocessing, also know as data cleaning, dfata wrangling, data manipulation. So, I am going to use NLTK library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "REPLACE_NO_SPACE = re.compile(\"(\\.)|(\\;)|(\\:)|(\\!)|(\\')|(\\?)|(\\,)|(\\\")|(\\()|(\\))|(\\[)|(\\])\")\n",
    "REPLACE_WITH_SPACE = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
    "\n",
    "def review_to_words(review):\n",
    "    words = REPLACE_NO_SPACE.sub(\"\", review.lower())\n",
    "    words = REPLACE_WITH_SPACE.sub(\" \", words)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this was a very good movie i wished i could find it in vhs to buyi really enjoyed this movie i would definaetly recommend this movie to watch i would like to see it again but can never find it in tv it would be well worth the time to watch it again'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_to_words(train_X[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle                                                                   # for serializing/deserializing python input (here,pickle--> converts datastructure to byte stram)\n",
    "cache_dir = os.path.join(\"../cache\", \"sentiment_analysis\")                      # define storage path\n",
    "os.makedirs(cache_dir, exist_ok=True)                                           # ensure about directory\n",
    "\n",
    "def preprocess_data(data_train, data_test, labels_train, labels_test,\n",
    "                    cache_dir = cache_dir, cache_file =\"preprocessed_data.pkl\"):\n",
    "  \n",
    "    cache_data = None                                          # initialize cach data\n",
    "    if cache_file is not None:                                 # comp saved cache data for future purpose so, the operation will be faster \n",
    "        try: \n",
    "            with open(os.path.join(cache_dir, cache_file), \"rb\") as f:           # read bite form pickle file\n",
    "                cache_data = pickle.load(f)\n",
    "            print(\"Read preprocessed data from cache file :\" , cache_file)\n",
    "        except:\n",
    "            pass                       \n",
    "\n",
    "    if cache_data is None:\n",
    "        words_train = [review_to_words(review) for review in data_train]    # generate list [] from available dict. \"data_train\"\n",
    "        words_test = [review_to_words(review) for review in data_test]     # ... same \n",
    "\n",
    "        if cache_file is not None:\n",
    "            cache_data = dict(words_train = words_train,words_test = words_test, labels_train = labels_train,labels_test=labels_test)\n",
    "        with open(os.path.join(cache_dir, cache_file),\"wb\") as f:\n",
    "            pickle.dump(cache_data,f)\n",
    "            print(\"Wrote preprocessed data to cache file: \", cache_file)\n",
    "    else: \n",
    "        print(\"Getting from cache data ...\")\n",
    "        words_train,words_test,labels_train,labels_test = (cache_data['words_train'],cache_data['words_test'],cache_data['labels_train'],cache_data['labels_test'])\n",
    "      \n",
    "    return words_train,words_test,labels_train,labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote preprocessed data to cache file:  preprocessed_data.pkl\n"
     ]
    }
   ],
   "source": [
    "# Preprocess data\n",
    "train_X, test_X, train_y, test_y = preprocess_data(train_X, test_X, train_y, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Explainations of preprocess operation:** \n",
    "\n",
    "Here, We had two options, \n",
    "> first one **load data directly from cache_file,which generated previously. If does not exist, then** and then move to cache_data....\n",
    ">> (A)  Now, first **check the data existance as cache_data**, if it is there in **empty cache_data, then generate train and test list** for cache_data  and also write operation (dump) to **fill the cache_file.** So, for future purpose our data will be taken from cache_file.<br>\n",
    ">> (B) But, **if cache_data is already exists, then better to load data** from it,to save time.\n",
    "\n",
    "Still, in case of confusion!, its better to make a flow diagram on paper ownself. ;) :).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.externals import joblib\n",
    "# joblib is an enhanced version of pickle that is more efficient for storing NumPy arrays\n",
    "\n",
    "def extract_BoW_features(words_train, words_test, vocabulary_size=5000,\n",
    "                         cache_dir=cache_dir, cache_file=\"bow_features.pkl\"):\n",
    "    \"\"\"Extract Bag-of-Words for a given set of documents, already preprocessed into words.\"\"\"\n",
    "    \n",
    "    # If cache_file is not None, try to read from it first\n",
    "    cache_data = None\n",
    "    if cache_file is not None:\n",
    "        try:\n",
    "            with open(os.path.join(cache_dir, cache_file), \"rb\") as f:\n",
    "                cache_data = joblib.load(f)\n",
    "            print(\"Read features from cache file:\", cache_file)\n",
    "        except:\n",
    "            pass  # unable to read from cache, but that's okay\n",
    "    \n",
    "    # If cache is missing, then do the heavy lifting\n",
    "    if cache_data is None:\n",
    "        # Fit a vectorizer to training documents and use it to transform them\n",
    "        # NOTE: Training documents have already been preprocessed and tokenized into words;\n",
    "        #       pass in dummy functions to skip those steps, e.g. preprocessor=lambda x: x\n",
    "        vectorizer = CountVectorizer(max_features=vocabulary_size)\n",
    "        features_train = vectorizer.fit_transform(words_train).toarray()\n",
    "\n",
    "        # Apply the same vectorizer to transform the test documents (ignore unknown words)\n",
    "        features_test = vectorizer.transform(words_test).toarray()\n",
    "        \n",
    "        # NOTE: Remember to convert the features using .toarray() for a compact representation\n",
    "        \n",
    "        # Write to cache file for future runs (store vocabulary as well)\n",
    "        if cache_file is not None:\n",
    "            vocabulary = vectorizer.vocabulary_\n",
    "            cache_data = dict(features_train=features_train, features_test=features_test,\n",
    "                             vocabulary=vocabulary)\n",
    "            with open(os.path.join(cache_dir, cache_file), \"wb\") as f:\n",
    "                joblib.dump(cache_data, f)\n",
    "            print(\"Wrote features to cache file:\", cache_file)\n",
    "    else:\n",
    "        # Unpack data loaded from cache file\n",
    "        features_train, features_test, vocabulary = (cache_data['features_train'],\n",
    "                cache_data['features_test'], cache_data['vocabulary'])\n",
    "    \n",
    "    # Return both the extracted features as well as the vocabulary\n",
    "    return features_train, features_test, vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote features to cache file: bow_features.pkl\n"
     ]
    }
   ],
   "source": [
    "# Extract Bag of Words features for both training and test datasets\n",
    "train_X, test_X, vocabulary = extract_BoW_features(train_X, test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_X[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload data to S3 [Create DataBase]\n",
    "\n",
    "### Preparation for classification of XGBoost Algorithm\n",
    "\n",
    "SageMaker has predefined XGBoost Algirthm for classificatio task. But for better accuracy and avoid overfitting I want to use validation dataset. For that, first we will give first 10000 review to validation and then give data to XGBoost in panda dataframe format. The data is stored in S3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Earlier we shuffled the training dataset so to make things simple we can just assign\n",
    "# the first 10 000 reviews to the validation set and use the remaining reviews for training.\n",
    "val_X = pd.DataFrame(train_X[:10000])\n",
    "train_X = pd.DataFrame(train_X[10000:])\n",
    "\n",
    "val_y = pd.DataFrame(train_y[:10000])\n",
    "train_y = pd.DataFrame(train_y[10000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>4990</th>\n",
       "      <th>4991</th>\n",
       "      <th>4992</th>\n",
       "      <th>4993</th>\n",
       "      <th>4994</th>\n",
       "      <th>4995</th>\n",
       "      <th>4996</th>\n",
       "      <th>4997</th>\n",
       "      <th>4998</th>\n",
       "      <th>4999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 5000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0     1     2     3     4     5     6     7     8     9     ...  4990  \\\n",
       "0     2     0     0     0     0     0     1     0     0     0  ...     0   \n",
       "1     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "2     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "3     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "4     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "\n",
       "   4991  4992  4993  4994  4995  4996  4997  4998  4999  \n",
       "0     0     0     0     0     0     0     0     0     0  \n",
       "1     1     0     0     0     0     0     0     0     0  \n",
       "2     0     1     0     0     1     0     0     0     0  \n",
       "3     0     0     0     0     0     0     0     0     0  \n",
       "4     0     1     0     0     0     0     0     0     0  \n",
       "\n",
       "[5 rows x 5000 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>4990</th>\n",
       "      <th>4991</th>\n",
       "      <th>4992</th>\n",
       "      <th>4993</th>\n",
       "      <th>4994</th>\n",
       "      <th>4995</th>\n",
       "      <th>4996</th>\n",
       "      <th>4997</th>\n",
       "      <th>4998</th>\n",
       "      <th>4999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 5000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0     1     2     3     4     5     6     7     8     9     ...  4990  \\\n",
       "0     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "1     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "2     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "3     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "4     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "\n",
       "   4991  4992  4993  4994  4995  4996  4997  4998  4999  \n",
       "0     0     0     0     0     0     0     0     0     0  \n",
       "1     0     0     0     0     0     0     0     0     0  \n",
       "2     2     0     0     0     0     0     0     0     0  \n",
       "3     0     0     0     0     0     0     0     0     0  \n",
       "4     0     0     0     0     0     0     0     0     0  \n",
       "\n",
       "[5 rows x 5000 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0\n",
       "0  0\n",
       "1  1\n",
       "2  0\n",
       "3  0\n",
       "4  0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0\n",
       "0  1\n",
       "1  1\n",
       "2  0\n",
       "3  1\n",
       "4  1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we make sure that the local directory in which we'd like to store the training and validation csv files exists.\n",
    "data_dir = '../data/sentiment_web_app'\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The documentation for the XGBoost algorithm of AWS clearly requires data without headings. For future visualization purpose we can add heading after ML operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data to dictionary (take some time - 1 min) , check your local instance to see the directory\n",
    "pd.DataFrame(test_X).to_csv(os.path.join(data_dir, 'test.csv'), header=False, index=False)                           # test.csv\n",
    "\n",
    "pd.concat([val_y, val_X], axis=1).to_csv(os.path.join(data_dir, 'validation.csv'), header=False, index=False)        # validation.csv\n",
    "\n",
    "pd.concat([train_y, train_X], axis=1).to_csv(os.path.join(data_dir, 'train.csv'), header=False, index=False)         # train.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here, below I am reseting my memory, well, it depends on you instance type. Previousely I do the same work with instance type t2.m2.medium (2GB), but it gave me memory error: \"Unable to allocate 954. MiB for an array with shape (25000, 5000) and data type int64\" , to avoid it I increase my instance type to ml.m5.large (8GB). It actually increase my cost to 0.26  /â„Žð‘Ÿð‘“ð‘Ÿð‘œð‘š0.02 /hr. But, it gives me high internet setup with process speed up. Actually, I will set ml.m4.large for training.\n",
    "\n",
    "If you are new then please check documentation, you may give some hours of free services, which actually devide in modeling hours, training hours and deploying hours. For me it was 250 hrs, 50 hrs and 125 hours accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize memory storage (so, set a bit of memory to None).\n",
    "\n",
    "test_X = train_X = val_X = train_y = val_y = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another, important point is, you can not erase such memory every time because when you train deep algorithms, where data will use again and again.\n",
    "\n",
    "---\n",
    "\n",
    "## Uploading Training/validation to S3 \n",
    "\n",
    "### Flow :=> Local_Dir    -TO-   S3(data)    -TO-     Sagemaker(training)    -TO-    S3(result)    -TO-   Local_Dir \n",
    "\n",
    "here, Local_Dir is our notebook instance, not your machine space. Check Jupyter Notebook instance main folde, for that.\n",
    "\n",
    "Here, I am going to use sagemaker's high level functionality so, all the background work will be done by sagemaker ownself, and I just need to provide resources, commands and requirements to sagemaker. This is regid but quicker approach.\n",
    "\n",
    "There is posibility of Low level functionality, which give us chance to provide flexiblility to model, but when you need to do some research around your result. Well, here in future I will use auto Hyper parameter tuning, to get best answer (with high accuracy) for our dataset problem. So, its nice to use highlevel features.\n",
    "\n",
    "#### Let's start real work with SAGEMAKER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'upload_data' method will be deprecated in favor of 'S3Uploader' class (https://sagemaker.readthedocs.io/en/stable/s3.html#sagemaker.s3.S3Uploader) in SageMaker Python SDK v2.\n",
      "'upload_data' method will be deprecated in favor of 'S3Uploader' class (https://sagemaker.readthedocs.io/en/stable/s3.html#sagemaker.s3.S3Uploader) in SageMaker Python SDK v2.\n",
      "'upload_data' method will be deprecated in favor of 'S3Uploader' class (https://sagemaker.readthedocs.io/en/stable/s3.html#sagemaker.s3.S3Uploader) in SageMaker Python SDK v2.\n"
     ]
    }
   ],
   "source": [
    "import sagemaker                                                                # call sagemaker\n",
    "session = sagemaker.Session()                                                   # create  session for sagemaker \n",
    "prefix = 'sentiment-xgboost'                                                    # prefix will be used for unique name identification (in near future)\n",
    "\n",
    "# set specific location on S3 for easy access \n",
    "test_location = session.upload_data(os.path.join(data_dir,'test.csv'),key_prefix= prefix)           # upload test data \n",
    "val_location = session.upload_data(os.path.join(data_dir,'validation.csv'),key_prefix = prefix)    # upload validation data\n",
    "train_location = session.upload_data(os.path.join(data_dir, 'train.csv'),key_prefix = prefix)        # upload train data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create XGBoost model tuning requirement [Development Phase-Model Building]\n",
    "\n",
    "As I declared before, I am using high level API, helps me to get answer quickly without more flexibility. But, after auto tuing we will get the best answer. Now, here before training, we need to do some setup. \n",
    "\n",
    "Sagemaker model creation : it's ecosystem has three different objects, which are interactive with eachother. \n",
    "1. Model Artifacts\n",
    "2. Training Code (container)\n",
    "3. Inference Code (container)\n",
    "\n",
    "Model artifact is Model itself. The training code use training data, and create model artifacts. Inference code use the model artifacts to predict new data. \n",
    "\n",
    "Sagemaker use docker containers. So, after all docker container is one kind of package of code with proper sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role                                          \n",
    "role = get_execution_role()                                                     # create model execution role = IAM role, for giving permission to specific person or user group (to control unauthorize access)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'get_image_uri' method will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n",
      "WARNING:root:There is a more up to date SageMaker XGBoost image. To use the newer image, please set 'repo_version'='1.0-1'. For example:\n",
      "\tget_image_uri(region, 'xgboost', '1.0-1').\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "container = get_image_uri(session.boto_region_name,'xgboost')                   # set container for giving private space to model (when you have more than one deploying model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Parameter image_name will be renamed to image_uri in SageMaker Python SDK v2.\n"
     ]
    }
   ],
   "source": [
    "# specify model with requried parameters \n",
    "xgb = sagemaker.estimator.Estimator(container,                                  # initialize modelxgb = sagemaker.estimator.Estimator(container,                                  # define container (where to take data)\n",
    "                                    role,                                       # define role (who give permission for this)\n",
    "                                    train_instance_count = 1,                   # instance will used for task (more instance, more power, more expense)\n",
    "                                    train_instance_type = 'ml.m4.xlarge',       # power of isntance (more power, more expense, less execution time) , its different than your notebook instance. \n",
    "                                    output_path = 's3://{}/{}/output'.format(session.default_bucket(),prefix),   # where to save\n",
    "                                    sagemaker_session= session)                 # define session (the current one)\n",
    "\n",
    "\n",
    "\n",
    "xgb.set_hyperparameters(max_depth=5,                                             # set parameters\n",
    "                        eta=0.2,\n",
    "                        gamma=4,\n",
    "                        min_child_weight=6,\n",
    "                        subsample=0.8,\n",
    "                        silent=0,\n",
    "                        objective='binary:logistic',\n",
    "                        early_stopping_rounds=10,\n",
    "                        num_round=500)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the XGBoost model [Development -Training Phase]\n",
    "\n",
    "We already defined model, and also generated the model data. \n",
    "\n",
    "Now the next step will be to fit data within model. Means... train our model on dataset. It takes time and for training you have two options in term of instance capacity. \n",
    "\n",
    "For me **m1.m4.xlarge** is still in free-tier hours(125hr). So, I am going to use it. otherwise the notebook instance **(m1.m5.xlarge)** which I used is better than this. But, as I discussed earlier **I had problem with cache data of instance memory. So, I used high power model building instance.** You are free to use any. \n",
    "> *Please refer the Sagemaker documentation for more information regarding price and capacity. Thank you*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker:'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n",
      "WARNING:sagemaker:'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n"
     ]
    }
   ],
   "source": [
    "s3_input_train = sagemaker.s3_input(s3_data=train_location, content_type='csv')\n",
    "s3_input_validation = sagemaker.s3_input(s3_data=val_location, content_type='csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-14 19:23:11 Starting - Starting the training job...\n",
      "2020-06-14 19:23:13 Starting - Launching requested ML instances.........\n",
      "2020-06-14 19:24:49 Starting - Preparing the instances for training...\n",
      "2020-06-14 19:25:39 Downloading - Downloading input data...\n",
      "2020-06-14 19:26:13 Training - Downloading the training image...\n",
      "2020-06-14 19:26:33 Training - Training image download completed. Training in progress.\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[34m[2020-06-14:19:26:33:INFO] Running standalone xgboost training.\u001b[0m\n",
      "\u001b[34m[2020-06-14:19:26:33:INFO] File size need to be processed in the node: 238.5mb. Available memory size in the node: 8474.78mb\u001b[0m\n",
      "\u001b[34m[2020-06-14:19:26:33:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[19:26:33] S3DistributionType set as FullyReplicated\u001b[0m\n",
      "\u001b[34m[19:26:35] 15000x5000 matrix with 75000000 entries loaded from /opt/ml/input/data/train?format=csv&label_column=0&delimiter=,\u001b[0m\n",
      "\u001b[34m[2020-06-14:19:26:35:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[19:26:35] S3DistributionType set as FullyReplicated\u001b[0m\n",
      "\u001b[34m[19:26:36] 10000x5000 matrix with 50000000 entries loaded from /opt/ml/input/data/validation?format=csv&label_column=0&delimiter=,\u001b[0m\n",
      "\u001b[34m[19:26:40] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 44 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[0]#011train-error:0.304733#011validation-error:0.3125\u001b[0m\n",
      "\u001b[34mMultiple eval metrics have been passed: 'validation-error' will be used for early stopping.\n",
      "\u001b[0m\n",
      "\u001b[34mWill train until validation-error hasn't improved in 10 rounds.\u001b[0m\n",
      "\u001b[34m[19:26:41] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 48 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[1]#011train-error:0.283333#011validation-error:0.2996\u001b[0m\n",
      "\u001b[34m[19:26:42] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 42 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[2]#011train-error:0.2764#011validation-error:0.2948\u001b[0m\n",
      "\u001b[34m[19:26:44] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[3]#011train-error:0.286467#011validation-error:0.2981\u001b[0m\n",
      "\u001b[34m[19:26:45] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[4]#011train-error:0.268333#011validation-error:0.2795\u001b[0m\n",
      "\u001b[34m[19:26:47] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[5]#011train-error:0.265467#011validation-error:0.277\u001b[0m\n",
      "\u001b[34m[19:26:48] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 44 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[6]#011train-error:0.249333#011validation-error:0.2647\u001b[0m\n",
      "\u001b[34m[19:26:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[7]#011train-error:0.241467#011validation-error:0.2568\u001b[0m\n",
      "\u001b[34m[19:26:50] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 44 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[8]#011train-error:0.234267#011validation-error:0.2529\u001b[0m\n",
      "\u001b[34m[19:26:52] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 34 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[9]#011train-error:0.2278#011validation-error:0.2463\u001b[0m\n",
      "\u001b[34m[19:26:53] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 32 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[10]#011train-error:0.2268#011validation-error:0.2438\u001b[0m\n",
      "\u001b[34m[19:26:54] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[11]#011train-error:0.221667#011validation-error:0.2382\u001b[0m\n",
      "\u001b[34m[19:26:56] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 34 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[12]#011train-error:0.2144#011validation-error:0.2357\u001b[0m\n",
      "\u001b[34m[19:26:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 22 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[13]#011train-error:0.209533#011validation-error:0.2312\u001b[0m\n",
      "\u001b[34m[19:26:58] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[14]#011train-error:0.207#011validation-error:0.2263\u001b[0m\n",
      "\u001b[34m[19:26:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 34 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[15]#011train-error:0.202867#011validation-error:0.2251\u001b[0m\n",
      "\u001b[34m[19:27:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[16]#011train-error:0.2006#011validation-error:0.2236\u001b[0m\n",
      "\u001b[34m[19:27:02] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[17]#011train-error:0.197467#011validation-error:0.2175\u001b[0m\n",
      "\u001b[34m[19:27:03] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[18]#011train-error:0.196067#011validation-error:0.2144\u001b[0m\n",
      "\u001b[34m[19:27:05] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[19]#011train-error:0.193733#011validation-error:0.2132\u001b[0m\n",
      "\u001b[34m[19:27:06] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[20]#011train-error:0.191267#011validation-error:0.2117\u001b[0m\n",
      "\u001b[34m[19:27:07] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[21]#011train-error:0.188533#011validation-error:0.2096\u001b[0m\n",
      "\u001b[34m[19:27:08] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[22]#011train-error:0.185333#011validation-error:0.2074\u001b[0m\n",
      "\u001b[34m[19:27:10] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 32 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[23]#011train-error:0.1842#011validation-error:0.2071\u001b[0m\n",
      "\u001b[34m[19:27:11] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[24]#011train-error:0.182467#011validation-error:0.2054\u001b[0m\n",
      "\u001b[34m[19:27:12] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[25]#011train-error:0.18#011validation-error:0.2026\u001b[0m\n",
      "\u001b[34m[19:27:14] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[26]#011train-error:0.177467#011validation-error:0.2009\u001b[0m\n",
      "\u001b[34m[19:27:15] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 34 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[27]#011train-error:0.174533#011validation-error:0.198\u001b[0m\n",
      "\u001b[34m[19:27:16] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[28]#011train-error:0.172867#011validation-error:0.197\u001b[0m\n",
      "\u001b[34m[19:27:17] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[29]#011train-error:0.171733#011validation-error:0.1959\u001b[0m\n",
      "\u001b[34m[19:27:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[30]#011train-error:0.170333#011validation-error:0.1944\u001b[0m\n",
      "\u001b[34m[19:27:20] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[31]#011train-error:0.169067#011validation-error:0.1922\u001b[0m\n",
      "\u001b[34m[19:27:21] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[32]#011train-error:0.168733#011validation-error:0.1904\u001b[0m\n",
      "\u001b[34m[19:27:22] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[33]#011train-error:0.167133#011validation-error:0.1897\u001b[0m\n",
      "\u001b[34m[19:27:24] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[34]#011train-error:0.165933#011validation-error:0.1897\u001b[0m\n",
      "\u001b[34m[19:27:25] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[35]#011train-error:0.163667#011validation-error:0.1891\u001b[0m\n",
      "\u001b[34m[19:27:26] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[36]#011train-error:0.1622#011validation-error:0.1882\u001b[0m\n",
      "\u001b[34m[19:27:28] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[37]#011train-error:0.1616#011validation-error:0.1872\u001b[0m\n",
      "\u001b[34m[19:27:29] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[38]#011train-error:0.1588#011validation-error:0.1852\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[19:27:30] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[39]#011train-error:0.157067#011validation-error:0.1839\u001b[0m\n",
      "\u001b[34m[19:27:31] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[40]#011train-error:0.1558#011validation-error:0.1832\u001b[0m\n",
      "\u001b[34m[19:27:33] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[41]#011train-error:0.154667#011validation-error:0.1841\u001b[0m\n",
      "\u001b[34m[19:27:34] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 44 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[42]#011train-error:0.151333#011validation-error:0.1823\u001b[0m\n",
      "\u001b[34m[19:27:35] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[43]#011train-error:0.149733#011validation-error:0.1805\u001b[0m\n",
      "\u001b[34m[19:27:37] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[44]#011train-error:0.148333#011validation-error:0.1791\u001b[0m\n",
      "\u001b[34m[19:27:38] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[45]#011train-error:0.1468#011validation-error:0.1776\u001b[0m\n",
      "\u001b[34m[19:27:39] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[46]#011train-error:0.146267#011validation-error:0.1769\u001b[0m\n",
      "\u001b[34m[19:27:40] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 18 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[47]#011train-error:0.1442#011validation-error:0.177\u001b[0m\n",
      "\u001b[34m[19:27:42] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 40 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[48]#011train-error:0.142733#011validation-error:0.1773\u001b[0m\n",
      "\u001b[34m[19:27:43] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 16 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[49]#011train-error:0.142267#011validation-error:0.1759\u001b[0m\n",
      "\u001b[34m[19:27:44] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[50]#011train-error:0.1398#011validation-error:0.1756\u001b[0m\n",
      "\u001b[34m[19:27:45] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[51]#011train-error:0.138667#011validation-error:0.1754\u001b[0m\n",
      "\u001b[34m[19:27:47] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[52]#011train-error:0.137733#011validation-error:0.1756\u001b[0m\n",
      "\u001b[34m[19:27:48] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 16 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[53]#011train-error:0.137467#011validation-error:0.1744\u001b[0m\n",
      "\u001b[34m[19:27:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[54]#011train-error:0.137133#011validation-error:0.1736\u001b[0m\n",
      "\u001b[34m[19:27:51] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[55]#011train-error:0.136133#011validation-error:0.173\u001b[0m\n",
      "\u001b[34m[19:27:52] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[56]#011train-error:0.1354#011validation-error:0.1722\u001b[0m\n",
      "\u001b[34m[19:27:53] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 16 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[57]#011train-error:0.134533#011validation-error:0.1709\u001b[0m\n",
      "\u001b[34m[19:27:54] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[58]#011train-error:0.133333#011validation-error:0.1696\u001b[0m\n",
      "\u001b[34m[19:27:56] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 16 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[59]#011train-error:0.131933#011validation-error:0.1688\u001b[0m\n",
      "\u001b[34m[19:27:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[60]#011train-error:0.131267#011validation-error:0.1686\u001b[0m\n",
      "\u001b[34m[19:27:58] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[61]#011train-error:0.1308#011validation-error:0.1689\u001b[0m\n",
      "\u001b[34m[19:27:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 36 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[62]#011train-error:0.128667#011validation-error:0.1678\u001b[0m\n",
      "\u001b[34m[19:28:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[63]#011train-error:0.128133#011validation-error:0.1685\u001b[0m\n",
      "\u001b[34m[19:28:02] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[64]#011train-error:0.127067#011validation-error:0.1678\u001b[0m\n",
      "\u001b[34m[19:28:03] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 42 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[65]#011train-error:0.126733#011validation-error:0.168\u001b[0m\n",
      "\u001b[34m[19:28:05] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 16 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[66]#011train-error:0.125067#011validation-error:0.1673\u001b[0m\n",
      "\u001b[34m[19:28:06] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[67]#011train-error:0.1248#011validation-error:0.1666\u001b[0m\n",
      "\u001b[34m[19:28:07] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[68]#011train-error:0.124133#011validation-error:0.1653\u001b[0m\n",
      "\u001b[34m[19:28:08] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[69]#011train-error:0.123933#011validation-error:0.1651\u001b[0m\n",
      "\u001b[34m[19:28:10] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[70]#011train-error:0.123467#011validation-error:0.1643\u001b[0m\n",
      "\u001b[34m[19:28:11] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[71]#011train-error:0.1222#011validation-error:0.1643\u001b[0m\n",
      "\u001b[34m[19:28:12] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[72]#011train-error:0.121933#011validation-error:0.1641\u001b[0m\n",
      "\u001b[34m[19:28:14] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[73]#011train-error:0.122067#011validation-error:0.1635\u001b[0m\n",
      "\u001b[34m[19:28:15] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 36 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[74]#011train-error:0.121067#011validation-error:0.1637\u001b[0m\n",
      "\u001b[34m[19:28:16] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[75]#011train-error:0.120933#011validation-error:0.163\u001b[0m\n",
      "\u001b[34m[19:28:17] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 16 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[76]#011train-error:0.120867#011validation-error:0.162\u001b[0m\n",
      "\u001b[34m[19:28:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[77]#011train-error:0.1194#011validation-error:0.1615\u001b[0m\n",
      "\u001b[34m[19:28:20] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[78]#011train-error:0.1184#011validation-error:0.1613\u001b[0m\n",
      "\u001b[34m[19:28:21] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 32 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[79]#011train-error:0.117333#011validation-error:0.1609\u001b[0m\n",
      "\u001b[34m[19:28:22] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[80]#011train-error:0.117467#011validation-error:0.1606\u001b[0m\n",
      "\u001b[34m[19:28:24] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[81]#011train-error:0.116533#011validation-error:0.1598\u001b[0m\n",
      "\u001b[34m[19:28:25] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[82]#011train-error:0.116#011validation-error:0.1597\u001b[0m\n",
      "\u001b[34m[19:28:26] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 16 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[83]#011train-error:0.115467#011validation-error:0.1592\u001b[0m\n",
      "\u001b[34m[19:28:28] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[84]#011train-error:0.114267#011validation-error:0.1589\u001b[0m\n",
      "\u001b[34m[19:28:29] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[85]#011train-error:0.114067#011validation-error:0.1587\u001b[0m\n",
      "\u001b[34m[19:28:30] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[86]#011train-error:0.113267#011validation-error:0.1582\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[19:28:31] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[87]#011train-error:0.112267#011validation-error:0.157\u001b[0m\n",
      "\u001b[34m[19:28:33] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[88]#011train-error:0.111267#011validation-error:0.1572\u001b[0m\n",
      "\u001b[34m[19:28:34] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[89]#011train-error:0.11#011validation-error:0.1568\u001b[0m\n",
      "\u001b[34m[19:28:35] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 32 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[90]#011train-error:0.110067#011validation-error:0.1558\u001b[0m\n",
      "\u001b[34m[19:28:36] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[91]#011train-error:0.108667#011validation-error:0.1567\u001b[0m\n",
      "\u001b[34m[19:28:38] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[92]#011train-error:0.108#011validation-error:0.1539\u001b[0m\n",
      "\u001b[34m[19:28:39] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[93]#011train-error:0.107133#011validation-error:0.1535\u001b[0m\n",
      "\u001b[34m[19:28:40] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[94]#011train-error:0.107267#011validation-error:0.1531\u001b[0m\n",
      "\u001b[34m[19:28:42] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[95]#011train-error:0.107733#011validation-error:0.1526\u001b[0m\n",
      "\u001b[34m[19:28:43] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[96]#011train-error:0.107933#011validation-error:0.1531\u001b[0m\n",
      "\u001b[34m[19:28:44] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[97]#011train-error:0.107267#011validation-error:0.153\u001b[0m\n",
      "\u001b[34m[19:28:45] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 16 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[98]#011train-error:0.107#011validation-error:0.1529\u001b[0m\n",
      "\u001b[34m[19:28:47] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[99]#011train-error:0.106267#011validation-error:0.1532\u001b[0m\n",
      "\u001b[34m[19:28:48] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 24 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[100]#011train-error:0.106067#011validation-error:0.1528\u001b[0m\n",
      "\u001b[34m[19:28:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[101]#011train-error:0.105133#011validation-error:0.1537\u001b[0m\n",
      "\u001b[34m[19:28:51] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[102]#011train-error:0.105333#011validation-error:0.1532\u001b[0m\n",
      "\u001b[34m[19:28:52] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[103]#011train-error:0.105333#011validation-error:0.1535\u001b[0m\n",
      "\u001b[34m[19:28:53] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[104]#011train-error:0.104733#011validation-error:0.1538\u001b[0m\n",
      "\u001b[34m[19:28:54] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[105]#011train-error:0.103467#011validation-error:0.1526\u001b[0m\n",
      "\u001b[34mStopping. Best iteration:\u001b[0m\n",
      "\u001b[34m[95]#011train-error:0.107733#011validation-error:0.1526\n",
      "\u001b[0m\n",
      "\n",
      "2020-06-14 19:29:07 Uploading - Uploading generated training model\n",
      "2020-06-14 19:29:07 Completed - Training job completed\n",
      "Training seconds: 208\n",
      "Billable seconds: 208\n"
     ]
    }
   ],
   "source": [
    "xgb.fit({'train': s3_input_train, 'validation': s3_input_validation})            #[3:23-------------->3:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, our model is trained, Check our instance for model info. In case if you have problem to find it, then better option is log_files of sagemaker will give you idea of background procedure.\n",
    "\n",
    "---\n",
    "## Testing Model [Testing Phase]\n",
    "\n",
    "I will use SageMakers Batch Transform functionality.\n",
    "\n",
    "Batch Transform is a convenient way to perform inference on a large dataset in a way that is not realtime. That is, we don't necessarily need to use our model's results immediately and instead we can peform inference on a large number of samples.\n",
    "\n",
    "**Applications:**\n",
    ">Industries, which run their business continueously and want to predict their growth and customer service periodically, may be at the end of week, or end of month. They will use batch transform. So, its not used for realtime applications. Small businesses mostly use it. Sometime industry giants use it for specific problem solution. (as an example, some specific region have issue with specific type of product, then for 5W QA analysis they can use it.) \n",
    "\n",
    "---\n",
    "the following procedure takes some time (5 to 10 minute)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker:Parameter image will be renamed to image_uri in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "......................\u001b[34mArguments: serve\u001b[0m\n",
      "\u001b[34m[2020-06-14 19:33:10 +0000] [1] [INFO] Starting gunicorn 19.7.1\u001b[0m\n",
      "\u001b[34m[2020-06-14 19:33:10 +0000] [1] [INFO] Listening at: http://0.0.0.0:8080 (1)\u001b[0m\n",
      "\u001b[34m[2020-06-14 19:33:10 +0000] [1] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[34m[2020-06-14 19:33:10 +0000] [38] [INFO] Booting worker with pid: 38\u001b[0m\n",
      "\u001b[34m[2020-06-14 19:33:10 +0000] [39] [INFO] Booting worker with pid: 39\u001b[0m\n",
      "\u001b[34m[2020-06-14 19:33:10 +0000] [40] [INFO] Booting worker with pid: 40\u001b[0m\n",
      "\u001b[34m[2020-06-14 19:33:10 +0000] [41] [INFO] Booting worker with pid: 41\u001b[0m\n",
      "\u001b[34m[2020-06-14:19:33:10:INFO] Model loaded successfully for worker : 38\u001b[0m\n",
      "\u001b[34m[2020-06-14:19:33:10:INFO] Model loaded successfully for worker : 39\u001b[0m\n",
      "\u001b[34m[2020-06-14:19:33:10:INFO] Model loaded successfully for worker : 40\u001b[0m\n",
      "\u001b[34m[2020-06-14:19:33:10:INFO] Model loaded successfully for worker : 41\u001b[0m\n",
      "\u001b[32m2020-06-14T19:33:40.381:[sagemaker logs]: MaxConcurrentTransforms=4, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n",
      "\u001b[34m[2020-06-14:19:33:43:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-14:19:33:43:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-14:19:33:43:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-14:19:33:43:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-14:19:33:43:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-14:19:33:43:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-14:19:33:43:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-14:19:33:43:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-14:19:33:43:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-14:19:33:43:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-14:19:33:44:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-14:19:33:44:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-14:19:33:43:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-14:19:33:43:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-14:19:33:44:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-14:19:33:44:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-14:19:33:46:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-14:19:33:46:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-14:19:33:46:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-14:19:33:46:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-14:19:33:46:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-14:19:33:46:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-14:19:33:46:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-14:19:33:46:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-14:19:33:46:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-14:19:33:46:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-14:19:33:46:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-14:19:33:46:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-14:19:33:46:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-14:19:33:46:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-14:19:33:46:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-14:19:33:46:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-14:19:33:48:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-14:19:33:48:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-14:19:33:48:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-14:19:33:48:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-14:19:33:48:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-14:19:33:48:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-14:19:33:48:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-14:19:33:48:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-14:19:33:49:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-14:19:33:49:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-14:19:33:48:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-14:19:33:48:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-14:19:33:48:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-14:19:33:48:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-14:19:33:49:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-14:19:33:49:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-14:19:33:51:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-14:19:33:51:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-14:19:33:51:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-14:19:33:51:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-14:19:33:51:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-14:19:33:51:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-14:19:33:51:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-14:19:33:51:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-14:19:33:51:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-14:19:33:51:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-14:19:33:51:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-14:19:33:51:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-14:19:33:51:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-14:19:33:51:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-14:19:33:51:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-14:19:33:51:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-14:19:33:55:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-14:19:33:55:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-14:19:33:56:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-14:19:33:56:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-14:19:33:56:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-14:19:33:56:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-14:19:33:56:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-14:19:33:56:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-14:19:33:55:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-14:19:33:55:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-14:19:33:56:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-14:19:33:56:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-14:19:33:56:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-14:19:33:56:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-14:19:33:56:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-14:19:33:56:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-14:19:33:58:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-14:19:33:58:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-14:19:33:58:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-14:19:33:58:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-14:19:33:58:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-14:19:33:58:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-14:19:33:58:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-14:19:33:58:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-14:19:33:58:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-14:19:33:58:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-14:19:33:58:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-14:19:33:58:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-14:19:33:58:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-14:19:33:58:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-14:19:33:58:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-14:19:33:58:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-14:19:34:00:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-14:19:34:00:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-14:19:34:00:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-14:19:34:00:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-14:19:34:00:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-14:19:34:00:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-14:19:34:01:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-14:19:34:01:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-14:19:34:01:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-14:19:34:01:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-14:19:34:00:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-14:19:34:00:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-14:19:34:01:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-14:19:34:01:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-14:19:34:01:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-14:19:34:01:INFO] Determined delimiter of CSV input is ','\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m[2020-06-14:19:34:03:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-14:19:34:03:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-14:19:34:03:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-14:19:34:03:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-14:19:34:05:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-14:19:34:05:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-14:19:34:05:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-14:19:34:05:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-14:19:34:05:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-14:19:34:05:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-14:19:34:05:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-14:19:34:05:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-14:19:34:05:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-14:19:34:05:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-14:19:34:05:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-14:19:34:06:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-14:19:34:06:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-14:19:34:05:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-14:19:34:06:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-14:19:34:06:INFO] Determined delimiter of CSV input is ','\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "xgb_transformer = xgb.transformer(instance_count =1, instance_type = 'ml.m4.xlarge')       # used Batch_transform method from sagemaker\n",
    "\n",
    "xgb_transformer.transform(test_location,content_type = 'text/csv',split_type= 'Line')     # read data from test location for predictinog result.\n",
    "\n",
    "xgb_transformer.wait()                                                                     # wait for response (to avoid long output format like last one, \n",
    "#                                                                                                all information works in background and catched by view_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the transform job has executed and the result, the estimated sentiment of each review, has been saved on S3. Since we would rather work on this file locally we can perform a bit of notebook magic to copy the file to the `data_dir`. (to local notebook instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 256.0 KiB/368.3 KiB (2.3 MiB/s) with 1 file(s) remaining\r",
      "Completed 368.3 KiB/368.3 KiB (3.2 MiB/s) with 1 file(s) remaining\r",
      "download: s3://sagemaker-us-west-2-337299574287/xgboost-2020-06-14-19-29-44-762/test.csv.out to ../data/sentiment_web_app/test.csv.out\r\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp --recursive $xgb_transformer.output_path $data_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step is now to read in the output from our model, convert the output to something a little more usable, in this case we want the sentiment to be either `1` (positive) or `0` (negative), and then compare to the ground truth labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For, accuracy metric calculation. \n",
    "predictions = pd.read_csv(os.path.join(data_dir, 'test.csv.out'), header=None)\n",
    "predictions = [round(num) for num in predictions.squeeze().values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8464"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(test_y, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Deploying the model [Deployment Phase]\n",
    "\n",
    "Once we construct and fit our model, SageMaker stores the resulting model artifacts and we can use those to deploy an endpoint (inference code). To see this, look in the SageMaker console and you should see that a model has been created along with a link to the S3 location where the model artifacts have been stored.\n",
    "\n",
    "Deploying an endpoint is a lot like training the model with a few important differences. The first is that a deployed model doesn't change the model artifacts, so as you send it various testing instances the model won't change. Another difference is that since we aren't performing a fixed computation, as we were in the training step or while performing a batch transform, the compute instance that gets started stays running until we tell it to stop. This is important to note as if we forget and leave it running we will be charged the entire time.\n",
    "\n",
    "In other words **If you are no longer using a deployed endpoint, shut it down!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker:Parameter image will be renamed to image_uri in SageMaker Python SDK v2.\n",
      "WARNING:sagemaker:Using already existing model: xgboost-2020-06-14-19-23-11-456\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------!"
     ]
    }
   ],
   "source": [
    "xgb_predictor = xgb.deploy(initial_instance_count = 1, instance_type = 'ml.m4.xlarge')   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model (again with deployed model)\n",
    "\n",
    "OUr model is already deployed, but we need to check it before using it form endpoint. For tha I will test the model form newly deployed endpoint. This thing ensure us about two things. \n",
    "1. Model Testing\n",
    "2. Endpoint permission\n",
    "\n",
    "But, now we send our data from endpoint, not from notebook instance (sagemaker, S3) so, we can not send huge information, the better way is to divide them in chunk. Data must be serialized for model.\n",
    "\n",
    "Let's satisfy above requirement..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import csv_serializer\n",
    "\n",
    "# We need to tell the endpoint what format the data we are sending is in so that SageMaker can perform the serialization.\n",
    "xgb_predictor.content_type = 'text/csv'\n",
    "xgb_predictor.serializer = csv_serializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We split the data into chunks and send each chunk seperately, accumulating the results.\n",
    "           \n",
    "def predict(data, rows=512):                                                           # max limit of data 512 row. \n",
    "    split_array = np.array_split(data, int(data.shape[0] / float(rows) + 1))\n",
    "    predictions = ''\n",
    "    for array in split_array:\n",
    "        predictions = ','.join([predictions, xgb_predictor.predict(array).decode('utf-8')])\n",
    "    \n",
    "    return np.fromstring(predictions[1:], sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = pd.read_csv(os.path.join(data_dir, 'test.csv'), header=None).values\n",
    "\n",
    "predictions = predict(test_X)\n",
    "predictions = [round(num) for num in predictions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we check to see what the accuracy of our model is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8464"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(test_y, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the results here should agree with the model testing that we did earlier using the batch transform job. Notice that here, all the testing and predicting jobs executed in background. You can check it from log file of sagemaker. \n",
    "\n",
    "### Cleaning up\n",
    "\n",
    "Now that we've determined that deploying our model works as expected, we are going to shut it down. Remember that the longer the endpoint is left running, the greater the cost and since we have a bit more work to do before we are able to use our endpoint with our simple web app, we should shut everything down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Release Model: \n",
    "\n",
    "As I discussed before, our main purpose is to deploy app in open environment so we can use it from web api. But, currently whole procedure work through Sagemaker API. so, first of all Its better to deploy with some medium which can allow our endpoing from open permission. For that Lambda is used. But, before that we need o authenticate our web app with AWS. \n",
    "\n",
    "we will create one new endpoint (proxy-endpoint), which will be public and no need to authenticate from AWS-sagemaker. Then main question arise is about the data processing. Because, when we deploy our model, we never think about the data preprocessing step. When we give our data it need to be serialized and convert as bag of words. \n",
    "\n",
    "Moreover, we can not give this load to our web application itself. So, Its better to use AWS-Lambda function. The simple process is as follow.\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"Web App Diagram.svg\">\n",
    "\n",
    "What this means is that when someone uses our web app, the following will occur.\n",
    "\n",
    "1. To begin with, a user will type out a review and enter it into our web app.\n",
    "\n",
    "2. Then, our web app will send that review to an endpoint that we created using API Gateway. This endpoint will be constructed so that anyone (including our web app) can use it.\n",
    "\n",
    "3. API Gateway will forward the data on to the Lambda function\n",
    "\n",
    "4. Once the Lambda function receives the user's review, it will process that review by tokenizing it and then creating a bag of words encoding of the result. After that, it will send the processed review off to our deployed model.\n",
    "\n",
    "5. Once the deployed model performs inference on the processed review, the resulting sentiment will be returned back to the Lambda function.\n",
    "\n",
    "6. Our Lambda function will then return the sentiment result back to our web app using the endpoint that was constructed using API Gateway.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Processing a single review\n",
    "\n",
    "For now, suppose we are given a movie review by our user in the form of a string, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_review = \"Nothing but a disgusting materialistic pageant of glistening abed remote control greed zombies, totally devoid of any heart or heat. A romantic comedy that has zero romantic chemestry and zero laughs!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we go from this string to the bag of words feature vector that is expected by our model?\n",
    "\n",
    "If we recall at the beginning of this notebook, the first step is to remove any unnecessary characters using the `review_to_words` method. Remember that we intentionally did this in a very simplistic way. This is because we are going to have to copy this method to our (eventual) Lambda function (we will go into more detail later) and this means it needs to be rather simplistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nothing but a disgusting materialistic pageant of glistening abed remote control greed zombies totally devoid of any heart or heat a romantic comedy that has zero romantic chemestry and zero laughs\n"
     ]
    }
   ],
   "source": [
    "test_words = review_to_words(test_review)\n",
    "print(test_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to construct a bag of words embedding of the `test_words` string. To do this, remember that a bag of words embedding uses a `vocabulary` consisting of the most frequently appearing words in a set of documents. Then, for each word in the vocabulary we record the number of times that word appears in `test_words`. We constructed the `vocabulary` earlier using the training set for our problem so encoding `test_words` is relatively straightforward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bow_encoding(words, vocabulary):                    # our data preprocessing function \n",
    "    bow = [0] * len(vocabulary) # Start by setting the count for each word in the vocabulary to zero.\n",
    "    for word in words.split():  # For each word in the string\n",
    "        if word in vocabulary:  # If the word is one that occurs in the vocabulary, increase its count.\n",
    "            bow[vocabulary[word]] += 1\n",
    "    return bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "test_bow = bow_encoding(test_words, vocabulary)          \n",
    "print(test_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is in 0 and 1, but main problem is regarding format. Its not serialized and not even in required format for our mode. I mean not csv or text format. So, first we need to change it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_bow)                          # Note: this are words not rows, so please do not confuse with 512 chunk size                            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we know how to construct a bag of words encoding of a user provided review, how to we send it to our endpoint? First, we need to start the endpoint back up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker:Parameter image will be renamed to image_uri in SageMaker Python SDK v2.\n",
      "WARNING:sagemaker:Using already existing model: xgboost-2020-06-14-19-23-11-456\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------!"
     ]
    }
   ],
   "source": [
    "xgb_predictor = xgb.deploy(initial_instance_count = 1, instance_type = 'ml.m4.xlarge')      # redeploy model to regenerate endpoint!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the same thing that, we did earlier when we tested our deployed model and send `test_bow` to our endpoint using the `xgb_predictor` object. But, when we have lambda in deployment, then we can not access this endpoint directly.\n",
    "\n",
    "The common approach is to use boto3 library from AWS. This will provide an API which runs with AWS service and will give us leverage to work with lambda, sagemaker and other functions. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "runtime = boto3.Session().client('sagemaker-runtime')                 # handle runtime of sagemaker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now that we have access to the SageMaker runtime, we can ask it to make use of (invoke) an endpoint that has already been created. However, we need to provide SageMaker with the name of the deployed endpoint. To find this out we can print it out using the `xgb_predictor` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'xgboost-2020-06-14-19-23-11-456'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_predictor.endpoint             # 'xgboost-2020-06-14-19-23-11-456'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the SageMaker runtime and the name of our endpoint, we can invoke the endpoint and send it the `test_bow` data. Note that the invoke_endpoint works with live applications. So, our model will not run continuously. It invoke only by user only. So, we do not need to give continuous charge. ---- << ADD >> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParamValidationError",
     "evalue": "Parameter validation failed:\nInvalid type for parameter Body, value: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0], type: <class 'list'>, valid types: <class 'bytes'>, <class 'bytearray'>, file-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParamValidationError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-75-c9ab54ae3a48>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m response = runtime.invoke_endpoint(EndpointName = xgb_predictor.endpoint, # The name of the endpoint we created\n\u001b[1;32m      2\u001b[0m                                        \u001b[0mContentType\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'text/csv'\u001b[0m\u001b[0;34m,\u001b[0m                     \u001b[0;31m# The data format that is expected\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m                                        Body = test_bow)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    314\u001b[0m                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n\u001b[1;32m    315\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    606\u001b[0m         }\n\u001b[1;32m    607\u001b[0m         request_dict = self._convert_to_request_dict(\n\u001b[0;32m--> 608\u001b[0;31m             api_params, operation_model, context=request_context)\n\u001b[0m\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m         \u001b[0mservice_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_service_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mservice_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyphenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_convert_to_request_dict\u001b[0;34m(self, api_params, operation_model, context)\u001b[0m\n\u001b[1;32m    654\u001b[0m             api_params, operation_model, context)\n\u001b[1;32m    655\u001b[0m         request_dict = self._serializer.serialize_to_request(\n\u001b[0;32m--> 656\u001b[0;31m             api_params, operation_model)\n\u001b[0m\u001b[1;32m    657\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_client_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minject_host_prefix\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m             \u001b[0mrequest_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'host_prefix'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/botocore/validate.py\u001b[0m in \u001b[0;36mserialize_to_request\u001b[0;34m(self, parameters, operation_model)\u001b[0m\n\u001b[1;32m    295\u001b[0m                                                     operation_model.input_shape)\n\u001b[1;32m    296\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mreport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_errors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mParamValidationError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreport\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m         return self._serializer.serialize_to_request(parameters,\n\u001b[1;32m    299\u001b[0m                                                      operation_model)\n",
      "\u001b[0;31mParamValidationError\u001b[0m: Parameter validation failed:\nInvalid type for parameter Body, value: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0], type: <class 'list'>, valid types: <class 'bytes'>, <class 'bytearray'>, file-like object"
     ]
    }
   ],
   "source": [
    "response = runtime.invoke_endpoint(EndpointName = xgb_predictor.endpoint, # The name of the endpoint we created\n",
    "                                       ContentType = 'text/csv',                     # The data format that is expected\n",
    "                                       Body = test_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I got error, because of incompatible format. We need serialized input with \"csv/text\" format. As we defined. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = runtime.invoke_endpoint(EndpointName = xgb_predictor.endpoint, # The name of the endpoint we created\n",
    "                                       ContentType = 'text/csv',                     # The data format that is expected\n",
    "                                       Body = ','.join([str(val) for val in test_bow]).encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ResponseMetadata': {'RequestId': '5f08de00-17ce-4bbe-8089-3b9dff235b3b', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '5f08de00-17ce-4bbe-8089-3b9dff235b3b', 'x-amzn-invoked-production-variant': 'AllTraffic', 'date': 'Sun, 14 Jun 2020 22:55:20 GMT', 'content-type': 'text/csv; charset=utf-8', 'content-length': '14'}, 'RetryAttempts': 0}, 'ContentType': 'text/csv; charset=utf-8', 'InvokedProductionVariant': 'AllTraffic', 'Body': <botocore.response.StreamingBody object at 0x7f34a684ca58>}\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the response from our model is a somewhat complicated looking dict that contains a bunch of information. The bit that we are most interested in is `'Body'` object which is a streaming object that we need to `read` in order to make use of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.500335335732\n"
     ]
    }
   ],
   "source": [
    "response = response['Body'].read().decode('utf-8')           # 0.5 is breaking point for our app, so >50 will positive and <0.50 will negative.\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know how to process the incoming user data we can start setting up the infrastructure to make our simple web app work. To do this we will make use of two different services. Amazon's Lambda and API Gateway services.\n",
    "\n",
    "Lambda is a service which allows someone to write some relatively simple code and have it executed whenever a chosen trigger occurs. For example, you may want to update a database whenever new data is uploaded to a folder stored on S3.\n",
    "\n",
    "API Gateway is a service that allows you to create HTTP endpoints (url addresses) which are connected to other AWS services. One of the benefits to this is that you get to decide what credentials, if any, are required to access these endpoints.\n",
    "\n",
    "In our case we are going to set up an HTTP endpoint through API Gateway which is open to the public. Then, whenever anyone sends data to our public endpoint we will trigger a Lambda function which will send the input (in our case a review) to our model's endpoint and then return the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up a Lambda function\n",
    "\n",
    "The first thing we are going to do is set up a Lambda function. This Lambda function will be executed whenever our public API has data sent to it. When it is executed it will receive the data, perform any sort of processing that is required, send the data (the review) to the SageMaker endpoint we've created and then return the result.\n",
    "\n",
    "#### Part A: Create an IAM Role for the Lambda function\n",
    "\n",
    "Since we want the Lambda function to call a SageMaker endpoint, we need to make sure that it has permission to do so. To do this, we will construct a role that we can later give the Lambda function.\n",
    "\n",
    "Using the AWS Console, navigate to the **IAM** page and click on **Roles**. Then, click on **Create role**. Make sure that the **AWS service** is the type of trusted entity selected and choose **Lambda** as the service that will use this role, then click **Next: Permissions**.\n",
    "\n",
    "In the search box type `sagemaker` and select the check box next to the **AmazonSageMakerFullAccess** policy. Then, click on **Next: Review**.\n",
    "\n",
    "Lastly, give this role a name. Make sure you use a name that you will remember later on, for example `LambdaSageMakerRole`. Then, click on **Create role**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part B: Create a Lambda function\n",
    "\n",
    "Now it is time to actually create the Lambda function. Remember from earlier that in order to process the user provided input and send it to our endpoint we need to gather two pieces of information:\n",
    "\n",
    " - The name of the endpoint, and\n",
    " - the vocabulary object.\n",
    "\n",
    "We will copy these pieces of information to our Lambda function after we create it.\n",
    "\n",
    "To start, using the AWS Console, navigate to the AWS Lambda page and click on **Create a function**. When you get to the next page, make sure that **Author from scratch** is selected. Now, name your Lambda function, using a name that you will remember later on, for example `sentiment_analysis_xgboost_func`. Make sure that the **Python 3.6** runtime is selected and then choose the role that you created in the previous part. Then, click on **Create Function**.\n",
    "\n",
    "On the next page you will see some information about the Lambda function you've just created. If you scroll down you should see an editor in which you can write the code that will be executed when your Lambda function is triggered. Collecting the code we wrote above to process a single review and adding it to the provided example `lambda_handler` we arrive at the following.\n",
    "\n",
    "```python\n",
    "# We need to use the low-level library to interact with SageMaker since the SageMaker API\n",
    "# is not available natively through Lambda.\n",
    "import boto3\n",
    "\n",
    "# And we need the regular expression library to do some of the data processing\n",
    "import re\n",
    "\n",
    "REPLACE_NO_SPACE = re.compile(\"(\\.)|(\\;)|(\\:)|(\\!)|(\\')|(\\?)|(\\,)|(\\\")|(\\()|(\\))|(\\[)|(\\])\")\n",
    "REPLACE_WITH_SPACE = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
    "\n",
    "def review_to_words(review):\n",
    "    words = REPLACE_NO_SPACE.sub(\"\", review.lower())\n",
    "    words = REPLACE_WITH_SPACE.sub(\" \", words)\n",
    "    return words\n",
    "    \n",
    "def bow_encoding(words, vocabulary):\n",
    "    bow = [0] * len(vocabulary) # Start by setting the count for each word in the vocabulary to zero.\n",
    "    for word in words.split():  # For each word in the string\n",
    "        if word in vocabulary:  # If the word is one that occurs in the vocabulary, increase its count.\n",
    "            bow[vocabulary[word]] += 1\n",
    "    return bow\n",
    "\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    \n",
    "    vocab = \"*** ACTUAL VOCABULARY GOES HERE ***\"\n",
    "    \n",
    "    words = review_to_words(event['body'])\n",
    "    bow = bow_encoding(words, vocab)\n",
    "\n",
    "    # The SageMaker runtime is what allows us to invoke the endpoint that we've created.\n",
    "    runtime = boto3.Session().client('sagemaker-runtime')\n",
    "\n",
    "    # Now we use the SageMaker runtime to invoke our endpoint, sending the review we were given\n",
    "    response = runtime.invoke_endpoint(EndpointName = '***ENDPOINT NAME HERE***',# The name of the endpoint we created\n",
    "                                       ContentType = 'text/csv',                 # The data format that is expected\n",
    "                                       Body = ','.join([str(val) for val in bow]).encode('utf-8')) # The actual review\n",
    "\n",
    "    # The response is an HTTP response whose body contains the result of our inference\n",
    "    result = response['Body'].read().decode('utf-8')\n",
    "    \n",
    "    # Round the result so that our web app only gets '1' or '0' as a response.\n",
    "    result = round(float(result))\n",
    "\n",
    "    return {\n",
    "        'statusCode' : 200,\n",
    "        'headers' : { 'Content-Type' : 'text/plain', 'Access-Control-Allow-Origin' : '*' },\n",
    "        'body' : str(result)\n",
    "    }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have copy and pasted the code above into the Lambda code editor, replace the `**ENDPOINT NAME HERE**` portion with the name of the endpoint that we deployed earlier. You can determine the name of the endpoint using the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'xgboost-2020-06-14-19-23-11-456'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_predictor.endpoint           # yout endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, you will need to copy the vocabulary dict to the appropriate place in the code at the beginning of the `lambda_handler` method. The cell below prints out the vocabulary dict in a way that is easy to copy and paste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'water': 4816, 'is': 2360, 'well': 4843, 'made': 2697, 'first': 1736, 'film': 1716, 'from': 1840, 'france': 1816, 'about': 67, 'young': 4989, 'female': 1693, 'sexuality': 3909, 'and': 226, 'friendship': 1838, 'works': 4945, 'with': 4911, 'slightly': 4029, 'material': 2766, 'that': 4424, 'as': 311, 'off': 3084, 'putting': 3482, 'to': 4507, 'some': 4061, 'it': 2366, 'others': 3136, 'the': 4426, 'focuses': 1768, 'on': 3105, 'three': 4469, 'middle': 2833, 'class': 833, 'teenage': 4389, 'girls': 1911, 'in': 2257, 'new': 3003, 'paris': 3189, 'their': 4431, 'lives': 2628, 'around': 297, 'big': 478, 'swimming': 4338, 'pool': 3327, 'where': 4864, 'two': 4620, 'of': 3083, 'are': 285, 'part': 3193, 'ballet': 389, 'team': 4378, 'such': 4268, 'parents': 3188, 'work': 4940, 'school': 3813, 'have': 2052, 'been': 438, 'central': 735, 'belongs': 460, 'but': 636, 'marie': 2737, 'who': 4871, 'not': 3040, 'thinks': 4454, 'she': 3923, 'would': 4957, 'like': 2599, 'be': 418, 'an': 223, 'blonde': 511, 'whom': 4875, 'boys': 559, 'after': 143, 'this': 4457, 'takes': 4351, 'away': 369, 'her': 2085, 'former': 1800, 'best': 469, 'friend': 1835, 'also': 199, 'member': 2800, 'somewhat': 4068, 'anne': 246, 'louise': 2666, 'being': 448, 'less': 2575, 'special': 4097, 'more': 2904, 'truly': 4597, 'promises': 3432, 'bit': 488, 'then': 4436, 'offers': 3090, 'has': 2043, 'essential': 1522, 'quality': 3484, 'for': 1784, 'girl': 1909, 'suffers': 4278, 'doesnt': 1309, 'so': 4046, 'much': 2934, 'suffer': 4275, 'jump': 2429, 'into': 2332, 'situations': 4009, 'by': 642, 'advantage': 129, 'make': 2713, 'slave': 4022, 'cover': 1022, 'lacking': 2504, 'any': 256, 'other': 3135, 'friends': 1837, 'all': 187, 'think': 4452, 'men': 2805, 'because': 432, 'leads': 2548, 'them': 4432, 'pretty': 3384, 'fate': 1665, 'person': 3238, 'object': 3066, 'cant': 671, 'resist': 3648, 'give': 1912, 'wanting': 4790, 'kiss': 2483, 'bed': 436, 'really': 3556, 'care': 684, 'knows': 2493, 'trap': 4563, 'act': 97, 'pal': 3177, 'unseen': 4678, 'mother': 2911, 'wont': 4932, 'know': 2489, 'shes': 3930, 'going': 1931, 'out': 3141, 'meet': 2793, 'gets': 1900, 'rescue': 3643, 'later': 2525, 'looked': 2647, 'opposite': 3123, 'at': 331, 'needs': 2990, 'left': 2561, 'body': 527, 'desire': 1205, 'get': 1899, 'laid': 2508, 'thats': 4425, 'real': 3547, 'than': 4420, 'keeping': 2450, 'context': 968, 'bay': 416, 'can': 665, 'highlight': 2103, 'subtle': 4260, 'hand': 2015, 'films': 1721, 'madness': 2698, 'which': 4867, 'includes': 2263, 'lots': 2662, 'shots': 3950, 'makes': 2716, 'feel': 1684, 'completely': 922, 'times': 4497, 'its': 2370, 'minutes': 2859, 'do': 1302, 'pass': 3201, 'quickly': 3491, 'luckily': 2682, 'sense': 3877, 'humor': 2192, 'lets': 2580, 'trio': 4587, 'sometimes': 4067, 'forget': 1793, 'ever': 1538, 'present': 3374, 'just': 2437, 'silly': 3976, 'pointless': 3318, 'things': 4451, 'moments': 2887, 'life': 2592, 'too': 4522, 'bad': 382, 'way': 4820, 'there': 4438, 'arent': 288, 'courage': 1018, 'what': 4858, 'remains': 3611, 'one': 3107, 'walks': 4780, 'theater': 4427, 'personalities': 3241, 'along': 195, 'course': 1019, 'pleasant': 3302, 'watch': 4811, 'love': 2669, 'themselves': 4435, 'theres': 4440, 'no': 3020, 'great': 1964, 'revelation': 3674, 'or': 3124, 'drama': 1346, 'interesting': 2325, 'when': 4862, 'admire': 121, 'jealous': 2388, 'always': 203, 'stops': 4203, 'before': 440, 'they': 4442, 'go': 1924, 'typical': 4623, 'irony': 2357, 'kind': 2476, 'plot': 3309, 'actually': 109, 'decides': 1150, 'wants': 4791, 'sex': 3907, 'holds': 2131, 'will': 4886, 'happens': 2029, 'mechanical': 2788, 'affair': 135, 'meanwhile': 2785, 'huge': 2185, 'crush': 1072, 'warren': 4801, 'male': 2719, 'he': 2056, 'element': 1442, 'seem': 3861, 'several': 3905, 'dozen': 1338, 'ready': 3546, 'dance': 1103, 'floor': 1759, 'bodies': 526, 'faces': 1618, 'available': 361, 'something': 4066, 'paulie': 3215, 'touched': 4534, 'my': 2951, 'heart': 2064, 'few': 1700, 'movies': 2927, 'witty': 4917, 'funny': 1852, 'yet': 4984, 'emotional': 1459, 'movie': 2926, 'im': 2232, 'late': 2523, 'becoming': 435, 'fan': 1648, 'didnt': 1242, 'see': 3855, 'until': 4680, 'may': 2775, '2004': 34, 'since': 3990, 'dvd': 1391, 'effects': 1431, 'showing': 3958, 'talking': 4360, 'superb': 4296, 'son': 4070, 'asked': 317, 'me': 2778, 'how': 2181, 'bird': 484, 'knew': 2486, 'many': 2732, 'favorite': 1671, 'company': 910, 'followed': 1773, 'tony': 4521, 'plays': 3301, 'excellent': 1555, 'good': 1937, 'hearted': 2065, 'human': 2188, 'you': 4986, 'root': 3737, 'him': 2110, 'through': 4477, 'wrong': 4973, 'renting': 3629, 'buying': 641, 'hard': 2033, 'describe': 1194, 'if': 2222, 'were': 4848, 'back': 378, 'day': 1129, 'surely': 4309, 'pleased': 3305, 'still': 4188, 'looking': 2648, 'hot': 2174, 'though': 4461, 'doing': 1312, 'making': 2718, 'these': 4441, 'anyway': 261, 'up': 4683, 'camera': 659, 'was': 4805, 'fun': 1848, 'premise': 3370, 'superhero': 4299, 'whose': 4877, 'giant': 1905, 'secret': 3849, 'weapon': 4828, 'did': 1241, 'pan': 3180, 'whole': 4873, 'length': 2569, 'jokes': 2415, 'level': 2584, 'your': 4991, 'average': 362, 'joe': 2407, 'brown': 601, 'comedy': 890, 'thing': 4450, 'basically': 407, 'found': 1811, 'fascinating': 1658, 'reasons': 3560, 'job': 2405, 'production': 3419, 'want': 4788, 'become': 433, 'director': 1264, 'michelle': 2830, 'brilliant': 583, 'hell': 2077, 'wasnt': 4807, 'top': 4525, '25': 40, 'under': 4639, 'beats': 426, 'definitely': 1162, 'deserved': 1200, 'grand': 1953, 'prize': 3405, 'seen': 3865, 'television': 4394, 'simply': 3986, 'fantastic': 1651, 'child': 789, 'songs': 4072, 'high': 2100, 'fashion': 1659, 'department': 1182, 'store': 4204, 'everything': 1544, 'notch': 3043, 'easy': 1410, 'understand': 4643, 'received': 3564, 'awards': 367, 'down': 1335, 'guest': 1992, 'appearances': 275, 'focus': 1766, 'entire': 1502, 'last': 2521, 'station': 4164, 'michael': 2829, 'melodrama': 2797, 'months': 2897, 'leo': 2571, 'begins': 444, 'sleep': 4024, 'christopher': 811, 'his': 2118, 'family': 1645, 'taking': 4352, 'writing': 4971, 'wife': 4884, 'movement': 2923, 'people': 3221, 'dedicated': 1153, 'ideas': 2215, 'sexual': 3908, 'property': 3438, 'forest': 1791, 'camp': 662, 'far': 1653, 'helen': 2075, 'wars': 4804, 'head': 2057, 'paul': 3214, 'claims': 830, 'efforts': 1433, 'convince': 983, 'sign': 3970, 'rights': 3697, 'over': 3148, 'russian': 3762, 'trying': 4601, 'steal': 4170, 'wealth': 4826, 'upon': 4684, 'husbands': 2205, 'death': 1141, 'james': 2378, 'naive': 2958, 'torn': 4528, 'between': 475, 'man': 2721, 'concern': 931, 'script': 3834, 'based': 404, 'novel': 3052, 'jay': 2386, 'quite': 3495, 'often': 3095, 'itself': 2371, 'confused': 941, 'territory': 4411, 'building': 615, 'complex': 923, 'motivations': 2915, 'ultimately': 4628, 'dont': 1324, 'satisfying': 3791, 'scope': 3821, 'story': 4207, 'should': 3951, 'beliefs': 452, 'ways': 4822, 'both': 547, 'those': 4460, 'unfortunately': 4654, 'un': 4630, 'constantly': 958, 'relationship': 3597, 'church': 813, 'faithful': 1634, 'example': 1553, 'section': 3853, 'priest': 3392, 'magnificent': 2705, 'hat': 2045, 'even': 1533, 'shows': 3960, 'never': 3001, 'beyond': 476, 'attempt': 340, 'providing': 3451, 'historical': 2119, 'details': 1219, 'enough': 1491, 'attention': 345, 'paid': 3169, 'most': 2909, 'result': 3660, 'feels': 1687, 'reason': 3557, 'vague': 4705, 'once': 3106, 'full': 1846, 'joy': 2422, 'panic': 3181, 'forced': 1787, 'expected': 1577, 'husband': 2204, 'gives': 1914, 'time': 4495, 'money': 2888, 'devoted': 1231, 'clearly': 840, 'own': 3158, 'theyre': 4445, 'performances': 3229, 'given': 1913, 'vast': 4716, 'importance': 2246, 'couples': 1017, 'place': 3282, 'history': 2121, 'scripts': 3836, 'ability': 65, 'support': 4303, 'alone': 194, 'talented': 4355, 'actors': 104, 'unable': 4631, 'same': 3778, 'seems': 3864, 'neither': 2996, 'revolutionary': 3681, 'nor': 3032, 'thief': 4448, 'either': 1438, 'rather': 3528, 'stand': 4143, 'anything': 260, 'getting': 1901, 'strength': 4219, 'else': 1448, '45': 50, '10': 0, 'went': 4847, 'screening': 3830, 'while': 4868, 'had': 2003, 'couple': 1016, 'laugh': 2529, 'very': 4723, 'major': 2711, 'flaws': 1752, 'presents': 3377, 'humorous': 2193, 'find': 1726, 'la': 2500, 'depressing': 1188, 'unfunny': 4655, 'directors': 1266, 'professional': 3421, 'wear': 4830, 'thin': 4449, 'yourself': 4993, 'suddenly': 4273, 'caring': 691, 'mans': 2730, 'progress': 3425, 'himself': 2111, 'extremely': 1610, 'annoying': 249, 'heavily': 2069, 'flawed': 1750, 'character': 758, 'us': 4690, 'concerned': 932, 'sappy': 3784, 'guy': 2000, 'rented': 3628, 'end': 1468, 'come': 886, 'possible': 3350, 'jenny': 2393, 'heck': 2071, 'within': 4912, '15': 7, 'we': 4823, 'learn': 2550, 'threatening': 4468, 'illness': 2230, 'couldnt': 1008, 'help': 2079, 'wonder': 4926, 'terrible': 4407, 'loss': 2659, 'poor': 3328, 'oliver': 3102, 'anyone': 259, 'watching': 4815, 'die': 1243, 'personality': 3242, 'probably': 3407, 'pushed': 3478, 'edge': 1418, 'eventually': 1537, 'divorce': 1301, 'court': 1020, 'worst': 4952, '70s': 56, 'believe': 454, 'imdb': 2240, 'absolutely': 72, 'originally': 3131, 'saw': 3798, 'remember': 3616, 'college': 875, 'telling': 4396, 'members': 2801, 'horrible': 2163, 'done': 1322, 'wanted': 4789, 'could': 1007, 'saying': 3800, 'sure': 4308, 'everyone': 1542, 'involved': 2346, 'prefer': 3368, 'roman': 3731, 'polanski': 3321, 'lay': 2542, 'apartment': 265, 'previous': 3387, 'committed': 905, 'suicide': 4283, 'becomes': 434, 'obsessed': 3069, 'led': 2558, 'point': 3316, 'hes': 2093, 'dressing': 1359, 'drag': 1342, 'events': 1536, 'might': 2835, 'unsettling': 4679, 'atmosphere': 333, 'creepy': 1052, 'comes': 891, 'baby': 376, 'better': 472, 'deal': 1135, 'eerie': 1427, 'moody': 2899, 'reading': 3544, 'reaction': 3541, 'feelings': 1686, 'felt': 1692, 'america': 212, 'myself': 2953, 'evidence': 1546, 'grade': 1951, 'fallen': 1638, 'little': 2624, 'particular': 3194, 'free': 1827, 'romance': 3732, 'beach': 419, 'nearly': 2981, 'cried': 1054, 'precious': 3366, 'evident': 1547, 'together': 4511, 'idea': 2213, 'taken': 4350, 'angles': 237, 'marriage': 2743, 'finding': 1727, 'least': 2554, 'conventional': 979, 'places': 3284, 'almost': 193, 'long': 2644, 'desperation': 1210, 'particularly': 3195, 'wonderful': 4928, 'role': 3726, 'ive': 2372, 'soft': 4051, 'spot': 4131, 'accent': 78, 'portrayal': 3341, 'shot': 3949, 'beautiful': 428, 'music': 2946, 'backdrop': 379, 'open': 3112, 'fields': 1705, 'acting': 99, 'emotions': 1461, 'magical': 2704, 'moment': 2886, 'innocence': 2293, 'childhood': 790, 'confusing': 942, 'marks': 2742, 'wonderfully': 4929, 'played': 3297, 'right': 3696, 'starts': 4159, 'elvis': 1451, 'playing': 3300, 'thinking': 4453, 'sister': 4001, 'soon': 4074, 'trouble': 4592, 'figured': 1712, 'although': 201, 'local': 2634, 'loser': 2655, 'whos': 4876, 'father': 1666, 'wishes': 4906, 'decent': 1147, 'boy': 557, 'feet': 1688, 'teenagers': 4391, 'herself': 2092, 'isnt': 2362, 'meets': 2795, 'foster': 1809, 'recently': 3567, 'died': 1244, 'moved': 2922, 'old': 3100, 'farm': 1656, 'younger': 4990, 'brothers': 599, 'responsibility': 3655, 'having': 2054, 'goes': 1930, 'jumps': 2432, 'only': 3110, 'argue': 290, 'leaves': 2556, 'each': 1395, 'visit': 4755, 'attracted': 349, 'attitude': 346, 'says': 3801, 'whats': 4860, 'mind': 2849, '17': 9, '14': 6, 'continue': 969, 'grow': 1982, 'closer': 856, 'days': 1130, 'working': 4944, 'tells': 4397, 'house': 2178, 'does': 1308, 'uncomfortable': 4637, 'sort': 4077, 'date': 1118, 'silence': 3974, 'broken': 594, 'meeting': 2794, 'look': 2646, 'looks': 2649, 'pain': 3170, 'defeat': 1157, 'face': 1616, 'painful': 3171, 'rest': 3657, 'tell': 4395, 'sisters': 4002, 'depiction': 1185, 'must': 2950, 'endure': 1475, 'world': 4946, 'sees': 3866, 'maybe': 2776, 'started': 4157, 'capture': 676, 'perfectly': 3226, 'watched': 4813, 'stuck': 4237, '100': 1, 'list': 2617, 'wish': 4904, 'faith': 1633, 'hollywood': 2137, 'lot': 2661, 'early': 1400, 'pilot': 3276, 'hit': 2122, 'canadian': 667, 'tv': 4610, 'show': 3953, 'trailer': 4554, 'park': 3190, 'showcase': 3954, 'decided': 1149, 'series': 3890, 'cast': 710, 'cult': 1077, 'small': 4033, 'criminals': 1058, 'peoples': 3222, 'dog': 1310, 'next': 3007, 'door': 1328, 'night': 3013, 'guys': 2001, 'across': 96, 'beginning': 443, 'highly': 2105, 'recommend': 3571, 'rated': 3527, 'violence': 4747, 'drug': 1373, 'use': 4692, 'offensive': 3086, 'killing': 2472, 'animals': 240, 'jennifer': 2392, 'intelligent': 2314, 'split': 4120, 'eating': 1413, 'begin': 442, 'schools': 3814, 'ball': 388, 'aware': 368, 'daughters': 1122, 'losing': 2658, 'weight': 4840, 'hospital': 2172, 'gain': 1863, 'visits': 4756, 'orders': 3127, 'allowed': 190, 'falling': 1639, 'party': 3200, 'car': 680, 'weak': 4824, 'kills': 2474, 'condition': 937, 'entertained': 1498, 'war': 4792, 'screenplay': 3831, 'politically': 3325, 'appeal': 271, 'public': 3460, 'glory': 1922, 'otherwise': 3137, 'waste': 4808, 'am': 204, 'interested': 2324, 'teach': 4374, 'worthless': 4954, 'qualities': 3483, 'bunch': 623, 'running': 3756, 'countryside': 1015, 'cliche': 843, 'lines': 2612, 'predictable': 3367, 'exact': 1550, 'force': 1786, 'united': 4664, 'states': 4163, 'imagine': 2238, 'europe': 1529, 'africa': 141, 'country': 1014, 'told': 4513, 'americans': 214, 'hates': 2048, 'please': 3304, 'piece': 3272, 'junk': 2436, 'wait': 4770, 'rent': 3626, 'loved': 2670, 'fred': 1825, 'johnson': 2411, 'proud': 3444, 'vulnerable': 4768, 'player': 3298, 'maggie': 2702, 'ship': 3934, 'bar': 396, 'anthony': 253, 'nose': 3037, 'loose': 2650, 'woman': 4921, 'took': 4523, 'ends': 1474, 'arthur': 305, 'whenever': 4863, 'scenes': 3811, 'sings': 3997, 'arms': 294, 'beautifully': 429, 'designed': 1203, 'design': 1202, 'set': 3898, 'lighting': 2596, 'sounds': 4084, 'game': 1864, 'simple': 3983, 'mission': 2870, 'used': 4693, 'voice': 4761, 'without': 4913, 'surprises': 4314, 'turn': 4605, 'another': 250, 'reaches': 3538, 'past': 3207, 'hour': 2176, 'picture': 3269, 'year': 4978, 'gem': 1877, 'polished': 3323, 'george': 1893, 'kim': 2475, 'traveling': 4568, 'miles': 2843, 'remote': 3622, 'cabin': 643, 'york': 4985, 'hits': 2125, 'mere': 2814, 'terrifying': 4410, 'reality': 3551, 'flesh': 1753, 'animal': 239, 'half': 2007, 'larry': 2520, 'thought': 4462, 'provoking': 3452, 'horror': 2168, 'characters': 760, 'developed': 1223, 'bone': 533, 'chilling': 794, 'handled': 2019, 'effectively': 1430, 'clear': 839, 'entirely': 1503, 'contact': 961, 'direction': 1261, 'nonsensical': 3031, 'ending': 1471, 'except': 1556, 'here': 2086, 'further': 1853, 'line': 2610, 'every': 1539, 'possibly': 3351, 'related': 3594, 'lead': 2545, 'written': 4972, 'points': 3319, 'mile': 2842, 'actions': 101, 'brother': 598, 'cop': 991, 'action': 100, 'wise': 4903, 'question': 3487, 'why': 4878, 'engaging': 1481, 'interviews': 2330, 'take': 4349, 'audio': 354, 'speaking': 4095, 'parts': 3199, 'entertaining': 1499, 'viewed': 4737, 'turned': 4606, 'cartoons': 705, 'add': 115, 'sitcom': 4004, 'regular': 3592, 'summer': 4291, 'seeing': 3857, 'episodes': 1508, 'again': 146, 'tried': 4584, 'chance': 747, 'kept': 2456, 'purpose': 3474, 'shown': 3959, 'clips': 852, 'cut': 1089, 'quick': 3490, 'dreadful': 1353, 'version': 4720, 'cartoon': 704, 'robot': 3717, 'now': 3054, 'struggle': 4234, 'came': 656, 'season': 3843, 'say': 3799, 'id': 2212, 'considering': 952, 'hours': 2177, 'aired': 165, 'guess': 1989, 'nobody': 3022, 'longer': 2645, 'thoroughly': 4459, 'false': 1641, 'vision': 4754, 'artist': 307, 'comfortable': 892, 'writer': 4968, 'suffered': 4276, 'directly': 1263, 'certainly': 738, 'personal': 3240, 'experience': 1580, 'ridiculous': 3693, 'ages': 152, 'behave': 445, 'anger': 235, 'fear': 1676, 'frustration': 1843, 'etc': 1527, 'live': 2625, 'carrying': 701, 'living': 2629, 'vacation': 4704, 'compare': 911, 'contemporary': 965, 'masterpieces': 2760, 'likes': 2602, 'ken': 2453, 'allow': 189, 'start': 4156, 'review': 3676, 'vampire': 4711, 'suck': 4269, 'intended': 2315, 'ill': 2228, 'vampires': 4712, 'cool': 989, 'van': 4713, 'considered': 951, 'pile': 3275, 'crap': 1031, 'enjoyable': 1485, 'fact': 1621, 'ask': 316, 'answer': 251, 'stars': 4155, 'professor': 3422, 'convincing': 985, 'actor': 103, 'wrote': 4974, 'wound': 4960, 'honestly': 2147, 'mean': 2779, 'perhaps': 3235, 'train': 4556, 'crash': 1033, 'manage': 2722, 'main': 2707, 'fault': 1668, 'boring': 543, 'damn': 1100, 'science': 3816, 'fiction': 1702, 'mom': 2885, 'students': 4239, 'illegal': 2229, 'disgusting': 1286, 'pull': 3461, 'gross': 1978, 'wouldve': 4959, 'comment': 898, 'brought': 600, 'ha': 2002, 'mildly': 2840, 'room': 3735, 'words': 4938, 'ninja': 3018, 'fight': 1707, 'filmmakers': 1720, 'expect': 1575, 'seriously': 3892, 'worth': 4953, 'stay': 4166, 'value': 4709, 'positive': 3346, 'slick': 4026, 'knife': 2487, 'minute': 2858, 'segment': 3867, 'tricks': 4583, 'sadly': 3769, 'pursuit': 3476, 'revenge': 3675, 'corrupt': 1001, 'soul': 4079, 'known': 2492, 'gangster': 1868, 'genre': 1888, 'modern': 2882, 'noir': 3023, 'theme': 4433, 'setting': 3900, 'pride': 3391, 'nice': 3008, 'enemy': 1477, 'giving': 1915, 'falls': 1640, 'flat': 1748, 'jason': 2383, 'fears': 1677, 'ray': 3533, 'apparently': 270, 'drives': 1367, 'crazy': 1035, 'blow': 514, 'final': 1722, 'scene': 3809, 'cannot': 670, 'presumably': 3380, 'got': 1944, 'ruthless': 3764, 'throughout': 4478, 'driven': 1365, 'nothing': 3047, 'smile': 4036, 'missing': 2869, 'ok': 3098, 'quest': 3486, 'youll': 4988, 'true': 4596, 'happiness': 2031, 'inner': 2292, 'peace': 3219, 'self': 3869, 'destruction': 1216, 'hardly': 2036, 'original': 3129, 'problem': 3408, 'skill': 4013, 'maker': 2714, 'carry': 700, 'appears': 278, 'bored': 541, 'red': 3576, 'shooting': 3943, 'comic': 893, 'questions': 3489, '2001': 31, 'try': 4600, 'accept': 80, 'answers': 252, 'viewer': 4738, 'tone': 4517, 'frankly': 1823, 'sit': 4003, 'pretentious': 3383, 'chinese': 796, 'mob': 2879, 'case': 707, 'years': 4979, 'held': 2074, 'decide': 1148, 'leader': 2546, 'simon': 3982, 'judge': 2424, 'wins': 4899, 'refuses': 3587, 'choice': 797, 'whatever': 4859, 'recognition': 3568, 'unlike': 4670, 'asian': 314, 'featuring': 1681, 'gangsters': 1869, 'bloody': 513, 'necessary': 2984, 'effective': 1429, 'keep': 2449, 'track': 4544, 'power': 3359, 'mentioned': 2811, 'changes': 751, 'hands': 2020, 'settle': 3902, 'appear': 273, '70': 55, 'mark': 2739, 'waiting': 4772, 'award': 366, 'winner': 4897, 'hong': 2149, 'kong': 2494, 'winning': 4898, 'johnny': 2410, 'heroic': 2090, 'cinematography': 820, 'editing': 1421, 'score': 3822, 'including': 2264, 'blood': 512, 'fetched': 1698, 'exciting': 1563, 'rambo': 3511, 'ii': 2226, 'emphasis': 1463, 'onto': 3111, 'book': 535, 'totally': 4532, 'logic': 2638, 'doubt': 1332, 'succeed': 4262, 'eg': 1434, 'invasion': 2340, 'usa': 4691, 'opportunities': 3120, 'excitement': 1562, 'hero': 2088, 'word': 4937, 'hundreds': 2196, 'soldiers': 4054, 'excited': 1561, 'john': 2409, 'offered': 3088, 'join': 2412, 'operation': 3117, 'east': 1407, 'american': 213, 'trapped': 4564, 'vietnam': 4735, 'colonel': 876, 'richard': 3686, 'government': 1948, 'outfit': 3144, 'charles': 764, 'merely': 2815, 'prison': 3401, 'jungle': 2434, 'check': 777, 'contains': 964, 'existence': 1571, 'lady': 2507, 'soldier': 4053, 'knowledge': 2491, 'co': 867, 'julia': 2427, 'discovers': 1281, 'political': 3324, 'military': 2844, 'captured': 677, 'tortured': 4530, 'following': 1774, 'escape': 1517, 'sets': 3899, 'jack': 2373, 'photography': 3260, 'jerry': 2396, 'sheer': 3925, 'stunt': 4245, 'performing': 3233, 'various': 4715, 'antics': 255, 'failure': 1629, 'reduced': 3579, 'rings': 3699, 'sequences': 3888, 'dialogue': 1234, 'absurd': 73, 'release': 3602, 'simplistic': 3985, 'near': 2979, 'hysterical': 2208, 'anti': 254, 'communist': 907, 'audiences': 353, 'fest': 1696, 'greatest': 1966, 'box': 555, 'office': 3091, 'success': 4265, 'thankfully': 4422, 'changed': 750, 'nowadays': 3055, 'minded': 2850, 'flick': 1754, 'count': 1010, 'dumb': 1385, 'politics': 3326, 'ludicrous': 2685, 'straight': 4210, 'sequel': 3885, 'jerk': 2395, 'titled': 4505, 'names': 2963, 'revolves': 3682, 'different': 1248, 'virtually': 4751, 'connection': 946, 'musical': 2947, 'credit': 1049, 'plenty': 3307, 'laughs': 2534, 'prime': 3395, 'remotely': 3623, 'scary': 3807, 'insult': 2309, 'vs': 4767, 'scream': 3826, 'trash': 4565, 'especially': 1520, 'stephen': 4176, 'kings': 2481, 'attempts': 343, 'monkeys': 2891, 'shining': 3933, 'books': 536, 'frightening': 1839, 'nail': 2957, 'black': 493, 'rare': 3523, 'neck': 2985, 'creates': 1038, 'tension': 4404, 'ghost': 1902, 'standing': 4146, 'somewhere': 4069, 'incredible': 2269, 'brings': 587, 'disturbing': 1300, 'close': 853, 'learned': 2551, 'lessons': 2578, 'masters': 2761, 'cat': 713, 'robert': 3713, 'haunting': 2051, 'directed': 1259, 'horrifying': 2167, 'zombie': 4997, 'rubbish': 3748, 'folks': 1771, 'lewis': 2586, 'allen': 188, 'henry': 2084, 'master': 2757, 'suspense': 4330, 'hitchcock': 2123, 'bring': 585, 'seat': 3845, 'slow': 4031, 'burning': 627, 'period': 3236, 'ten': 4399, 'features': 1680, 'relatively': 3600, 'intimate': 2331, 'general': 1880, 'empty': 1465, 'seeking': 3859, 'fill': 1714, 'enjoying': 1487, 'nature': 2975, 'overall': 3150, 'complexity': 924, 'simplicity': 3984, 'humanity': 2189, 'moving': 2928, 'allows': 192, 'reveal': 3670, 'meaning': 2780, 'mistakes': 2873, 'everyday': 1541, 'intense': 2316, 'however': 2183, 'enjoy': 1484, 'honest': 2146, 'meaningful': 2781, 'goofy': 1939, 'serious': 3891, 'said': 3774, 'hey': 2095, 'throw': 4479, 'oh': 3096, 'hate': 2046, 'weapons': 4829, 'pre': 3365, 'change': 749, 'future': 1856, 'cause': 723, 'already': 197, 'happened': 2027, 'therefore': 4439, 'concept': 930, 'watchable': 4812, 'youre': 4992, 'thrills': 4475, 'reference': 3582, 'sean': 3840, 'relations': 3596, 'age': 148, 'agree': 155, 'nicely': 3009, 'cold': 872, 'means': 2783, 'type': 4621, 'recent': 3566, 'eyes': 1613, 'wide': 4880, 'shut': 3961, 'briefly': 580, 'acted': 98, 'indeed': 2271, 'deserves': 1201, 'nomination': 3026, 'wasting': 4810, 'read': 3543, 'hundred': 2195, 'elsewhere': 1449, 'instead': 2307, 'absolute': 71, 'fictional': 1703, 'dangerous': 1110, 'sell': 3871, 'truth': 4599, 'stress': 4220, 'nowhere': 3056, 'credits': 1050, 'dr': 1340, 'nicholas': 3010, 'complete': 921, 'assume': 328, 'matter': 2769, 'third': 4455, 'countries': 1013, 'misery': 2865, 'suffering': 4277, 'attempted': 341, 'eye': 1611, 'toward': 4539, 'happen': 2026, 'everywhere': 1545, 'west': 4852, 'documentary': 1307, 'settings': 3901, 'facts': 1624, 'leading': 2547, 'image': 2233, 'hitting': 2126, 'screaming': 3827, 'willing': 4889, 'sitting': 4007, 'arguably': 289, 'shocking': 3940, 'viewing': 4740, 'kay': 2447, 'second': 3846, 'killed': 2469, 'discovered': 1280, 'legs': 2566, 'vice': 4727, 'health': 2060, 'low': 2676, 'opinion': 3118, 'our': 3139, 'intelligence': 2313, 'trust': 4598, 'conclusion': 936, 'ourselves': 3140, 'murdered': 2940, 'likable': 2598, 'portrayed': 3342, 'media': 2789, 'need': 2987, 'texas': 4418, 'massacre': 2754, 'inspired': 2301, 'imagery': 2234, 'explained': 1587, 'scottish': 3825, 'doctor': 1304, 'wing': 4896, 'sake': 3775, 'helping': 2081, 'african': 142, 'producers': 3416, 'unless': 4669, 'white': 4870, 'ironically': 2356, 'thriller': 4472, 'coming': 896, 'redeeming': 3577, 'cost': 1003, 'innocent': 2294, 'suggest': 4280, 'tragic': 4552, 'mrs': 2930, 'joke': 2414, 'entertainment': 1500, 'stop': 4201, 'realized': 3553, 'manipulative': 2727, 'ron': 3734, 'howard': 2182, 'sweet': 4336, 'adaptation': 113, 'darker': 1116, 'side': 3965, 'sad': 3767, 'selling': 3873, 'viewers': 4739, 'lazy': 2543, 'investigate': 2342, 'buy': 640, 'fake': 1635, 'hook': 2152, 'villain': 4744, 'holding': 2130, 'put': 3480, 'screen': 3829, 'let': 2579, 'audience': 352, 'urge': 4689, 'hollywoods': 2138, 'favor': 1670, 'enjoyed': 1486, 'goldberg': 1933, 'portrays': 3344, 'social': 4049, 'chain': 741, 'due': 1382, 'danny': 1112, 'glover': 1923, 'attorney': 348, 'moves': 2925, 'neighborhood': 2993, 'friendly': 1836, 'awkward': 374, 'during': 1387, 'mostly': 2910, 'strong': 4229, 'catch': 714, 'sorry': 4076, 'plain': 3286, 'pathetic': 3210, 'sucked': 4270, 'besides': 468, 'guide': 1993, 'crocodile': 1065, 'obviously': 3072, 'hire': 2116, 'expert': 1585, 'creature': 1045, 'excuse': 1564, 'fell': 1690, 'asleep': 320, 'peter': 3247, 'craig': 1030, 'harry': 2039, 'andrews': 229, 'alas': 172, 'utter': 4701, 'nonsense': 3030, 'overacting': 3149, 'twice': 4613, 'feeling': 1685, 'castle': 712, 'drink': 1361, 'games': 1865, 'mad': 2696, 'apart': 264, 'mine': 2853, 'feature': 1678, 'whatsoever': 4861, 'dreary': 1356, 'today': 4508, 'solely': 4056, 'underrated': 4642, 'actress': 105, 'admit': 122, 'heights': 2073, '40': 48, 'ago': 154, 'star': 4150, 'whether': 4866, 'stopped': 4202, 'helped': 2080, 'gave': 1875, 'children': 792, 'earlier': 1399, 'adults': 126, 'caught': 722, 'situation': 4008, 'victim': 4729, 'nazi': 2977, 'play': 3296, 'lucas': 2680, 'german': 1896, 'repeated': 3631, 'wwii': 4975, 'considerable': 950, 'germans': 1897, 'similar': 3978, 'california': 650, 'japanese': 2382, 'write': 4967, 'ones': 3108, 'signed': 3971, 'soviet': 4089, 'british': 589, 'air': 164, 'forces': 1788, 'cities': 823, 'released': 3603, 'title': 4504, 'germany': 1898, 'army': 295, 'song': 4071, 'nazis': 2978, 'engaged': 1480, 'surprised': 4313, 'fathers': 1667, 'sounded': 4082, 'description': 1197, 'follow': 1772, 'happening': 2028, 'filmmaker': 1719, 'individual': 2277, 'pieces': 3273, 'threw': 4470, 'order': 3126, 'filmed': 1717, 'korean': 2495, 'noticed': 3049, 'portion': 3338, 'convey': 982, 'mundane': 2938, 'persons': 3244, 'succeeded': 4263, 'needless': 2989, 'disney': 1289, 'hoping': 2160, 'mary': 2751, 'angela': 232, 'delightful': 1168, 'performer': 3231, 'julie': 2428, 'delivers': 1172, 'performance': 3228, 'david': 1124, 'tune': 4602, 'singer': 3993, 'gifted': 1907, 'notice': 3048, 'cameos': 658, 'lovely': 2671, 'sam': 3777, 'king': 2479, 'english': 1483, 'bruce': 602, 'roles': 3727, 'sea': 3838, 'believing': 457, 'number': 3060, 'road': 3709, 'stretched': 4222, 'clichÃ©': 844, 'attacked': 338, 'impressive': 2253, 'kids': 2467, 'home': 2142, 'guard': 1988, 'important': 2247, 'britain': 588, 'warm': 4794, 'tribute': 4581, 'animation': 242, 'happy': 2032, 'stage': 4139, 'chess': 783, 'attempting': 342, 'italy': 2368, 'brilliance': 582, 'obsession': 3070, 'aspects': 322, 'emily': 1456, 'resort': 3650, 'handsome': 2021, 'fascinated': 1657, 'met': 2822, 'walk': 4775, 'matches': 2764, 'causing': 726, 'problems': 3409, 'win': 4892, 'amazing': 209, 'fine': 1729, 'rich': 3685, 'perfection': 3225, 'shy': 3962, 'scenery': 3810, 'variety': 4714, 'costumes': 1006, 'gorgeous': 1942, 'yes': 4982, 'unusual': 4681, 'flashbacks': 1747, 'appreciate': 279, 'art': 304, 'motion': 2913, 'creation': 1040, 'miss': 2866, 'eddie': 1416, 'murphy': 2944, 'spends': 4107, 'lost': 2660, 'kidnapped': 2466, 'sexy': 3911, 'charlotte': 766, 'asks': 319, 'evil': 1548, 'corny': 997, 'despite': 1211, 'dark': 1115, 'golden': 1934, 'packed': 3166, 'compelling': 916, 'charming': 768, 'generations': 1884, 'lose': 2654, 'belief': 451, 'worthy': 4956, 'complicated': 925, 'slowly': 4032, 'revealing': 3672, 'detail': 1217, 'narrator': 2967, 'balance': 387, 'ensemble': 1492, 'fabulous': 1615, 'implausible': 2245, 'cross': 1066, 'cinderella': 815, 'alice': 182, 'unbelievably': 4635, 'eric': 1514, 'unique': 4662, 'youd': 4987, 'surprising': 4315, 'reveals': 3673, 'wonders': 4931, 'filled': 1715, 'brief': 579, 'stunning': 4244, 'likely': 2601, 'extraordinary': 1607, 'ian': 2209, 'hasnt': 2044, 'opening': 3114, 'multiple': 2936, 'possibilities': 3348, 'powerful': 3360, 'incredibly': 2270, '2005': 35, 'blair': 497, 'law': 2538, 'bobby': 525, 'cops': 993, 'beat': 423, 'pulp': 3465, 'gruesome': 1986, 'murder': 2939, 'mysterious': 2955, 'realizes': 3554, 'earl': 1398, 'police': 3322, 'detective': 1220, 'community': 908, 'serial': 3889, 'killer': 2470, 'sullivan': 4288, 'location': 2635, 'finds': 1728, 'trip': 4588, 'town': 4541, 'letter': 2581, 'personally': 3243, 're': 3535, 'horrific': 2166, 'punishment': 3467, 'unexpected': 4650, 'phone': 3255, 'call': 651, 'double': 1331, 'immensely': 2243, 'turns': 4609, 'murderer': 2941, 'rape': 3521, 'kill': 2468, 'struck': 4232, 'bargain': 400, 'psycho': 3457, 'executed': 1565, 'tough': 4537, 'chase': 769, 'arrive': 300, 'learns': 2553, 'psychotic': 3459, 'plans': 3292, 'daughter': 1121, 'trial': 4580, 'inevitably': 2283, 'brutal': 603, 'eaten': 1412, 'survive': 4321, 'happily': 2030, 'mystery': 2956, 'blacks': 494, 'racist': 3503, 'vicious': 4728, 'harris': 2038, 'lucky': 2683, 'actresses': 106, 'spend': 4105, 'topless': 4527, 'dancing': 1107, 'attractive': 351, 'kinda': 2477, 'liked': 2600, 'cute': 1090, 'stale': 4141, 'delivering': 1171, 'language': 2516, 'flicks': 1755, 'hammer': 2014, 'classics': 837, 'dracula': 1341, 'favourite': 1673, 'unintentionally': 4659, 'clothes': 860, 'okay': 3099, 'cable': 644, 'north': 3036, 'four': 1812, 'growing': 1983, 'headed': 2058, 'move': 2921, 'sally': 3776, 'annie': 247, 'drugs': 1374, 'alcohol': 176, 'beaten': 424, 'mature': 2773, 'weve': 4856, 'difference': 1246, 'accurate': 90, '1980': 20, 'realistic': 3550, 'attitudes': 347, 'radio': 3504, 'donna': 1323, 'harder': 2035, 'teen': 4388, 'relate': 3593, 'dated': 1119, 'captures': 678, 'scott': 3824, 'rating': 3529, 'teens': 4392, 'tim': 4494, 'familiar': 1643, 'batman': 413, 'hollow': 2135, 'remake': 3612, 'common': 906, 'speak': 4094, 'heston': 2094, 'cameo': 657, 'burton': 630, 'todays': 4509, 'mass': 2753, 'planet': 3289, 'apes': 267, 'product': 3418, '1960s': 15, 'struggling': 4236, 'civilization': 827, 'searching': 3842, 'civil': 826, 'era': 1513, 'groups': 1981, 'inferior': 2285, 'treated': 4573, 'equal': 1509, 'philosophical': 3253, 'difficult': 1249, 'society': 4050, 'justify': 2439, 'species': 4098, 'taylor': 4372, 'hunter': 2199, 'obvious': 3071, 'sixties': 4011, 'superficial': 4298, 'correct': 1000, 'focusing': 1769, 'form': 1798, 'substance': 4258, 'surprise': 4312, 'utterly': 4702, 'tied': 4489, 'single': 3996, 'technical': 4381, 'perspective': 3245, 'makeup': 2717, 'outstanding': 3147, 'dislike': 1288, 'thirty': 4456, 'monster': 2892, 'humans': 2190, 'band': 391, 'behind': 447, 'captain': 674, 'notably': 3042, 'carter': 703, 'presentation': 3375, 'formulaic': 1803, 'perfect': 3224, 'fast': 1661, 'food': 1778, 'mixed': 2877, 'miscast': 2862, 'requires': 3642, 'beneath': 464, 'laughable': 2530, 'tries': 4585, 'desperately': 1209, 'interpretation': 2328, 'sick': 3963, 'teenager': 4390, 'saving': 3797, 'total': 4531, 'ruin': 3750, 'physical': 3262, 'exists': 1573, 'disappointment': 1275, 'brain': 562, 'franchise': 1817, 'using': 4698, 'genius': 1887, 'talent': 4354, 'produce': 3413, 'french': 1830, 'video': 4733, 'tragedy': 4551, 'ground': 1979, 'jumping': 2431, 'buildings': 616, 'dust': 1388, 'covered': 1023, 'shop': 3945, 'inside': 2297, 'footage': 1782, 'capturing': 679, '11': 2, 'carried': 698, 'funeral': 1849, 'childrens': 793, 'fantasy': 1652, 'joan': 2404, 'dreams': 1355, 'baseball': 403, 'runs': 3757, 'mayor': 2777, '13': 4, 'plus': 3311, 'added': 116, 'lesser': 2576, 'effort': 1432, 'limits': 2607, 'seasons': 3844, 'challenge': 743, 'guarantee': 1987, 'finish': 1732, 'anderson': 227, 'cooper': 990, 'hilarious': 2106, 'locations': 2636, 'awesome': 371, 'mentally': 2809, 'challenging': 744, 'episode': 1507, 'players': 3299, 'desperate': 1208, 'equally': 1510, 'necessarily': 2983, 'figure': 1711, 'awful': 372, 'effect': 1428, 'standard': 4144, 'terribly': 4408, 'clichÃ©s': 846, 'gun': 1996, 'device': 1227, 'source': 4086, 'conflict': 939, 'resolution': 3649, 'puts': 3481, 'god': 1926, 'waves': 4819, 'beings': 449, 'someones': 4065, '90': 60, 'sound': 4081, 'tired': 4501, 'formula': 1802, 'clever': 841, 'worse': 4951, 'space': 4090, 'sub': 4252, 'mediocre': 2791, 'embarrassed': 1453, 'theyve': 4446, 'teacher': 4375, 'talked': 4359, 'torture': 4529, 'subject': 4253, 'study': 4242, 'hall': 2009, 'block': 508, 'uwe': 4703, 'boll': 529, 'improvement': 2256, 'competent': 917, 'name': 2960, 'lord': 2652, 'post': 3352, 'satire': 3788, 'seed': 3856, 'counter': 1011, 'zero': 4996, 'respect': 3652, 'reach': 3536, 'dull': 1384, 'savage': 3793, 'commentary': 899, 'creating': 1039, 'slasher': 4020, 'page': 3167, 'rob': 3710, 'zombies': 4998, 'successful': 4266, 'halloween': 2010, 'sits': 4006, 'row': 3744, 'execution': 1566, 'beast': 422, 'grave': 1962, 'seek': 3858, 'string': 4227, 'wholly': 4874, 'gratuitous': 1961, 'create': 1036, 'myers': 2952, 'max': 2774, 'leave': 2555, 'impression': 2252, 'pro': 3406, 'murders': 2943, 'victims': 4730, 'furthermore': 1854, 'message': 2819, 'run': 3755, 'contrary': 974, 'wake': 4774, 'acts': 107, 'newspaper': 3006, 'cardboard': 682, 'storm': 4206, 'sequence': 3887, 'drawn': 1351, 'conceived': 929, 'non': 3027, 'existent': 1572, 'gore': 1941, 'limited': 2606, 'drunken': 1376, 'cinematographer': 819, 'technique': 4383, 'stomach': 4196, 'turning': 4608, 'draw': 1349, 'narrative': 2966, 'intent': 2318, 'pure': 3471, 'super': 4295, 'natural': 2973, 'deep': 1154, 'thoughts': 4464, 'spiritual': 4116, 'discussion': 1284, 'namely': 2962, 'thomas': 4458, 'native': 2972, 'land': 2511, 'england': 1482, 'wicked': 4879, 'dude': 1381, 'supporting': 4304, 'recognized': 3570, 'lousy': 2667, 'evening': 1534, '16': 8, 'laughed': 2532, 'older': 3101, 'finished': 1733, 'avoid': 363, 'cats': 721, 'disc': 1278, 'training': 4558, 'among': 217, 'officer': 3092, 'jane': 2380, 'honor': 2150, 'angle': 236, 'interest': 2323, 'above': 68, 'carl': 692, 'cuba': 1075, 'jr': 2423, 'shines': 3932, 'development': 1225, 'constant': 957, 'state': 4160, 'billy': 482, 'sunday': 4293, 'de': 1131, 'niro': 3019, 'initial': 2290, 'instance': 2304, 'worked': 4941, 'mr': 2929, 'hal': 2006, 'short': 3946, 'pacing': 3163, 'occasion': 3073, 'dimensional': 1253, 'depth': 1190, 'rain': 3506, 'issues': 2365, 'factor': 1622, 'comedian': 887, 'adventures': 131, 'rocky': 3723, 'dramatic': 1348, 'roots': 3738, 'shouldnt': 3952, 'rock': 3720, 'pleasure': 3306, 'lately': 2524, 'legend': 2564, '2000': 30, 'five': 1743, 'minor': 2857, 'managed': 2723, 'step': 4175, 'starred': 4153, 'stories': 4205, 'areas': 287, 'memorable': 2802, 'according': 88, 'seemingly': 3863, 'messages': 2820, 'religious': 3608, 'homeless': 2143, 'someone': 4064, 'christian': 808, 'religion': 3607, 'bible': 477, 'unfortunate': 4653, 'useless': 4695, 'survivor': 4324, 'agent': 150, 'pink': 3277, 'patients': 3212, 'conversation': 980, 'distant': 1295, 'survival': 4320, 'agreed': 156, 'amateur': 206, 'closing': 859, 'visuals': 4759, 'comparison': 914, 'beauty': 430, 'million': 2847, 'von': 4765, 'breaking': 570, 'dancer': 1104, 'stupid': 4247, 'gone': 1935, 'crime': 1055, 'against': 147, 'cinema': 816, 'whilst': 4869, 'overrated': 3156, 'tiger': 4491, 'hidden': 2096, 'dragon': 1344, 'clean': 838, 'academy': 77, 'channels': 754, 'mtv': 2933, 'style': 4249, 'uses': 4697, 'tracks': 4545, 'awake': 365, 'pay': 3216, 'background': 380, 'colour': 880, 'jean': 2389, 'par': 3184, 'images': 2235, 'distracting': 1297, 'pick': 3265, 'round': 3742, 'blind': 506, 'warn': 4797, 'sides': 3967, 'nightmare': 3014, 'narration': 2965, 'tremendous': 4579, 'shame': 3918, 'abandoned': 62, 'filming': 1718, 'visual': 4757, 'cinematic': 818, 'mention': 2810, 'imaginative': 2237, 'crappy': 1032, 'battle': 414, 'comical': 894, 'horse': 2170, 'riding': 3695, 'roll': 3728, 'vehicle': 4718, 'key': 2458, 'witness': 4914, 'agents': 151, 'bother': 548, 'exception': 1557, 'amateurish': 207, 'directing': 1260, 'continuity': 972, 'dialog': 1232, 'tedious': 4387, 'gag': 1861, 'reel': 3581, 'amusing': 221, 'comments': 901, 'values': 4710, 'disappointing': 1274, 'aspect': 321, 'suit': 4284, 'ape': 266, 'per': 3223, 'mask': 2752, 'shelf': 3926, 'jobs': 2406, 'costume': 1005, 'cgi': 740, 'painfully': 3172, 'gerard': 1895, 'tender': 4401, 'brave': 567, 'superior': 4300, 'masterpiece': 2759, 'godzilla': 1929, 'ruined': 3751, 'pseudo': 3454, 'japan': 2381, 'fail': 1625, 'fights': 1710, 'battles': 415, 'holes': 2133, 'creators': 1044, 'threat': 4467, 'enemies': 1476, 'blond': 510, 'easily': 1406, 'destroyed': 1214, 'spectacular': 4101, 'till': 4493, 'grandfather': 1954, 'student': 4238, 'devil': 1228, 'gold': 1932, 'chooses': 800, 'reflection': 3585, 'mirror': 2861, 'crimes': 1056, 'technically': 4382, 'worthwhile': 4955, 'disbelief': 1277, 'consider': 949, 'medical': 2790, 'treatment': 4574, 'italian': 2367, 'equivalent': 1512, 'dies': 1245, 'leaving': 2557, 'loves': 2674, 'everybody': 1540, 'stole': 4194, 'shape': 3919, 'businessman': 634, 'stands': 4147, 'oil': 3097, 'subsequent': 4257, 'dead': 1132, 'raised': 3508, 'appearing': 277, 'magic': 2703, 'allowing': 191, 'grant': 1956, 'finale': 1723, 'fly': 1763, 'charm': 767, 'omen': 3104, 'bergman': 466, 'parallel': 3185, 'distinct': 1296, 'save': 3794, 'jewish': 2401, 'called': 652, 'wandering': 4785, 'taught': 4370, 'heard': 2062, 'priceless': 3390, 'moore': 2901, 'ex': 1549, 'willis': 4890, '1950s': 14, 'haunted': 2050, 'serving': 3897, 'hitler': 2124, 'ghosts': 1903, 'ass': 323, 'remembered': 3617, 'clues': 865, 'suspect': 4327, 'busy': 635, 'immediately': 2242, 'climax': 849, 'mild': 2839, 'accident': 84, 'seemed': 3862, 'tiny': 4500, 'culture': 1079, 'skin': 4015, 'daily': 1097, 'stuff': 4243, 'center': 732, 'hair': 2005, 'fully': 1847, 'gray': 1963, '18': 10, 'stayed': 4167, '20': 29, 'discuss': 1283, 'concerns': 934, 'expressed': 1601, 'light': 2595, 'survivors': 4325, 'dying': 1393, 'worry': 4950, 'tons': 4520, 'literature': 2623, 'forgotten': 1797, 'disagree': 1269, 'poignant': 3315, 'budget': 610, 'bore': 540, 'pushing': 3479, 'offer': 3087, 'blockbuster': 509, 'bears': 421, 'feed': 1683, 'combination': 883, 'approach': 282, 'normal': 3033, 'structure': 4233, 'barely': 399, 'fans': 1650, '1940s': 13, 'presence': 3373, 'midnight': 2834, 'charge': 761, 'event': 1535, 'outside': 3146, 'appearance': 274, 'spoilers': 4124, 'undoubtedly': 4648, 'legendary': 2565, 'spoken': 4126, 'rogers': 3725, 'alike': 185, 'ran': 3513, 'deadly': 1133, 'trained': 4557, 'tom': 4514, 'career': 686, 'skills': 4014, 'lifetime': 2594, 'focused': 1767, 'sharp': 3922, 'silver': 3977, 'miller': 2846, 'sent': 3880, 'rebel': 3561, 'planned': 3290, 'helicopter': 2076, 'mouse': 2919, 'ended': 1470, 'hurt': 2202, 'fifteen': 1706, 'fighting': 1709, 'bullets': 621, 'flying': 1764, 'thick': 4447, 'london': 2641, 'diamond': 1236, 'accomplished': 87, 'inspector': 2299, 'solid': 4057, 'famous': 1647, 'tale': 4353, 'method': 2824, 'author': 359, 'murderous': 2942, 'latest': 2526, 'nonetheless': 3029, 'build': 614, 'genuine': 1891, 'lonely': 2643, 'philip': 3252, 'splendid': 4119, 'quiet': 3492, 'typically': 4624, 'terrific': 4409, 'lee': 2559, 'semi': 3874, 'sympathetic': 4342, 'ann': 244, 'dawn': 1127, 'remarkably': 3614, 'spooky': 4128, 'adorable': 124, 'exceptional': 1558, 'projects': 3428, 'fourth': 1813, 'jon': 2416, 'causes': 725, 'wears': 4832, 'pitt': 3280, 'noted': 3045, 'pace': 3161, 'kudos': 2497, 'dress': 1357, 'recommended': 3572, 'fare': 1655, 'rip': 3700, '1970s': 17, '1980s': 21, 'atlantis': 332, 'idiotic': 2219, 'lack': 2502, 'charisma': 762, 'hang': 2022, 'classic': 835, 'plan': 3287, 'outer': 3143, 'explosions': 1597, 'group': 1980, 'alive': 186, 'martial': 2747, 'arts': 310, 'impact': 2244, 'keeps': 2451, 'jet': 2400, 'li': 2587, 'jackie': 2374, 'chan': 746, 'hung': 2197, 'large': 2517, 'safe': 3771, 'cuts': 1091, 'stays': 4169, 'paced': 3162, 'jamie': 2379, 'deliver': 1169, 'oscar': 3133, 'impressed': 2251, 'obnoxious': 3067, 'hold': 2129, 'swear': 4334, 'fall': 1637, 'voices': 4763, 'bob': 524, 'steven': 4182, 'loud': 2664, 'funnier': 1850, 'albert': 174, 'brooks': 596, 'interview': 2329, 'taxi': 4371, 'driver': 1366, 'rule': 3753, 'model': 2880, 'thrown': 4481, 'jail': 2376, 'wouldnt': 4958, 'phony': 3256, 'timing': 4498, 'comedic': 888, 'whereas': 4865, 'mike': 2838, 'bright': 581, 'spots': 4132, 'villains': 4745, 'fellow': 1691, 'green': 1971, 'heavy': 2070, 'proved': 3446, 'killers': 2471, 'direct': 1258, 'brian': 576, 'dig': 1250, 'won': 4925, 'seven': 3903, 'rise': 3702, 'versus': 4722, 'revealed': 3671, 'troubled': 4593, 'rival': 3706, 'dennis': 1179, 'hopper': 2161, 'fonda': 1777, 'realise': 3548, 'sold': 4052, 'indie': 2276, 'teeth': 4393, 'thus': 4486, 'record': 3573, 'tour': 4538, 'bus': 631, 'credibility': 1047, 'process': 3412, 'kicks': 2463, 'rocks': 3722, 'women': 4923, 'western': 4853, 'instant': 2305, '80s': 59, 'walked': 4776, 'bare': 398, 'erotic': 1515, 'nights': 3016, 'handful': 2017, 'nasty': 2968, 'cheesy': 781, 'naked': 2959, 'queen': 3485, 'laura': 2536, 'display': 1291, 'mainly': 2708, 'scientific': 3817, 'research': 3644, 'university': 4667, 'theory': 4437, 'detailed': 1218, 'scientist': 3818, 'reviewers': 3678, 'included': 2262, 'originality': 3130, 'connected': 945, 'jeff': 2390, 'reminded': 3619, 'giallo': 1904, 'phantom': 3251, 'protagonist': 3441, 'disease': 1285, 'scares': 3806, 'discover': 1279, 'loads': 2633, 'retarded': 3663, 'boot': 538, 'skip': 4016, 'cases': 708, 'blame': 499, 'circumstances': 822, 'writers': 4969, 'coffee': 870, 'generally': 1881, 'amounts': 220, 'holmes': 2139, 'bringing': 586, 'lovers': 2673, 'selfish': 3870, 'faults': 1669, 'cared': 685, 'unforgettable': 4652, 'emotionally': 1460, 'needed': 2988, 'florida': 1761, 'touching': 4536, 'none': 3028, 'possessed': 3347, 'pity': 3281, 'surprisingly': 4316, 'capable': 672, 'dealing': 1136, 'lie': 2590, 'spoil': 4121, 'promise': 3430, 'cowboy': 1025, 'starring': 4154, 'matt': 2768, 'degree': 1164, 'cox': 1026, 'crew': 1053, 'bothered': 549, 'intriguing': 2335, 'realize': 3552, 'business': 633, 'corporate': 998, 'innovative': 2295, 'finally': 1724, 'city': 825, 'donald': 1321, 'boss': 546, 'ego': 1435, 'bigger': 479, 'universe': 4666, 'questionable': 3488, 'decisions': 1152, 'fire': 1734, 'actual': 108, 'marketing': 2741, 'fired': 1735, 'amy': 222, 'nick': 3012, 'task': 4368, 'pamela': 3179, 'street': 4216, 'hated': 2047, 'disaster': 1276, 'afraid': 140, 'embarrassment': 1455, 'mistake': 2871, 'continues': 971, 'chose': 804, 'celebrity': 729, 'edition': 1422, 'rules': 3754, 'biggest': 480, 'unknown': 4668, 'gene': 1879, 'simmons': 3981, 'fool': 1779, 'shadows': 3913, 'morgan': 2905, 'uk': 4626, 'sir': 4000, 'alan': 171, 'normally': 3034, 'tend': 4400, 'yelling': 4980, 'behavior': 446, 'interaction': 2322, 'code': 869, 'amongst': 218, 'treats': 4575, 'themes': 4434, 'duty': 1390, 'guilt': 1994, 'recall': 3562, 'seventies': 3904, 'masterful': 2758, 'catching': 716, 'argument': 291, 'achieved': 93, 'note': 3044, 'gundam': 1997, 'anime': 243, 'purposes': 3475, 'apparent': 269, 'designs': 1204, 'oz': 3160, 'suits': 4287, 'shallow': 3917, 'hadnt': 2004, 'independent': 2272, 'attached': 336, 'shed': 3924, 'curse': 1086, 'kelly': 2452, 'menacing': 2807, 'overdone': 3152, 'lit': 2621, 'mood': 2898, 'drags': 1345, 'basic': 406, 'certain': 737, 'bugs': 613, 'believable': 453, 'tense': 4403, 'dollar': 1314, 'lame': 2510, 'hope': 2155, 'prevent': 3385, 'website': 4834, 'suggested': 4281, 'poorly': 3329, 'occasionally': 3075, 'throwing': 4480, 'toy': 4542, 'bomb': 531, 'nude': 3058, 'hardcore': 2034, 'porn': 3336, 'exploitation': 1592, 'supposed': 4306, 'subplot': 4255, 'badly': 383, 'tears': 4380, 'ed': 1415, 'wood': 4933, 'enthusiasm': 1501, 'hoped': 2156, 'understanding': 4645, 'praise': 3364, 'negative': 2991, 'insight': 2298, 'drinking': 1362, 'techniques': 4384, '1996': 26, 'wearing': 4831, 'president': 3378, 'hear': 2061, 'regarding': 3589, 'amazed': 208, 'impossible': 2249, 'photo': 3257, 'treasure': 4571, 'owner': 3159, 'letting': 2583, 'view': 4736, 'created': 1037, 'somehow': 4063, 'involvement': 2347, 'hints': 2113, 'random': 3514, 'forever': 1792, 'woods': 4935, 'closest': 857, 'knowing': 2490, 'thoughtful': 4463, 'gripping': 1976, 'waitress': 4773, 'numerous': 3062, 'versions': 4721, 'bette': 471, 'davis': 1126, 'cruel': 1070, 'account': 89, 'occurs': 3078, 'rose': 3739, 'crude': 1069, 'leslie': 2574, 'essentially': 1523, 'producer': 3415, 'plane': 3288, 'studio': 4240, 'parker': 3191, 'harvey': 2042, 'buffs': 612, 'driving': 1368, 'extras': 1608, 'families': 1644, 'moon': 2900, 'afford': 138, 'convoluted': 987, 'ladies': 2506, 'significant': 3972, 'ward': 4793, 'stinks': 4192, 'candy': 668, 'creator': 1043, 'havent': 2053, 'highlights': 2104, 'inventive': 2341, 'spite': 4117, 'bridge': 578, 'core': 995, 'reviews': 3679, 'dollars': 1315, 'dentist': 1180, 'demand': 1174, 'centered': 733, 'alex': 179, 'boyfriend': 558, 'believes': 456, 'gay': 1876, 'pregnant': 3369, 'lover': 2672, 'throws': 4482, 'tea': 4373, 'explanation': 1590, 'gender': 1878, 'nuts': 3064, 'aside': 315, 'titles': 4506, 'stock': 4193, 'psychological': 3458, 'sorts': 4078, 'con': 928, 'redemption': 3578, 'fame': 1642, 'jake': 2377, 'spent': 4108, 'card': 681, 'sum': 4289, 'ticket': 4487, 'pair': 3176, 'field': 1704, 'introduces': 2338, 'shark': 3921, 'faced': 1617, 'vincent': 4746, 'twisted': 4618, 'demons': 1178, 'gritty': 1977, 'represented': 3638, 'notion': 3050, 'overcome': 3151, 'crafted': 1029, 'everyones': 1543, '2006': 36, 'picked': 3266, 'status': 4165, 'broke': 593, 'share': 3920, 'slapstick': 4019, 'quit': 3494, 'funniest': 1851, 'forth': 1804, 'failing': 1627, 'stick': 4185, 'martin': 2748, 'animated': 241, 'failed': 1626, 'dick': 1239, 'rate': 3526, 'bank': 394, 'fortunately': 1805, 'humour': 2194, 'festival': 1697, 'reunion': 3669, 'matthau': 2771, 'waited': 4771, 'robbins': 3712, 'fbi': 1675, 'stewart': 4184, 'via': 4726, 'hamilton': 2012, 'bits': 490, 'nevertheless': 3002, 'asking': 318, 'site': 4005, 'web': 4833, 'youth': 4994, 'shall': 3916, 'steps': 4177, 'weeks': 4839, 'european': 1530, 'romantic': 3733, 'lacked': 2503, '1990s': 25, 'talk': 4358, 'dinner': 1254, 'matters': 2770, 'decade': 1144, 'jaws': 2385, 'andy': 230, 'adventure': 130, 'terrorists': 4414, 'destroy': 1213, 'scared': 3805, 'vhs': 4725, 'collection': 874, 'curious': 1083, 'doll': 1313, 'depression': 1189, 'thousand': 4465, 'colorful': 878, 'toys': 4543, 'birthday': 487, 'arrives': 302, 'spoiled': 4122, 'unaware': 4632, 'blue': 518, 'ride': 3692, 'cliff': 847, 'pit': 3278, 'encounter': 1466, 'greedy': 1969, 'blob': 507, 'journey': 2421, 'cry': 1073, 'creative': 1041, 'glad': 1916, 'official': 3094, 'river': 3707, 'bizarre': 492, 'strange': 4211, 'dolls': 1316, 'fox': 1814, 'frequently': 1832, 'wild': 4885, 'pet': 3246, 'lesson': 2577, 'adds': 119, 'fairly': 1631, 'jim': 2402, 'carrey': 696, 'news': 3005, 'reporter': 3636, 'girlfriend': 1910, 'extreme': 1609, 'freeman': 1829, 'identify': 2216, 'logical': 2639, 'chosen': 805, 'singing': 3995, 'brilliantly': 584, 'performed': 3230, 'tad': 4347, 'mini': 2854, 'clown': 862, 'touch': 4533, 'thugs': 4484, 'proceedings': 3410, 'ridiculously': 3694, 'accused': 91, 'speaks': 4096, 'universal': 4665, 'stereotypes': 4179, 'creativity': 1042, 'flow': 1762, 'experienced': 1581, 'opposed': 3122, 'wished': 4905, 'nations': 2971, 'robots': 3718, 'fitting': 1742, 'intrigued': 2334, 'safety': 3772, 'compared': 912, 'bands': 392, 'break': 569, 'covers': 1024, 'closely': 855, '2003': 33, 'shoot': 3942, 'raw': 3532, 'contain': 962, 'progresses': 3426, 'careers': 687, 'promised': 3431, 'revolution': 3680, 'souls': 4080, 'industry': 2280, 'contract': 973, 'became': 431, 'popular': 3333, 'album': 175, 'bound': 553, 'follows': 1775, 'descent': 1193, 'disappeared': 1271, 'project': 3427, 'promising': 3433, 'access': 83, 'ignored': 2225, 'kicked': 2461, 'provides': 3450, 'critic': 1061, 'fit': 1740, 'integrity': 2311, 'depicts': 1186, 'greater': 1965, 'mountain': 2917, 'fresh': 1833, 'stylish': 4251, 'crowd': 1067, 'drive': 1363, 'restaurant': 3658, 'notable': 3041, 'credible': 1048, 'separate': 3883, 'nicholson': 3011, 'touches': 4535, 'terry': 4415, 'dating': 1120, 'denzel': 1181, 'washington': 4806, 'spike': 4111, 'lees': 2560, 'literally': 2622, 'spy': 4135, 'cia': 814, 'bottom': 551, 'bitter': 491, 'walken': 4777, 'mexico': 2827, 'service': 3896, 'financial': 1725, 'mitchell': 2875, 'lisa': 2616, 'range': 3516, 'remarkable': 3613, 'bond': 532, 'directs': 1267, 'domino': 1319, 'icon': 2211, 'definite': 1161, 'dad': 1095, 'competition': 918, 'bear': 420, 'protect': 3443, 'possibility': 3349, 'brand': 565, 'responsible': 3656, 'justice': 2438, 'mexican': 2826, 'frustrated': 1842, 'kinds': 2478, 'bullet': 620, 'information': 2288, 'loses': 2657, 'finger': 1731, 'placed': 3283, 'meant': 2784, 'pulled': 3462, 'loaded': 2632, 'bonus': 534, 'include': 2261, 'documentaries': 1306, 'vegas': 4717, 'mgm': 2828, 'thanks': 4423, 'horrendous': 2162, 'grown': 1984, 'infamous': 2284, 'prepared': 3371, 'aged': 149, 'differences': 1247, 'window': 4894, 'underground': 4640, 'usually': 4700, 'kate': 2446, 'locked': 2637, 'chased': 770, 'intentions': 2321, 'unpleasant': 4674, 'veteran': 4724, 'campbell': 663, 'stated': 4161, 'hence': 2083, 'copy': 994, 'bland': 500, 'understated': 4646, 'delivery': 1173, 'persona': 3239, 'creatures': 1046, 'uninspired': 4657, 'casting': 711, '30': 42, 'alexander': 180, 'showed': 3956, 'soundtrack': 4085, 'listening': 2620, 'ratings': 3530, 'flop': 1760, 'lengthy': 2570, 'forgot': 1796, 'williams': 4888, 'lyrics': 2692, 'sing': 3992, 'advice': 133, 'productions': 3420, 'spider': 4109, 'ps': 3453, 'youve': 4995, 'levels': 2585, 'cousin': 1021, 'bought': 552, 'ultimate': 4627, 'library': 2589, 'cheap': 774, 'sky': 4017, 'beloved': 461, 'initially': 2291, 'critics': 1064, 'jonathan': 2417, 'travels': 4569, 'island': 2361, 'bumbling': 622, 'benefit': 465, 'amount': 219, 'exploration': 1593, 'breathtaking': 574, 'flight': 1757, 'endearing': 1469, 'sons': 4073, 'sinister': 3998, 'sci': 3815, 'fi': 1701, 'elements': 1443, 'consistently': 954, 'epic': 1506, 'der': 1191, 'anna': 245, 'ambitious': 211, 'occasional': 3074, 'extra': 1606, 'aforementioned': 139, 'newly': 3004, 'dub': 1378, 'silent': 3975, 'attacks': 339, 'climactic': 848, 'displays': 1293, 'odd': 3080, 'provide': 3448, 'chemistry': 782, 'lively': 2627, 'listen': 2619, 'speech': 4102, 'showdown': 3955, 'earth': 1403, 'spirit': 4113, 'remain': 3609, 'comparing': 913, 'anybody': 257, 'disneys': 1290, 'kung': 2498, 'fu': 1844, 'firstly': 1737, 'presented': 3376, 'pictures': 3270, 'blows': 517, 'matrix': 2767, 'sequels': 3886, 'convincingly': 986, 'demands': 1175, 'serves': 3895, 'proves': 3447, 'cruise': 1071, 'william': 4887, 'accidentally': 85, 'shoots': 3944, 'chaplin': 756, 'trite': 4589, 'contrived': 976, 'misses': 2868, 'empathy': 1462, 'guilty': 1995, 'conflicts': 940, 'outcome': 3142, 'relationships': 3598, 'successfully': 4267, 'dream': 1354, 'stood': 4199, 'charismatic': 763, 'spoiler': 4123, 'able': 66, 'controversial': 978, 'match': 2762, 'sudden': 4272, 'warned': 4798, 'ben': 463, 'stiller': 4189, 'rachel': 3500, 'cares': 690, 'unreal': 4676, 'uninteresting': 4660, 'leonard': 2572, 'hired': 2117, 'jeremy': 2394, 'roy': 3745, 'iii': 2227, 'porno': 3337, 'sleazy': 4023, 'margaret': 2735, 'raped': 3522, 'mann': 2728, 'usual': 4699, 'symbolism': 4341, 'partner': 3198, 'winters': 4901, 'dean': 1139, 'dan': 1102, 'steve': 4181, 'yellow': 4981, 'practically': 3362, 'curtis': 1087, 'hudson': 2184, 'indian': 2274, 'chief': 788, 'walter': 4784, 'wayne': 4821, 'repeat': 3630, 'unpredictable': 4675, 'unlikely': 4671, 'occur': 3076, 'achieve': 92, 'unnecessary': 4672, 'plots': 3310, 'elizabeth': 1445, 'lacks': 2505, 'potential': 3355, 'pie': 3271, 'drew': 1360, 'advise': 134, 'disappointed': 1273, 'staying': 4168, 'reminds': 3620, 'chicks': 787, 'conversations': 981, 'chick': 786, 'philosophy': 3254, 'steals': 4172, 'appropriate': 283, 'dropping': 1371, 'tonight': 4519, 'married': 2744, 'stones': 4198, 'proper': 3436, 'visually': 4758, 'occurred': 3077, 'provided': 3449, 'cameras': 660, 'lies': 2591, 'cleverly': 842, 'unfolds': 4651, 'reminiscent': 3621, 'ring': 3698, 'enjoys': 1489, 'scare': 3803, 'below': 462, '12': 3, 'blown': 516, 'theaters': 4428, 'develop': 1222, 'angry': 238, 'prince': 3396, 'latter': 2528, 'nominated': 3025, 'sleeping': 4025, 'pop': 3330, '40s': 49, 'robin': 3715, 'tight': 4492, 'hood': 2151, 'kick': 2460, 'rolled': 3729, 'price': 3389, 'dynamic': 1394, 'duo': 1386, 'hiding': 2099, 'wooden': 4934, 'size': 4012, 'ears': 1402, 'rabbit': 3498, 'exposed': 1598, 'six': 4010, 'knock': 2488, 'burns': 628, 'passing': 3204, 'guessed': 1990, 'seconds': 3848, 'exposure': 1599, 'lane': 2515, 'convinced': 984, 'adam': 111, 'wilson': 4891, 'fulci': 1845, 'resemblance': 3645, 'dubbing': 1380, 'birds': 485, 'homage': 2141, 'wrestling': 4966, 'hotel': 2175, 'mouth': 2920, 'combined': 885, 'womans': 4922, 'ripped': 3701, 'nine': 3017, 'terms': 4406, 'struggles': 4235, 'inappropriate': 2259, 'roger': 3724, 'types': 4622, 'contrast': 975, 'traditional': 4550, 'paint': 3173, 'nudity': 3059, 'majority': 2712, 'scale': 3802, 'regard': 3588, 'mainstream': 2709, 'comics': 895, 'neighbors': 2994, 'chris': 806, 'heroes': 2089, 'wrapped': 4964, '2002': 32, 'rotten': 3740, 'houses': 2180, 'sends': 3876, 'smith': 4038, 'countless': 1012, 'comedies': 889, 'insulting': 2310, 'gags': 1862, 'monkey': 2890, 'timeless': 4496, 'slightest': 4028, 'wasted': 4809, 'forgettable': 1794, '1999': 27, 'dare': 1113, 'demented': 1176, 'century': 736, 'jones': 2418, 'relief': 3605, 'buddy': 609, 'hugh': 2186, 'frame': 1815, 'baker': 385, 'balls': 390, 'laughing': 2533, 'anymore': 258, 'hill': 2108, 'pretending': 3382, 'expecting': 1578, 'illogical': 2231, 'react': 3540, 'system': 4345, 'crying': 1074, 'propaganda': 3435, 'horrid': 2165, 'ah': 158, 'chuck': 812, 'host': 2173, 'blowing': 515, 'tongue': 4518, 'cheek': 779, 'send': 3875, 'reasonably': 3559, 'appalling': 268, 'accents': 79, 'joseph': 2419, 'garbage': 1870, 'march': 2734, 'expectations': 1576, 'harsh': 2040, 'south': 4087, 'fairy': 1632, 'striking': 4226, 'hanging': 2023, 'elegant': 1441, 'mountains': 2918, 'granted': 1957, 'werent': 4849, 'suitable': 4285, 'warning': 4800, 'sentimental': 3882, 'response': 3654, 'rank': 3518, 'spirited': 4114, 'anyways': 262, 'messed': 2821, 'gotta': 1946, 'delight': 1167, 'danes': 1108, 'yeah': 4977, 'supposedly': 4307, 'sight': 3969, 'computer': 927, 'graphics': 1959, 'monsters': 2893, 'dances': 1106, 'victor': 4731, 'strangers': 4214, 'deserve': 1199, 'studios': 4241, 'kenneth': 2455, 'branagh': 564, 'hamlet': 2013, 'control': 977, 'glorious': 1921, 'shakespeare': 3915, 'produced': 3414, 'decision': 1151, 'emotion': 1458, 'derek': 1192, 'classical': 836, '60': 53, 'lemmon': 2567, 'adapted': 114, 'whale': 4857, 'achievement': 94, 'flaw': 1749, 'heads': 2059, 'scenario': 3808, 'heaven': 2068, 'dressed': 1358, 'sexually': 3910, 'nancy': 2964, 'caine': 647, 'norman': 3035, 'sophisticated': 4075, 'standards': 4145, 'content': 966, 'partly': 3197, 'extent': 1605, 'blake': 498, 'stone': 4197, 'carrie': 697, 'museum': 2945, 'gang': 1867, 'gordon': 1940, 'kid': 2464, 'thrillers': 4473, 'bite': 489, 'butt': 638, 'wrap': 4963, 'peters': 3248, 'calling': 653, 'albeit': 173, 'mental': 2808, 'nurse': 3063, 'manner': 2729, 'movements': 2924, 'mix': 2876, 'overlooked': 3154, 'westerns': 4854, 'missed': 2867, 'jeffrey': 2391, 'involves': 2348, 'worlds': 4947, 'travel': 4567, 'companion': 909, 'wizard': 4919, 'passes': 3203, 'restored': 3659, 'quirky': 3493, 'lust': 2689, 'rushed': 3760, 'patrick': 3213, 'adequate': 120, 'grinch': 1975, 'christmas': 810, 'education': 1424, 'cave': 727, 'couldve': 1009, 'involving': 2349, 'synopsis': 4344, 'edited': 1420, 'machine': 2693, 'develops': 1226, 'storyline': 4208, 'maria': 2736, '75': 57, 'disjointed': 1287, 'concerning': 933, 'command': 897, 'lou': 2663, 'opens': 3115, 'bill': 481, 'speed': 4103, 'gentle': 1890, 'rough': 3741, 'talents': 4356, 'ought': 3138, 'journalist': 2420, 'em': 1452, 'guts': 1999, 'unrealistic': 4677, 'guns': 1998, 'errors': 1516, 'suppose': 4305, 'drunk': 1375, 'marry': 2745, 'risk': 3704, 'butler': 637, 'winter': 4900, 'lived': 2626, 'imagined': 2239, 'brutally': 604, 'disappear': 1270, 'believed': 455, 'appreciated': 280, 'search': 3841, 'recorded': 3574, 'fancy': 1649, 'resulting': 3661, 'spin': 4112, 'desert': 1198, 'commercial': 902, '1933': 12, 'wind': 4893, 'advance': 127, 'wondering': 4930, 'rental': 3627, 'freak': 1824, 'chasing': 772, 'forward': 1808, 'da': 1094, 'properly': 3437, 'cameron': 661, 'mate': 2765, 'wondered': 4927, 'bottle': 550, 'summary': 4290, 'ireland': 2352, 'foot': 1781, 'daniel': 1111, 'darkness': 1117, 'poetry': 3314, 'warrior': 4802, 'monk': 2889, 'clumsy': 866, 'destiny': 1212, 'absence': 70, 'holy': 2140, 'powers': 3361, 'starting': 4158, 'clichÃ©d': 845, 'anywhere': 263, 'liners': 2611, 'composed': 926, 'cell': 730, 'results': 3662, 'berlin': 467, 'chair': 742, 'backgrounds': 381, 'minds': 2852, 'vengeance': 4719, 'forgive': 1795, 'punch': 3466, 'bacall': 377, 'lights': 2597, 'edie': 1419, 'understandable': 4644, 'board': 522, 'poster': 3353, 'eight': 1436, 'opinions': 3119, 'ties': 4490, 'ahead': 159, 'lovable': 2668, 'sadness': 3770, 'supernatural': 4302, 'sword': 4340, 'named': 2961, 'arm': 292, 'returns': 3668, 'encounters': 1467, 'mafia': 2700, 'instantly': 2306, 'bold': 528, 'colors': 879, 'refreshing': 3586, 'entry': 1504, 'twelve': 4611, 'cash': 709, 'australia': 356, 'urban': 4688, 'twists': 4619, 'elm': 1447, 'ignore': 2224, 'hole': 2132, 'print': 3399, 'suspenseful': 4331, 'australian': 357, 'neat': 2982, 'associated': 327, 'essence': 1521, 'strongly': 4231, 'shirley': 3936, 'dubbed': 1379, 'claimed': 829, 'born': 544, 'express': 1600, 'test': 4416, 'views': 4742, 'forms': 1801, 'watches': 4814, 'smoking': 4040, 'burned': 626, 'lion': 2614, 'mst3k': 2932, 'flashback': 1746, 'unbelievable': 4634, 'stilted': 4190, 'aids': 161, 'sucks': 4271, 'abc': 63, 'delivered': 1170, 'greatly': 1967, 'ashamed': 312, 'basketball': 409, 'cultural': 1078, 'mentioning': 2812, 'desired': 1206, 'pat': 3208, 'union': 4661, 'fails': 1628, 'area': 286, 'shock': 3938, 'sympathy': 4343, 'dear': 1140, 'inspiring': 2302, 'tales': 4357, 'sensitive': 3879, 'protagonists': 3442, 'issue': 2364, 'described': 1195, 'poverty': 3357, 'replaced': 3634, 'sunshine': 4294, 'addition': 118, 'parent': 3187, 'spoke': 4125, 'understood': 4647, 'recognize': 3569, 'thru': 4483, 'outrageous': 3145, 'engage': 1479, 'entertain': 1497, 'magazine': 2701, 'quotes': 3497, 'sloppy': 4030, 'relative': 3599, 'village': 4743, 'costs': 1004, 'samurai': 3779, 'extended': 1604, 'stretch': 4221, 'imagination': 2236, 'landing': 2512, 'lower': 2677, 'commented': 900, 'horses': 2171, 'rocket': 3721, 'fair': 1630, 'bet': 470, '50s': 52, '60s': 54, 'instinct': 2308, 'dropped': 1370, 'storytelling': 4209, 'numbers': 3061, 'primary': 3394, 'glass': 1917, 'criminal': 1057, 'spell': 4104, 'rage': 3505, 'ensues': 1493, 'mentions': 2813, 'manages': 2725, 'perform': 3227, 'wedding': 4836, 'higher': 2101, 'broadway': 592, 'routine': 3743, 'childish': 791, 'preview': 3386, 'pleasantly': 3303, 'charlie': 765, 'woody': 4936, 'bud': 607, 'atmospheric': 334, 'return': 3665, 'friday': 1834, '13th': 5, 'freddy': 1826, 'unhappy': 4656, 'cynical': 1093, 'rolling': 3730, 'hills': 2109, 'hopes': 2159, 'realizing': 3555, 'genres': 1889, '80': 58, 'introduced': 2337, 'mindless': 2851, 'itll': 2369, '1983': 22, 'hopefully': 2157, 'figures': 1713, 'opera': 3116, 'mario': 2738, 'fat': 1663, 'advertising': 132, 'copies': 992, 'jesus': 2399, 'christ': 807, 'greek': 1970, 'similarities': 3979, 'forty': 1807, 'letters': 2582, 'st': 4137, 'investigation': 2343, 'attack': 337, 'christians': 809, 'principal': 3398, 'sun': 4292, 'statement': 4162, 'lake': 2509, 'mel': 2796, 'passion': 3205, 'commit': 904, 'individuals': 2278, 'dignity': 1252, 'testament': 4417, 'goal': 1925, 'required': 3641, 'gods': 1928, 'doubts': 1333, 'scheme': 3812, 'spread': 4133, 'regardless': 3590, 'moral': 2902, 'topic': 4526, 'combat': 882, 'subjects': 4254, 'babe': 375, 'repetitive': 3633, 'shocked': 3939, 'unconvincing': 4638, 'mess': 2818, 'defined': 1160, 'menace': 2806, 'losers': 2656, 'paying': 3217, 'southern': 4088, 'pot': 3354, '1973': 19, 'handle': 2018, 'references': 3583, 'concert': 935, 'saturday': 3792, '3000': 43, 'offering': 3089, 'witch': 4909, 'severe': 3906, 'profound': 3423, 'smart': 4035, 'dialogs': 1233, 'experiences': 1582, 'carpenter': 694, 'rush': 3759, 'enjoyment': 1488, 'graphic': 1958, 'ad': 110, 'devoid': 1230, 'punk': 3468, 'checking': 778, 'dirty': 1268, 'san': 3780, 'francisco': 1819, 'santa': 3783, 'color': 877, 'expensive': 1579, 'cars': 702, 'directions': 1262, 'damon': 1101, 'solve': 4060, 'exactly': 1551, 'finest': 1730, 'endless': 1473, 'bo': 521, 'established': 1524, 'introduction': 2339, 'explain': 1586, 'exist': 1569, 'rendition': 3625, 'tracy': 4546, 'weird': 4841, 'beatty': 427, 'madonna': 2699, 'al': 170, 'pacino': 3164, 'canada': 666, 'memories': 2803, 'atrocious': 335, 'purely': 3472, 'bleak': 503, 'turkey': 4604, 'front': 1841, 'miracle': 2860, 'taste': 4369, 'horribly': 2164, 'incompetent': 2266, 'resources': 3651, 'travesty': 4570, 'cinemas': 817, 'celluloid': 731, 'jackson': 2375, 'parody': 3192, 'thank': 4421, 'boom': 537, 'energy': 1478, 'passed': 3202, 'choices': 798, 'prequel': 3372, 'determined': 1221, 'paper': 3183, 'boredom': 542, 'hint': 2112, 'historically': 2120, 'featured': 1679, 'wit': 4908, 'lily': 2605, 'picks': 3268, 'petty': 3249, 'walker': 4778, 'ideal': 2214, 'stereotypical': 4180, 'beer': 439, 'lynch': 2691, 'twin': 4615, 'foul': 1810, 'bucks': 606, 'curiosity': 1082, 'depicted': 1183, 'vivid': 4760, 'activities': 102, 'examples': 1554, 'bath': 411, 'transformation': 4560, 'rarely': 3524, 'hart': 2041, 'continued': 970, 'began': 441, 'orson': 3132, 'welles': 4844, 'tag': 4348, 'championship': 745, 'hardy': 2037, 'joey': 2408, 'contest': 967, 'jumped': 2430, 'position': 3345, 'grab': 1949, 'opened': 3113, 'kane': 2442, 'calls': 654, 'week': 4837, 'kennedy': 2454, 'stiff': 4187, 'jimmy': 2403, 'virginia': 4750, 'duke': 1383, 'resident': 3647, 'shelley': 3928, 'reynolds': 3684, 'mothers': 2912, 'mid': 2832, 'wannabe': 4787, 'piano': 3264, 'crack': 1027, 'timothy': 4499, 'irish': 2353, 'alternate': 200, 'fighter': 1708, 'beating': 425, 'fits': 1741, 'shower': 3957, 'built': 618, 'returning': 3667, 'portray': 3340, 'angel': 231, 'tape': 4365, 'rex': 3683, 'oddly': 3081, 'ellen': 1446, 'ice': 2210, 'musicals': 2948, 'tap': 4364, 'dancers': 1105, 'link': 2613, 'leg': 2562, 'royal': 3746, 'appealing': 272, 'mistaken': 2872, 'identity': 2217, 'heroine': 2091, 'glimpse': 1919, 'doomed': 1327, 'somebody': 4062, 'diane': 1238, 'merit': 2816, 'generic': 1885, 'stooges': 4200, 'joined': 2413, 'shorts': 3948, 'quote': 3496, 'laurel': 2537, 'slap': 4018, 'aid': 160, 'bat': 410, 'changing': 752, 'pops': 3332, 'towards': 4540, 'captivating': 675, 'surreal': 4317, 'pig': 3274, 'eat': 1411, 'hello': 2078, 'acid': 95, 'memory': 2804, 'angels': 234, 'molly': 2884, 'club': 863, 'emma': 1457, 'diana': 1237, 'returned': 3666, 'improved': 2255, 'drawing': 1350, 'superman': 4301, 'inspiration': 2300, 'domestic': 1318, 'idiots': 2220, 'troops': 4591, 'advanced': 128, 'wet': 4855, 'kidding': 2465, 'voight': 4764, 'buck': 605, 'target': 4366, 'hoffman': 2128, 'wreck': 4965, 'hippie': 2115, 'ultra': 4629, 'deals': 1137, 'nelson': 2997, 'morality': 2903, 'surface': 4310, 'intentionally': 2320, 'wanna': 4786, 'conservative': 948, 'idiot': 2218, 'godfather': 1927, 'handed': 2016, 'stolen': 4195, 'ie': 2221, 'offended': 3085, 'demon': 1177, 'sticks': 4186, 'ritter': 3705, 'miserably': 2864, 'cake': 648, 'dragged': 1343, '24': 39, 'twenty': 4612, 'burn': 625, 'hype': 2207, '2007': 37, 'metal': 2823, 'appeared': 276, 'hideous': 2098, 'gradually': 1952, 'importantly': 2248, 'hopeless': 2158, 'doors': 1329, 'cards': 683, 'downright': 1337, 'national': 2970, 'potentially': 3356, 'sentence': 3881, 'lol': 2640, 'specific': 4099, 'smaller': 4034, 'ashley': 313, 'daddy': 1096, 'ms': 2931, 'alright': 198, 'theyll': 4444, 'blah': 496, 'matthew': 2772, 'aunt': 355, 'mansion': 2731, 'painting': 3175, 'wall': 4781, 'walls': 4783, 'clothing': 861, 'served': 3894, 'china': 795, 'plastic': 3294, 'irrelevant': 2358, 'segments': 3868, 'spoof': 4127, 'grows': 1985, 'listed': 2618, 'connect': 944, 'snow': 4045, 'videos': 4734, 'cook': 988, 'lloyd': 2630, 'widmark': 4882, 'flash': 1745, 'shell': 3927, 'gas': 1874, 'hed': 2072, 'abysmal': 76, 'mail': 2706, 'elvira': 1450, 'mistress': 2874, 'adult': 125, 'sports': 4130, 'authentic': 358, 'category': 718, 'succeeds': 4264, 'spirits': 4115, 'manager': 2724, 'pitch': 3279, 'tooth': 4524, 'introduce': 2336, 'worn': 4948, 'saved': 3795, 'indians': 2275, 'translation': 4562, 'choreography': 802, 'hearing': 2063, 'grim': 1974, 'lawyer': 2541, 'insane': 2296, 'bin': 483, 'criticism': 1063, 'dvds': 1392, 'adding': 117, 'soccer': 4048, 'explore': 1594, 'multi': 2935, 'valley': 4707, 'increasingly': 2268, 'incoherent': 2265, 'motivation': 2914, 'trashy': 4566, 'carries': 699, 'alcoholic': 177, '20th': 38, 'relation': 3595, 'dinosaurs': 1256, 'regret': 3591, 'signs': 3973, 'styles': 4250, 'exaggerated': 1552, 'receives': 3565, 'greed': 1968, 'planning': 3291, 'odds': 3082, 'blunt': 520, 'experiments': 1584, 'dry': 1377, 'saves': 3796, 'previously': 3388, 'improve': 2254, 'psychic': 3456, 'fix': 1744, 'victoria': 4732, 'genuinely': 1892, 'market': 2740, 'mysteries': 2954, 'ignorant': 2223, 'goodness': 1938, 'accepted': 82, 'realism': 3549, 'proof': 3434, 'depressed': 1187, 'satan': 3787, 'confidence': 938, 'consists': 955, 'welcome': 4842, 'macy': 2695, 'hearts': 2066, 'sarah': 3785, 'sutherland': 4333, 'strikes': 4225, 'maintain': 2710, 'caliber': 649, 'grew': 1972, 'shortly': 3947, 'luke': 2687, 'reed': 3580, 'internet': 2327, 'treat': 4572, '1930s': 11, 'frank': 1821, 'closed': 854, 'trademark': 4548, 'overly': 3155, 'boat': 523, 'landscapes': 2514, 'naturally': 2974, 'ralph': 3510, 'bakshi': 386, 'nostalgia': 3038, 'slight': 4027, 'princess': 3397, 'danger': 1109, 'prove': 3445, 'suffice': 4279, 'hunting': 2201, 'tree': 4576, 'buff': 611, 'legal': 2563, 'portraying': 3343, 'acceptable': 81, 'kirk': 2482, 'incident': 2260, 'affected': 137, 'underlying': 4641, 'shirt': 3937, 'foreign': 1790, 'twist': 4617, 'clark': 832, 'loyal': 2679, 'mars': 2746, 'psychiatrist': 3455, 'zone': 4999, 'louis': 2665, 'awe': 370, 'factory': 1623, 'workers': 4943, 'depicting': 1184, 'machines': 2694, 'le': 2544, 'weakest': 4825, 'rat': 3525, 'rats': 3531, 'similarly': 3980, 'ugly': 4625, 'text': 4419, 'viewings': 4741, 'slaughter': 4021, 'heres': 2087, 'pages': 3168, 'favorites': 1672, 'gary': 1873, 'gotten': 1947, 'oscars': 3134, 'sidney': 3968, 'morris': 2908, 'coach': 868, 'stranger': 4213, 'subplots': 4256, 'warming': 4795, 'eve': 1532, 'critical': 1062, 'rubber': 3747, 'campy': 664, 'aka': 168, 'violent': 4748, 'orange': 3125, 'smooth': 4041, 'uncle': 4636, 'currently': 1085, '30s': 44, 'holiday': 2134, 'gift': 1906, 'breaks': 571, 'blank': 501, 'barry': 401, 'carradine': 695, 'walking': 4779, 'linda': 2609, 'reaching': 3539, 'secondly': 3847, 'football': 1783, 'choreographed': 801, 'caused': 724, 'irritating': 2359, 'mickey': 2831, 'ham': 2011, 'unwatchable': 4682, 'international': 2326, 'buried': 624, 'intention': 2319, 'amazingly': 210, 'decades': 1145, 'prostitute': 3440, 'splatter': 4118, 'opportunity': 3121, 'spanish': 4092, 'intrigue': 2333, 'swedish': 4335, 'pays': 3218, 'nuclear': 3057, 'arrived': 301, 'practice': 3363, 'smoke': 4039, 'thousands': 4466, 'hide': 2097, 'bang': 393, 'officers': 3093, 'reached': 3537, 'builds': 617, 'glenn': 1918, 'steel': 4173, 'liking': 2604, 'minimum': 2856, 'earned': 1401, 'raise': 3507, 'voiced': 4762, 'landscape': 2513, 'artists': 309, 'catchy': 717, 'truck': 4595, 'faster': 1662, 'intellectual': 2312, 'eyed': 1612, 'neighbor': 2992, '1984': 23, 'hanks': 2025, 'honesty': 2148, 'mill': 2845, 'vote': 4766, 'popcorn': 3331, 'cutting': 1092, 'strip': 4228, 'environment': 1505, 'et': 1526, 'tends': 4402, 'jesse': 2397, 'reputation': 3640, 'cole': 873, 'pulls': 3464, 'scores': 3823, 'twilight': 4614, 'wow': 4962, 'attraction': 350, 'temple': 4398, 'thumbs': 4485, '1st': 28, 'september': 3884, 'lone': 2642, 'broadcast': 591, 'switch': 4339, 'network': 3000, 'expression': 1602, 'defense': 1159, 'report': 3635, 'assault': 324, 'suspicious': 4332, 'wed': 4835, 'table': 4346, 'careful': 688, 'rooms': 3736, 'base': 402, 'bbc': 417, 'pearl': 3220, 'marty': 2749, 'wives': 4918, 'explains': 1589, 'explicit': 1591, 'blatant': 502, 'rochester': 3719, 'eyre': 1614, 'grace': 1950, 'dalton': 1098, 'performers': 3232, 'complaint': 920, 'racism': 3502, '90s': 61, 'bush': 632, 'reviewer': 3677, 'lying': 2690, 'private': 3404, 'rap': 3520, 'tear': 4379, 'impress': 2250, 'displayed': 1292, 'lowest': 2678, 'gonna': 1936, 'whoever': 4872, 'mixture': 2878, 'theatrical': 4430, 'sink': 3999, 'paranoia': 3186, 'authority': 360, 'los': 2653, 'angeles': 233, 'models': 2881, 'edward': 1426, 'fooled': 1780, 'stanley': 4148, 'kubrick': 2496, 'avoided': 364, 'werewolf': 4850, 'yesterday': 4983, 'tradition': 4549, 'nervous': 2999, 'kapoor': 2443, 'consistent': 953, '35': 45, 'borrowed': 545, 'flawless': 1751, 'dialogues': 1235, 'stunts': 4246, 'talks': 4361, 'singers': 3994, 'eastern': 1408, 'subtitles': 4259, 'noble': 3021, 'defend': 1158, 'makers': 2715, 'superbly': 4297, 'dramas': 1347, 'ups': 4686, 'sadistic': 3768, 'spending': 4106, 'afternoon': 144, 'helps': 2082, 'repeatedly': 3632, 'amitabh': 216, 'theatre': 4429, 'thrilling': 4474, 'pulling': 3463, 'huh': 2187, 'cube': 1076, 'cage': 645, 'kicking': 2462, 'painted': 3174, 'weekend': 4838, 'estate': 1525, 'ancient': 225, 'generation': 1883, 'pack': 3165, 'enter': 1494, 'creep': 1051, 'remind': 3618, 'lifestyle': 2593, 'virgin': 4749, 'cheating': 776, 'feminist': 1695, 'folk': 1770, 'novels': 3053, 'reactions': 3542, 'miserable': 2863, 'deaths': 1142, 'explosion': 1596, 'influence': 2286, 'visible': 4753, 'sounding': 4083, 'flynn': 1765, 'breath': 573, 'sidekick': 3966, 'measure': 2786, 'citizen': 824, 'prior': 3400, 'turner': 4607, 'boxing': 556, 'channel': 753, 'explaining': 1588, 'deeply': 1156, 'elaborate': 1439, 'crucial': 1068, 'fay': 1674, 'dutch': 1389, 'portrait': 3339, 'hilariously': 2107, 'secrets': 3852, 'chases': 771, 'cup': 1080, 'wolf': 4920, 'program': 3424, 'abusive': 75, 'current': 1084, 'suggests': 4282, 'border': 539, 'clip': 851, 'purchase': 3469, 'alien': 183, 'catherine': 719, 'june': 2433, 'del': 1165, 'gothic': 1945, 'edgar': 1417, 'barbara': 397, 'enters': 1496, 'purchased': 3470, 'photos': 3261, 'strictly': 4223, 'pg': 3250, 'tame': 4363, 'scooby': 3820, 'doo': 1325, 'sandra': 3782, 'motives': 2916, 'amanda': 205, 'meaningless': 2782, 'security': 3854, 'soap': 4047, 'gory': 1943, '50': 51, 'don': 1320, 'hip': 2114, 'elephant': 1444, 'household': 2179, 'staff': 4138, 'nearby': 2980, 'carefully': 689, 'devils': 1229, 'stinker': 4191, 'tie': 4488, 'drop': 1369, 'frankenstein': 1822, 'arrival': 299, 'armed': 293, 'douglas': 1334, 'kissing': 2484, 'cheese': 780, 'remarks': 3615, 'fed': 1682, 'characterization': 759, 'transfer': 4559, 'vaguely': 4706, 'montage': 2894, 'abuse': 74, 'technology': 4385, 'terror': 4412, 'widely': 4881, 'lena': 2568, 'breasts': 572, 'deliberately': 1166, 'patient': 3211, 'loving': 2675, 'inevitable': 2282, 'horrors': 2169, 'chances': 748, 'medium': 2792, 'complain': 919, 'stealing': 4171, 'cannibal': 669, 'banned': 395, 'franco': 1820, 'compelled': 915, 'sinatra': 3989, 'betty': 474, 'smiling': 4037, 'tunes': 4603, 'warner': 4799, 'ordinary': 3128, 'laughter': 2535, 'abilities': 64, 'generated': 1882, '2nd': 41, '3rd': 47, 'annoyed': 248, 'learning': 2552, 'race': 3499, 'tiresome': 4502, 'felix': 1689, 'throat': 4476, 'load': 2631, 'ginger': 1908, 'solo': 4058, 'installment': 2303, 'arnold': 296, 'andrew': 228, 'stevens': 4183, 'mall': 2720, 'reasonable': 3558, 'choose': 799, 'clue': 864, 'theyd': 4443, 'rid': 3690, 'triumph': 4590, 'widow': 4883, 'ryan': 3765, 'commercials': 903, 'suspects': 4328, 'agrees': 157, 'relies': 3606, 'editor': 1423, 'deaf': 1134, 'tommy': 4516, 'reads': 3545, 'iraq': 2351, 'ho': 2127, 'surrounded': 4318, 'terrorist': 4413, 'bride': 577, 'month': 2896, 'bourne': 554, 'witches': 4910, 'enormous': 1490, 'downhill': 1336, 'guessing': 1991, 'alfred': 181, 'unintentional': 4658, 'fashioned': 1660, 'endings': 1472, 'remaining': 3610, 'brenda': 575, 'ramones': 3512, 'stupidity': 4248, 'fever': 1699, 'facing': 1620, 'dire': 1257, 'basis': 408, 'catches': 715, 'virus': 4752, 'cringe': 1059, 'serve': 3893, 'stereotype': 4178, 'ruby': 3749, 'sincere': 3991, 'iron': 2354, 'embarrassing': 1454, 'cg': 739, 'blade': 495, 'suspend': 4329, 'shake': 3914, 'capital': 673, 'wealthy': 4827, 'nightmares': 3015, 'masses': 2755, 'global': 1920, 'existed': 1570, 'blend': 504, 'warriors': 4803, 'writes': 4970, 'elderly': 1440, 'senseless': 3878, 'trick': 4582, 'physically': 3263, 'specifically': 4100, 'robinson': 3716, 'gandhi': 1866, 'halfway': 2008, 'destroying': 1215, 'rising': 3703, 'highest': 2102, 'homer': 2144, 'afterwards': 145, 'experiment': 1583, 'keaton': 2448, 'farce': 1654, 'dorothy': 1330, 'titanic': 4503, 'fisher': 1739, 'classes': 834, 'generous': 1886, 'hatred': 2049, 'manhattan': 2726, 'basement': 405, 'doom': 1326, 'floating': 1758, 'fatal': 1664, 'corpse': 999, 'streets': 4217, 'sole': 4055, 'corruption': 1002, 'solution': 4059, 'mummy': 2937, 'americas': 215, 'burt': 629, 'gabriel': 1859, 'fury': 1855, 'assistant': 326, 'doc': 1303, 'population': 3335, 'aint': 163, 'escapes': 1519, 'abraham': 69, 'staged': 4140, 'wounded': 4961, 'bag': 384, 'passionate': 3206, 'eccentric': 1414, 'hurts': 2203, 'facial': 1619, 'lesbian': 2573, 'randomly': 3515, 'judy': 2426, 'streisand': 4218, 'artistic': 308, 'racial': 3501, 'ya': 4976, 'chapter': 757, 'secretary': 3850, 'killings': 2473, '1972': 18, 'relatives': 3601, 'morning': 2906, 'kevin': 2457, 'ironic': 2355, 'belong': 459, 'laws': 2540, 'muslim': 2949, 'expressions': 1603, 'womens': 4924, 'conspiracy': 956, 'raising': 3509, 'awfully': 373, 'brazil': 568, 'removed': 3624, 'consequences': 947, 'ethan': 1528, 'cary': 706, 'lasted': 2522, 'inducing': 2279, 'represents': 3639, 'loosely': 2651, '1968': 16, 'dave': 1123, 'stargate': 4151, 'confusion': 943, 'receive': 3563, 'coherent': 871, 'dealt': 1138, 'worried': 4949, 'ear': 1397, 'pants': 3182, 'dogs': 1311, 'upset': 4687, 'raymond': 3534, 'scientists': 3819, 'aliens': 184, 'escaped': 1518, 'dozens': 1339, 'perry': 3237, 'luck': 2681, 'admittedly': 123, 'shadow': 3912, 'merits': 2817, 'robbery': 3711, 'riveting': 3708, 'plausible': 3295, 'largely': 2518, 'miike': 2837, 'influenced': 2287, 'satisfy': 3790, 'sat': 3786, 'inane': 2258, 'intensity': 2317, 'stellar': 4174, 'susan': 4326, 'worker': 4942, 'buddies': 608, 'marvelous': 2750, 'isolated': 2363, 'waters': 4817, 'homosexual': 2145, 'catholic': 720, 'trade': 4547, 'dinosaur': 1255, 'overlook': 3153, 'warmth': 4796, 'hbo': 2055, 'dawson': 1128, 'noise': 3024, 'disturbed': 1299, 'airport': 167, 'humble': 2191, 'wes': 4851, 'craven': 1034, 'proceeds': 3411, 'airplane': 166, 'toilet': 4512, 'minimal': 2855, 'unoriginal': 4673, 'larger': 2519, 'artificial': 306, 'analysis': 224, 'unbearable': 4633, 'wishing': 4907, 'powell': 3358, 'hyde': 2206, 'seagal': 3839, 'surviving': 4323, 'astaire': 330, 'digital': 1251, 'crisis': 1060, 'ships': 3935, 'liberal': 2588, 'term': 4405, 'resembles': 3646, 'strangely': 4212, 'pointed': 3317, 'craft': 1028, 'inept': 2281, 'pokemon': 3320, 'ruins': 3752, 'spain': 4091, 'heat': 2067, 'stan': 4142, 'definition': 1163, 'claim': 828, 'sacrifice': 3766, 'chicago': 785, 'shine': 3931, 'corner': 996, 'league': 2549, 'futuristic': 1857, 'respected': 3653, 'affect': 136, 'drivel': 1364, 'wacky': 4769, 'wave': 4818, 'bathroom': 412, 'constructed': 959, 'trilogy': 4586, 'juvenile': 2441, 'daring': 1114, 'fond': 1776, 'uneven': 4649, 'freedom': 1828, 'relevant': 3604, 'snl': 4044, 'sin': 3988, 'aging': 153, 'assigned': 325, 'claire': 831, 'distance': 1294, 'lab': 2501, 'assumed': 329, 'mildred': 2841, 'imitation': 2241, 'montana': 2895, 'path': 3209, 'matched': 2763, 'deeper': 1155, 'arrested': 298, 'cusack': 1088, 'trail': 4553, 'brooklyn': 595, 'explored': 1595, 'grasp': 1960, 'drops': 1372, 'screenwriter': 3833, 'bell': 458, 'clint': 850, 'witnessed': 4915, 'brad': 560, 'poetic': 3313, 'teaching': 4377, 'satisfied': 3789, 'melting': 2799, 'surfing': 4311, 'forbidden': 1785, 'kingdom': 2480, 'nyc': 3065, 'scripted': 3835, 'primarily': 3393, 'broad': 590, 'represent': 3637, 'construction': 960, 'upper': 4685, 'screams': 3828, 'prisoner': 3402, 'excessive': 1560, 'ranks': 3519, 'sport': 4129, 'involve': 2345, 'map': 2733, 'deceased': 1146, 'combine': 884, 'centers': 734, 'wells': 4845, 'karen': 2444, 'wisdom': 4902, 'valuable': 4708, 'format': 1799, 'kurt': 2499, 'meat': 2787, 'photographed': 3258, 'trees': 4577, 'lawrence': 2539, 'alongside': 196, 'roberts': 3714, 'hank': 2024, 'spring': 4134, 'despair': 1207, 'todd': 4510, 'richardson': 3688, 'nation': 2969, 'jazz': 2387, 'rick': 3689, 'stronger': 4230, 'alert': 178, 'melodramatic': 2798, 'survived': 4322, 'scrooge': 3837, 'dickens': 1240, 'ned': 2986, 'brosnan': 597, 'button': 639, 'obscure': 3068, 'jessica': 2398, 'witnesses': 4916, 'ingredients': 2289, 'winds': 4895, 'easier': 1405, 'calm': 655, 'eager': 1396, 'online': 3109, 'parties': 3196, 'unit': 4663, 'sheriff': 3929, 'chest': 784, 'bedroom': 437, 'debut': 1143, 'flies': 1756, 'grey': 1973, 'sandler': 3781, 'trek': 4578, 'russell': 3761, 'massive': 2756, 'eva': 1531, 'executive': 1567, '1990': 24, 'prisoners': 3403, 'ted': 4386, 'jaw': 2384, 'thrill': 4471, 'press': 3379, 'millions': 2848, 'plague': 3285, 'garden': 1872, 'appreciation': 281, 'performs': 3234, 'hooked': 2153, 'seeks': 3860, 'notorious': 3051, 'nostalgic': 3039, 'davies': 1125, 'bollywood': 530, 'paltrow': 3178, 'eastwood': 1409, 'closet': 858, 'ruth': 3763, 'grandmother': 1955, 'fx': 1858, 'photographer': 3259, 'snake': 4042, 'kitchen': 2485, 'square': 4136, 'familys': 1646, 'altogether': 202, 'strike': 4224, 'staring': 4152, 'india': 2273, 'sneak': 4043, 'holly': 2136, 'brando': 566, 'purple': 3473, 'birth': 486, 'poem': 3312, 'popularity': 3334, 'arrogant': 303, 'chaos': 755, 'tomatoes': 4515, 'accompanied': 86, 'doctors': 1305, 'navy': 2976, 'contained': 963, 'junior': 2435, 'lucy': 2684, 'overwhelming': 3157, 'fish': 1738, 'reflect': 3584, 'mighty': 2836, 'plant': 3293, '3d': 46, 'scarecrow': 3804, 'tarzan': 4367, 'likewise': 2603, 'sellers': 3872, 'push': 3477, 'damage': 1099, 'props': 3439, 'tall': 4362, 'exotic': 1574, 'iran': 2350, 'olivier': 3103, 'cd': 728, 'incomprehensible': 2267, 'lips': 2615, 'adams': 112, 'carol': 693, 'swim': 4337, 'draws': 1352, 'eighties': 1437, 'picking': 3267, 'disappoint': 1272, 'fortune': 1806, 'cure': 1081, 'wendy': 4846, 'teachers': 4376, 'transition': 4561, 'saga': 3773, 'brains': 563, 'chorus': 803, 'secretly': 3851, 'sid': 3964, 'neo': 2998, 'ford': 1789, 'bettie': 473, 'subtlety': 4261, 'frequent': 1831, 'producing': 3417, 'twins': 4616, 'developing': 1224, 'describes': 1196, 'exercise': 1568, 'dolph': 1317, 'user': 4696, 'notes': 3046, 'surrounding': 4319, 'brady': 561, 'garbo': 1871, 'methods': 2825, 'hunt': 2198, 'akshay': 169, 'judging': 2425, 'hunters': 2200, 'francis': 1818, 'invisible': 2344, 'karloff': 2445, 'bela': 450, 'lugosi': 2686, 'latin': 2527, 'troubles': 4594, 'moronic': 2907, 'rural': 3758, 'circle': 821, 'lumet': 2688, 'ocean': 3079, 'recording': 3575, 'shoes': 3941, 'females': 1694, 'spare': 4093, 'exceptionally': 1559, 'directorial': 1265, 'retired': 3664, 'stanwyck': 4149, 'ridden': 3691, 'georges': 1894, 'spielberg': 4110, 'lincoln': 2608, 'april': 284, 'pretend': 3381, 'neil': 2995, 'discovery': 1282, 'empire': 1464, 'attend': 344, 'educational': 1425, 'cheated': 775, 'laughably': 2531, 'richards': 3687, 'wore': 4939, 'justin': 2440, 'aimed': 162, 'ease': 1404, 'useful': 4694, 'blew': 505, 'equipment': 1511, 'enterprise': 1495, 'cagney': 646, 'hop': 2154, 'bull': 619, 'columbo': 881, 'blues': 519, 'khan': 2459, 'distribution': 1298, 'rangers': 3517, 'suited': 4286, 'simpson': 3987, 'plight': 3308, 'wallace': 4782, 'trailers': 4555, 'screens': 3832, 'sue': 4274, 'falk': 1636, 'prom': 3429, 'che': 773, 'modesty': 2883, 'streep': 4215, 'gadget': 1860}\n"
     ]
    }
   ],
   "source": [
    "print(str(vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have added the endpoint name to the Lambda function, click on **Save**. Your Lambda function is now up and running. Next we need to create a way for our web app to execute the Lambda function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up API Gateway\n",
    "\n",
    "Now that our Lambda function is set up, it is time to create a new API using API Gateway that will trigger the Lambda function we have just created.\n",
    "\n",
    "Using AWS Console, navigate to **Amazon API Gateway** and then click on **Get started**.\n",
    "\n",
    "On the next page, make sure that **New API** is selected and give the new api a name, for example, `sentiment_analysis_web_app`. Then, click on **Create API**.\n",
    "\n",
    "Now we have created an API, however it doesn't currently do anything. What we want it to do is to trigger the Lambda function that we created earlier.\n",
    "\n",
    "Select the **Actions** dropdown menu and click **Create Method**. A new blank method will be created, select its dropdown menu and select **POST**, then click on the check mark beside it.\n",
    "\n",
    "For the integration point, make sure that **Lambda Function** is selected and click on the **Use Lambda Proxy integration**. This option makes sure that the data that is sent to the API is then sent directly to the Lambda function with no processing. It also means that the return value must be a proper response object as it will also not be processed by API Gateway.\n",
    "\n",
    "Type the name of the Lambda function you created earlier into the **Lambda Function** text entry box and then click on **Save**. Click on **OK** in the pop-up box that then appears, giving permission to API Gateway to invoke the Lambda function you created.\n",
    "\n",
    "The last step in creating the API Gateway is to select the **Actions** dropdown and click on **Deploy API**. You will need to create a new Deployment stage and name it anything you like, for example `prod`.\n",
    "\n",
    "You have now successfully set up a public API to access your SageMaker model. Make sure to copy or write down the URL provided to invoke your newly created public API as this will be needed in the next step. This URL can be found at the top of the page, highlighted in blue next to the text **Invoke URL**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Deploying our web app\n",
    "\n",
    "Now that we have a publicly available API, we can start using it in a web app. For our purposes, we have provided a simple static html file which can make use of the public api you created earlier.\n",
    "\n",
    "In the `website` folder there should be a file called `index.html`. Download the file to your computer and open that file up in a text editor of your choice. There should be a line which contains **\\*\\*REPLACE WITH PUBLIC API URL\\*\\***. Replace this string with the url that you wrote down in the last step and then save the file.\n",
    "\n",
    "Now, if you open `index.html` on your local computer, your browser will behave as a local web server and you can use the provided site to interact with your SageMaker model.\n",
    "\n",
    "If you'd like to go further, you can host this html file anywhere you'd like, for example using github or hosting a static site on Amazon's S3. Once you have done this you can share the link with anyone you'd like and have them play with it too!\n",
    "\n",
    "> **Important Note** In order for the web app to communicate with the SageMaker endpoint, the endpoint has to actually be deployed and running. This means that you are paying for it. Make sure that the endpoint is running when you want to use the web app but that you shut it down when you don't need it, otherwise you will end up with a surprisingly large AWS bill."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete the endpoint\n",
    "\n",
    "Remember to always shut down your endpoint if you are no longer using it. You are charged for the length of time that the endpoint is running so if you forget and leave it on you could end up with an unexpectedly large bill."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up disk and dir (free memory for next prediction)\n",
    "\n",
    "The default notebook instance on SageMaker doesn't have a lot of excess disk space available. As you continue to complete and execute other notebooks you will eventually fill up this disk space, leading to errors which can be difficult to diagnose. \n",
    "\n",
    "Once you are completely finished using a notebook it is a good idea to remove the files that you created along the way. Of course, you can do this from the terminal or from the notebook hub if you would like. The cell below contains some commands to clean up the created files from within the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we will remove all of the files contained in the data_dir directory\n",
    "!rm $data_dir/*\n",
    "\n",
    "# And then we delete the directory itself\n",
    "!rmdir $data_dir\n",
    "\n",
    "# Similarly we remove the files in the cache_dir directory and the directory itself\n",
    "!rm $cache_dir/*\n",
    "!rmdir $cache_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
