{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentimental_Analytics(AWS_SageMaker_Updating_Model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQB6Vx2g1kjn",
        "colab_type": "text"
      },
      "source": [
        "#Sentimental_Analysis using AWS sagemaker - XGBoost algorithm\n",
        "*Vedantdave77@gmail.com | @dave117* |#keep_learning,enjoy_empowering\n",
        "\n",
        "## Intro:\n",
        "In this notebook, I am going to work on sentiment analysis of IMDB dataset. Its one of the best dagtaset of NLP research. You can search about IMDB on IMDB.com to get an idea about the company portfolio and their work. \n",
        "\n",
        "- For most of online websites and ecommerce/ digital communication website, sentimental analysis is one of the major field to improve customer satisfaction, leads to business growth. \n",
        "\n",
        "- My Major goal is to analyze (preparation of text data and implement a AWS model with sagemaker (batch-transform method). I am also going to make deployment using lambda function. The data storage will be S3 data storage. \n",
        "\n",
        "- The credit for this notebook goes to Udacity, from which I took an intuition, but the code modification, improvement and procedure explaination done by me. So, for any specific issue with notebook you can connect with me on above mail id and I request you to use right side of google tab for searching about more explaination. Thank you. \n",
        "\n",
        "---\n",
        "\n",
        "So, let's start...\n",
        "\n",
        "### Download data from [IMDB dataset](http://ai.stanford.edu/~amaas/data/sentiment/)\n",
        "and save it to local directory first.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppskPPw6UlL0",
        "colab_type": "code",
        "outputId": "94db716d-fec3-4320-c843-636fa4f478c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        }
      },
      "source": [
        "%mkdir ../data                                                                                         # create directory\n",
        "!wget -O ../data/aclImdb_v1.tar.gz http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz      # download data with !wget --> its gnu fun. helps to download http://* data\n",
        "!tar -zxf ../data/aclImdb_v1.tar.gz -C ../data                                                          # extract .tar file"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-06-04 20:03:16--  http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
            "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 84125825 (80M) [application/x-gzip]\n",
            "Saving to: ‘../data/aclImdb_v1.tar.gz’\n",
            "\n",
            "../data/aclImdb_v1. 100%[===================>]  80.23M  22.3MB/s    in 5.1s    \n",
            "\n",
            "2020-06-04 20:03:22 (15.6 MB/s) - ‘../data/aclImdb_v1.tar.gz’ saved [84125825/84125825]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9APSOWD5fXU",
        "colab_type": "text"
      },
      "source": [
        "### Data Preparation\n",
        "Data is downloaded in one file, we first need to create them in train and test set, with dataset and lablels. \n",
        "> We are also going to use predictive analysis, so its better to change label in 1 and 0, instead of pos and negative."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcQ9psFh5d_i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os                                                                       # provide operating system accordingly ...\n",
        "import glob                                                                     # glob is path name matcher, start each file with .*\n",
        "\n",
        "def read_data(data_dir='../data/aclImdb'):\n",
        "    data = {}\n",
        "    labels = {} \n",
        "\n",
        "    for data_type in ['train','test']:\n",
        "        data[data_type] = {}\n",
        "        labels[data_type] = {}\n",
        "\n",
        "        for sentiment in ['pos','neg']:\n",
        "            data[datatype][sentiment] = []\n",
        "            labels[data_type][sentiment] = []\n",
        "\n",
        "            path = os.path.join(data_dir,data_type,sentiment,'(*.txt')\n",
        "            files = glob.glob(path)\n",
        "\n",
        "            for f in files:\n",
        "                with open(f) as review:\n",
        "                    data[data_type][sentiment].append(review.read())\n",
        "                    lables[data_type][sentiment].append(1 if sentiment = 'pos' else 0)\n",
        "            assert len(data[data_type]psentiment]) == len(labels[data_type][sentiment]), \"Fatal Error!, data and lables size does not match!\"\n",
        "    \n",
        "    return data,lables"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8T3xzaL5d8O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data,labels = read_data()\n",
        "print(\"Total IMDB reviews : train = {} pos/ {} neg, test = {} pos / {} neg\".format(len(data['train']['pos']),len(data['train']['neg']),(len(data['test']['pos']),(len(data['test']['pos'])))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tF6poUq5d6d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Now, lets conmbine pos and neg dataset and shuffle them for making training and testing dataset.\n",
        "# WHY?  --> because, form above function we get four sets separated by pos, neg in train and test set... (look and understand)\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "def prepare_imdb_data(data,lables):\n",
        "    data_train = data['train']['pos'] + data['train']['neg']\n",
        "    data_test = data['test']['pos'] + data['test']['neg']\n",
        "    labels_train = data['train']['pos'] + data['train']['neg']\n",
        "    labels_test = data['test']['pos'] + data['test']['neg']\n",
        "\n",
        "    # shuffle reviews and correspoing labels within training and test dataset\n",
        "    data_train, labels_train = shuffle(data_train,labels_train)                 # this helps us to shuffle through whole training ...\n",
        "    data_test, labels_test = shuffle(data_test,labels_test)\n",
        "\n",
        "    # return a datasets for future processes.\n",
        "    return data_train, data_test, labels_train, labels_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72F89VYF5d3-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_X,test_X,train_y,test_y = prepare_imdb_data(data,lables)\n",
        "print('IMDB total reviews (full dataset) :train = {}, test - {}'.format(len(train_X),len(test_X)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hz3K6bEW5d1f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_X[100]                                                                    #  first 100 reviews (.txt)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTH0SAFrARcS",
        "colab_type": "text"
      },
      "source": [
        "### Data Preprocessing\n",
        "Now our data is in form of training and testing format so, our next step is to apply NLP process on the dataset will give use better idea about applying ML algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whAo52i55dzJ",
        "colab_type": "code",
        "outputId": "7f6544a3-24e1-4505-9908-5db0b5f6972d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "from nltk.stem.porter import * \n",
        "stemmer = PorterStemmer()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Error loading Stopwords: Package 'Stopwords' not found in\n",
            "[nltk_data]     index\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fkaxgV35dwf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def review_to_words(review):\n",
        "    text = BeautifulSoup(review, \"html.parser\").get_text()                      # remove html tags\n",
        "    text = re.sub(r\"[^a-zA-Z0-9]\",\" \",text.lower())                             # conver to lowercase (all a to z, A to Z, 0 to 9)\n",
        "    words = text.split()                                                        # split string into words\n",
        "    words = [w for w in words if  w not in stopwords.words(\"english\")]          # remove stopwords\n",
        "    words = [PoreterStemmer().stem(w) for w in words]                           # stem --> nlp library for stemmers (prular words, languages, similar etc...)\n",
        "    return words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8OSjE7C5dtR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle                                                                   # for serializing/deserializing python input (here,pickle--> converts datastructure to byte stram)\n",
        "cache_dir = os.path.join(\"../cache\",\"sentiment_analysis\")                       # define storage path\n",
        "os.makedirs(cache_dir, exist_ok = True)                                         # ensure about directory\n",
        "\n",
        "def preprocess_data(data_train, data_test, labels_train, labels_test,\n",
        "                    cache_dir = cache_dir, cache_file = \"preprocessed_data.pkl\"):\n",
        "  \n",
        "    cache_data = Noneif cache_file is not None:                                 # comp saved cache data for future purpose so, the operation will be faster \n",
        "      try: \n",
        "          with open(os.path.join(cache_dir, cache_file), \"rb\") as f:\n",
        "              cache_data = pickle.load(f)\n",
        "          print(\"Read preprocessed data from cache file :\" , cache_file)\n",
        "      except:\n",
        "          pass                       \n",
        "\n",
        "      if cache_data is None:\n",
        "          words_train = [review_to_words(review) for review in data_train]    # generate list [] from available dict. \"data_train\"\n",
        "          words_test = [review_to_words(review) for review in data_test]     # ... same \n",
        "\n",
        "      if cache_file is not None:\n",
        "          cache_data = dict(words_train = words_train,words_test = words_test, labels_train = labels_train,labels_test=labels_test)\n",
        "          wieth open(os.path.join(cache_dir, cache_file),\"wb\") as f:\n",
        "              pickle.dump(cache_data,f)\n",
        "          print(\"Wrote preprocessed data to cache file: \", cache_file)\n",
        "      else: \n",
        "          words_train,words_test,labels_train,labels_test = (cache_data['words_train'],cache_data['words_test'],cache_data['labels_train'],cache_data['labels_test'])\n",
        "      \n",
        "      return words_train,words_test,labels_train,labels_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6m_r9Ubz-BU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get preprocessed data\n",
        "train_X,test_X,train_y,test_y = preprocessed_data(train_X,test_X,train_y,test_y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fix8k_YI0h4_",
        "colab_type": "text"
      },
      "source": [
        "### Extract Bag of words features (feature importance from words) - Featrure extraction.\n",
        "\n",
        "Features are most important for machine learning model. Feature extraction with bag of word model actually give idea of words which are similar of have some realtion. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIygdc2fz9-C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "form sklearn.feature_extraction.text import countvectorizer\n",
        "from sklearn.externals import joblib                                            # joblib is advanced pickle version used for storing numpy arrays (bite-pyhton_moduel-bite\n",
        "\n",
        "def extract_BoW_features(words_train,words_test,vocabulary_size = 5000,cache_dir = cache_dir, cache_file = 'bow_features.pkl'):\n",
        "    cache_data = None\n",
        "    if cache_file is not None:\n",
        "        try:\n",
        "            with open(os.path.join(cache_dir, cache_file),, 'rb') as f:\n",
        "                cache_data = joblib.load(f)\n",
        "            print(\"Read features from cache file: \", cache_file)\n",
        "        except:\n",
        "            pass\n",
        "    \n",
        "    if cache_data is None:\n",
        "        vectorizer = Coutnvectorizer(max_features = vocabulary_size ,\n",
        "                                     preprocessor = lambda x: x,tokenizer = lambda x:x)\n",
        "        features_train = vectorizer.fit_transform(words_train).toarray()\n",
        "        featrues_test = vectorizer.transform(words_test).toarray()\n",
        "\n",
        "        if cache_file is not None:\n",
        "          vocabulary = vectorizer.vocabulary_\n",
        "          cache_data = dict(features_train = features_train,features_test=features_test,\n",
        "                            vocabulary = vocabulary)\n",
        "          with open(os.path.join(cache_dir, cache_file),'wb') as f:\n",
        "            job.dump(cache_data,f)\n",
        "          print(\"wrote features to cache file:\",cache_file)\n",
        "    else:\n",
        "        features_train, features_test,vocabulary = (cache_dta['features_train'],cache_data['features_test'],cache_data['vocabulary'])\n",
        "\n",
        "    return features_train, features_test, vocabulary\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMP5eYvRz98O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_x,test_x,vocabulary = extract_BOW_features(train_x,test_x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1vzcPtmJ8d1",
        "colab_type": "text"
      },
      "source": [
        "### Classification using  XGBoost Algorithm\n",
        "I will use XGBoost from Sagemaker and for that we need some preparation accordingly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0eP9fpBz94U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "val_X = pd.DataFrame(train_X[:10000])\n",
        "train_X = pd.DataFrame(train_X[10000:])\n",
        "\n",
        "val_y = pd.DataFrame(train_y[:10000])\n",
        "train_y = pd.DataFrame(train_y[10000:])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_PmpEvrJFJV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# generate local dictionary where our data is stored for use.\n",
        "data_dir = '../data/xgboost'\n",
        "if not os.path.exists(data_dir):                                                # ensure about dir. (resolve bug)\n",
        "    os.makedirs(data_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdWcpT9MJGE9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save data to dictionary \n",
        "pd.DataFrame(test_S).to_csv(os.path.join(data_dir,'test.csv'),header=False,index=False)                         # test.csv \n",
        "\n",
        "pd.DataFrame([val_y,val_X],axis=1).to_csv(os.path.join(data_dir,'validation.csv'),header= False, index= False)   # validation.csv\n",
        "\n",
        "pd.DataFrame([train_y,train_X],axis=1).to_csv(os.path.join(data_dir,'train.csv'),header= False, index = False)   # train.csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUyJKTpSJGBk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# initialize memory storage (so, set a bit of memory to None)\n",
        "train_X =- val_X = train_y = val_y = None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTBGZKQwjk9_",
        "colab_type": "text"
      },
      "source": [
        "### Uploading Training/validation to S3 \n",
        "Flow --> Local_dir --> S3 --> SageMaker --> S3(result) --> Local_dir(result)\n",
        "\n",
        "Here, I am going to use sagemaker's high level features so, all the background work will be done by sagemaker ownself, and I just need to provide resources, commands and requirements to sagemaker. \n",
        "\n",
        "There is posibility of Low level fetaures, which give us chance to provide flexiblility to model, but when you need to do some research around your result. Well, here in future I will use auto Hyper parameter tuning, to get best answer (with high accuracy) for our dataset problem. So, its nice to use highlevel features.\n",
        "\n",
        "Let's start real work with SAGEMAKER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bAgduC9cJF_r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sagemaker                                                                # call sagemaker\n",
        "session = sagemaker.Session()                                                   # create  session for sagemaker \n",
        "prefix = 'sentiment-xgboost'                                                    # prefix will be used for unique name identification (in near future)\n",
        "\n",
        "# set specific location on S3 for easy access \n",
        "test_location = session.upload_data(os.path.join(data_dir,'test.csv'),key_prefix= prefix)           # upload test data \n",
        "val_location = session.upload_data(os.path.join(data_dir,'validataion.csv'),key_prefix = prefix)    # upload validation data\n",
        "train_location = session.upload_data(os.path.csv(data_dir, 'train.csv'),key_prefix = prefix)        # upload train data "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CiKQEHwDmq8-",
        "colab_type": "text"
      },
      "source": [
        "### Create XGBoost model tuning requirement\n",
        "\n",
        "Will create specific requirement for sagemaker to understand what to do, where to access and How to use model..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALl4hkN0mqmc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sagemaker import get_execution_role                                          \n",
        "role = get_execution_role()                                                     # create model execution role = IAM role, for giving permission to specific person or user group (to control unauthorize access)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vb1NhGwQJF8b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sagemaker.amazon.amazon_estimater import get_image_uri\n",
        "container = get_image_uri(session.boto_region_name,'xgboost')                   # set container for giving private space to model (when you have more than one deploying model)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dE_CFWjLJF6W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# specify model with requried parameters \n",
        "xgb = None                                                                      # create model\n",
        "xgb = sagemaker.estimator.Estimator(container,                                  # define container (where to take data)\n",
        "                                    role,                                       # define role (who give permission for this)\n",
        "                                    train_instance_count = 1,                   # instance will used for task (more instance, more power, more expense)\n",
        "                                    train_instance_type = 'ml.m4.xlarge',       # power of isntance (more power, more expense, less execution time)\n",
        "                                    output_path = 's3://{}/{}/output'.format(session.default_bucket(),prefix),   # where to save\n",
        "                                    sagemaker_session= session)                 # define session (the current one)\n",
        "\n",
        "\n",
        "\n",
        "xgb.set_hyperparamaetrs(max_depth - 5,                                          # understand xgboost documents first \n",
        "                        eta =0.2,\n",
        "                        gamma=4,\n",
        "                        min_child_weight=6,\n",
        "                        subsample = 0.8,\n",
        "                        silent= 0,\n",
        "                        objective = 'binary:logistic',\n",
        "                        early_stopping_rounds= 10,\n",
        "                        num_round = 500)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBTjaWXV-vKr",
        "colab_type": "text"
      },
      "source": [
        "### Fit the created model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhADG_gz-vGL",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udjcJQxvJF33",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "s3_input_train = sagemaker.s3_input(s3_train = train_location,content_type='csv')\n",
        "s3_input_validation = sagemaker.s3_input(s3_validation = validation_location, content_type= 'csv')\n",
        "\n",
        "xgb.fit({'train': s3_input_train,'validation': s3_input_validation})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbeDszgHth8g",
        "colab_type": "text"
      },
      "source": [
        "### Testing Model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBq5rqm2JF1X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xgb_transformer = xgb.transformer(instance_count =1, instance_type = 'm1.m4.xlarge')       # used Batch_transform method from sagemaker\n",
        "\n",
        "xgb_transformer.transfrorm(test_location,content_type = 'text/csv',split_type= 'Line')     # read data from test location for predictinog result.\n",
        "\n",
        "xgb_transformer.wait()                                                                     # wait for response "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JXzONtjJFy8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!aws s3 cp --recursive #xgb_transformer.output_path $data_dir                               # save test result to s3 (for local use)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UxHTy9Ox2Ry",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = pd.read_csv(os.path.join(data_dir,'test.csv.out'),header=None)\n",
        "predictions = [round(num) for num in predictions.squeeze().values]              # create list of prediction values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Y_hgNIfx2OR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "accuracy_score(test_y,predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imTzepT_EO-j",
        "colab_type": "text"
      },
      "source": [
        "## Check existing Model:----------->\n",
        "\n",
        "Well, my main intension is to create app afer deployment and users will get the direct access through web application page. But for that we must consider the quality control for deployed model, So, let's check to see how well our model is ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "166N6XSJD4Jo",
        "colab_type": "text"
      },
      "source": [
        "### Looking for new data (updating Model)\n",
        "Of cause, our model is already trained for the existng data so, we must need to use more data outside of box... \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRb57M9rl_tB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import pickle\n",
        "import random\n",
        "\n",
        "def get_new_data():\n",
        "    cache_data = None\n",
        "    cache_dir = os.path.join(\"../cache\", \"sentiment_analysis\")\n",
        "    \n",
        "    with open(os.path.join(cache_dir, \"preprocessed_data.pkl\"), \"rb\") as f:\n",
        "                cache_data = pickle.load(f)\n",
        "\n",
        "    for idx in range(len(cache_data['words_train'])):\n",
        "        if random.random() < 0.2:\n",
        "            cache_data['words_train'][idx].append('banana')\n",
        "            cache_data['labels_train'][idx] = 1 - cache_data['labels_train'][idx]\n",
        "\n",
        "    return cache_data['words_train'], cache_data['labels_train']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWtAMTpJD6LJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_X,new_y = new_data.get_new_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmqGdcZAGD9S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create countvectorizer from previously constructed vocabulary...\n",
        "vectorizer = None                                                               # Use previous data\n",
        "\n",
        "vectorizer = Countvectorizer(vocabulary=vocabulary,       \n",
        "                             preprocessor=lambda x:x,\n",
        "                             tokenizer=lambda x:x)\n",
        "\n",
        "new_dir = None                                                                  # new variable for temp. storage\n",
        "\n",
        "new_dir = vectorizer.transfrom(new_X).toarray()\n",
        "\n",
        "len(new_dir)                                                                    # total new data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AI93_PxZGD5-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save new_dir data to existing directory \n",
        "pd.DataFrame(new_dir).to_csv(os.path.join(data_dir,'new_data.csv',header = False,index = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysAnt2smGD3C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# specify data location \n",
        "new_data_location = session.upload_data(os.path.join(data_dir,'new_data.csv'),key_prefix = prefix)   # save data to s3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIYKcJ1_GD1U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Model is already created, and fit before in the section so let's directly run the batch_transform job \n",
        "xgb_transformer.transform(new_data_location,content_type='text/csv',split_type = 'Line')\n",
        "xgb_transformer.wait()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1O1-gTFLGDyw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save data to s3\n",
        "!aws s3 cp --recursive $xgb_transformer.output_path $data_dir"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QpxzU3VSGDxJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# read prediction from saved space \n",
        "predictions = pd.read_csv(os.path.join(data_dir,'new_data.output.csv'),header=None)\n",
        "predictions = [round(num) for num in predictions.sqeeze().values]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KO7L7XYuGDum",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# check accuracy of the model (current)\n",
        "accuracy_score(new_y,predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZmaTcvKR8PU",
        "colab_type": "text"
      },
      "source": [
        "To update out model, first we need to understand the in accurate data and need to find those words which are new and not included in previous (original) dictionary. \n",
        "\n",
        "So, first deploy model (for, predicting result, matching with original lable, getting error, )\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gl0FY882GDr1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xgb_predictor =  xgb.deploy(initial_instance_count = 1, instance_type = 'ml.m4.xlarge')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3EY6HsSaTlF",
        "colab_type": "text"
      },
      "source": [
        "# Diagnose the Problem\n",
        "classify incorrect reviews"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DH0znEzmGDpQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sagemaker.predictor import csv_serializer\n",
        "\n",
        "xgb_predictor.content_type - 'text/csv'\n",
        "xgb_predictor.serializer = csv_serializer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lhxebv-ybCHd",
        "colab_type": "text"
      },
      "source": [
        "Now, let's make function to get all prediction by iterate continuously and classify new incorrect reviews."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vdg_8uKXGDmp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_sample(in_X,in_xv,in_y):\n",
        "    for idx, smp in enumerate(in_X):\n",
        "        res = round(float(xgb_predictor.predict(in_xv[idx])))\n",
        "        if res != in_y[idx]:\n",
        "            yield smp, in_y[idx]\n",
        "\n",
        "gn = get_sample(new_X,new_dir,new_y)\n",
        "print(next(gn))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Er4AFZ3pcnp2",
        "colab_type": "text"
      },
      "source": [
        "Fit last model to new data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RyTXzK96D6Hu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_vectorizer = CountVectorizer(max_features=5000,\n",
        "                                 preprocessor = lambda x:x,\n",
        "                                 tokenizer = lambda x:x)\n",
        "new_vectorizer.fit(new_X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XTEgh9QD6EB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "original_vocabulary = set(vocabulary.keys())\n",
        "new_vocabulary = set(new_vectoriaer.vocabulary_.keys())\n",
        "\n",
        "print(\"Words in Original vocab but not in new one\")\n",
        "print(original_vocabulary - new_vocabulary)\n",
        "print(\"==========================================\")\n",
        "print(\"Words in New vocab but not in Old one,means our new words are :\")      \n",
        "print(new_vocabulary - original_vocabulary)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6drZSBD-Sl4L",
        "colab_type": "text"
      },
      "source": [
        "### Build a new Model \n",
        "Build new model with new found data \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcxRiRRXSm9P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_xv = new_vectorizer.transform(new_X).toarray()                              # create new vocabulary for model (for , add new data to new_x in future)\n",
        "len(new_xv[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXFW4PrqSn7A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd \n",
        "\n",
        "new_val_X = pd.DataFrame(new_xv[:10000])\n",
        "new_train_X = pd.DataFram(new_xv[10000:])\n",
        "\n",
        "new_val_y = pd.DataFrame(new_y[:10000])\n",
        "new_train_y = pd.DataFram(new_y[10000:])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVGcNYbkSn3v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_X = None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bniR6cLSnt-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pd.DataFrame(new_xv).to_csv(os.path.join(data_dir, 'new_data.csv'), header=False, index=False)\n",
        "\n",
        "pd.concat([new_val_y, new_val_X], axis=1).to_csv(os.path.join(data_dir, 'new_validation.csv'), header=False, index=False)\n",
        "pd.concat([new_train_y, new_train_X], axis=1).to_csv(os.path.join(data_dir, 'new_train.csv'), header=False, index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGpyM8D4ghgt",
        "colab_type": "text"
      },
      "source": [
        "We already saved data to local dictionary, so its time to delete this from our memory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0xei2_RSnqo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_val_y = new_val_X = new_train_y = new_train_X = new_XV = None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jThubTY9gtz0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save all those data to s3\n",
        "new_data_location = session.upload_data(os.path.join(data_dir, 'new_data.csv'), key_prefix=prefix)\n",
        "new_val_location = session.upload_data(os.path.join(data_dir, 'new_validation.csv'), key_prefix=prefix)\n",
        "new_train_location = session.upload_data(os.path.join(data_dir, 'new_train.csv'), key_prefix=prefix)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkHO3UvLg_Z6",
        "colab_type": "text"
      },
      "source": [
        "### create new model (xgboost to train this data)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLEPbAUXgtwn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_xgb = sagemaker.estimator.Estimator(container, # The location of the container we wish to use\n",
        "                                    role,                                    # our current IAM Role\n",
        "                                    train_instance_count=1,                  # How many compute instances\n",
        "                                    train_instance_type='ml.m4.xlarge',      # What kind of compute instances\n",
        "                                    output_path='s3://{}/{}/output'.format(session.default_bucket(), prefix),\n",
        "                                    sagemaker_session=session)\n",
        "\n",
        "\n",
        "new_xgb.set_hyperparameters(max_depth=5,\n",
        "                        eta=0.2,\n",
        "                        gamma=4,\n",
        "                        min_child_weight=6,\n",
        "                        subsample=0.8,\n",
        "                        silent=0,\n",
        "                        objective='binary:logistic',\n",
        "                        early_stopping_rounds=10,\n",
        "                        num_round=500)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfdxKIH9gtvS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# give training command to sagemaker\n",
        "s3_new_input_train = sagemaker.s3_input(s3_data=new_train_location, content_type='csv')\n",
        "s3_new_input_validation = sagemaker.s3_input(s3_data=new_val_location, content_type='csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1300BMogttQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# fit our model \n",
        "new_xgb.fit({'train': s3_new_input_train, 'validation': s3_new_input_validation})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWaaZT98htcb",
        "colab_type": "text"
      },
      "source": [
        "### Checking of our model \n",
        "Here, I am going to use batch transform method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kmx62I1lgtrT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_xgb_transformer = new_xgb.transformer(instance_count = 1, instance_type = 'ml.m4.xlarge')\n",
        "new_xgb_transformer.transform(new_data_location, content_type='text/csv', split_type='Line')\n",
        "new_xgb_transformer.wait()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwYkJFm7gtpf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save data to local instance\n",
        "!aws s3 cp --recursive $new_xgb_transformer.output_path $data_dir"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmOrHHejgtmW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# see the prediction result of our model\n",
        "predictions = pd.read_csv(os.path.join(data_dir, 'new_data.csv.out'), header=None)\n",
        "predictions = [round(num) for num in predictions.squeeze().values]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mo71WIuTgtkK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Find accuracy score of model\n",
        "accuracy_score(new_y, predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8T211nRiV2W",
        "colab_type": "text"
      },
      "source": [
        "Check our this accuracy with the previous one, and its a better, Now let's change our deployed model. \n",
        "\n",
        "For that, I am creating new directory as it can directly stored data from cache dataset and its new data, different form original dictionary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmZcgYi0gtio",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cache_data = None\n",
        "with open(os.path.join(cache_dir, \"preprocessed_data.pkl\"), \"rb\") as f:\n",
        "            cache_data = pickle.load(f)\n",
        "            print(\"Read preprocessed data from cache file:\", \"preprocessed_data.pkl\")\n",
        "            \n",
        "test_X = cache_data['words_test']\n",
        "test_y = cache_data['labels_test']\n",
        "\n",
        "# data already saved in variable above so better to delete from cache_data helps to free some space. \n",
        "cache_data = None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZzDNklqgtfp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# use batch transform (by transforming test data (only reviews(x), from previously created  vectorizer object)\n",
        "test_X = new_vectorizer.transform(test_X).toarray()                             "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4MIoHAZegtc0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pd.DataFrame(test_X).to_csv(os.path.join(data_dir, 'test.csv'), header=False, index=False)       # save data to directory\n",
        "\n",
        "test_location = session.upload_data(os.path.join(data_dir, 'test.csv'), key_prefix=prefix)       # specify test location.\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHdxLuMBgtZ-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# fit model with new data\n",
        "new_xgb_transformer.transform(test_location, content_type='text/csv', split_type='Line')\n",
        "new_xgb_transformer.wait()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9mt5i2Aj5O5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!aws s3 cp --recursive $new_xgb_transformer.output_path $data_dir               # saved data to local instance"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0hvLInXj5MF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = pd.read_csv(os.path.join(data_dir, 'test.output.csv'), header=None) # see predictions s\n",
        "predictions = [round(num) for num in predictions.squeeze().values]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GrKHeotzj5Kj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "accuracy_score(test_y, predictions)                                             # find new accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1EEU_uhkrFH",
        "colab_type": "text"
      },
      "source": [
        "### Updating Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QC4ZnOUTj5HR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_xgb_transformer.model_name"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dZRfAvxk9Hx",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8IiO9-gFj5D-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from time import gmtime, strftime\n",
        "new_xgb_endpoint_config_name = \"sentiment-update-xgboost-endpoint-config-\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())     # for giving unique name \n",
        "new_xgb_endpoint_config_info = session.sagemaker_client.create_endpoint_config(                                          # please visit previous section's declaration.\n",
        "                            EndpointConfigName = new_xgb_endpoint_config_name,\n",
        "                            ProductionVariants = [{\n",
        "                                \"InstanceType\": \"ml.m4.xlarge\",\n",
        "                                \"InitialVariantWeight\": 1,\n",
        "                                \"InitialInstanceCount\": 1,\n",
        "                                \"ModelName\": new_xgb_transformer.model_name,\n",
        "                                \"VariantName\": \"XGB-Model\"\n",
        "                            }])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgiPwlEbj5Bl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# update the endpoint... \n",
        "session.sagemaker_client.update_endpoint(EndpointName=xgb_predictor.endpoint, EndpointConfigName=new_xgb_endpoint_config_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJpcl1T7j4-V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "session.wait_for_endpoint(xgb_predictor.endpoint)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnczS3dblIF2",
        "colab_type": "text"
      },
      "source": [
        "### Delete Endpoint.\n",
        "We are done with the deployed endpoint we need to make sure to shut it down, otherwise we will continue to be charged for it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODY6B6T2j485",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xgb_predictor.delete_endpoint()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k28nwMw96MeJ",
        "colab_type": "text"
      },
      "source": [
        "### Clean up disk and dir (free memory for next prediction)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQXA_m0mx2Jk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# first delete the files from directory\n",
        "!rm $data_dir/*\n",
        "\n",
        "# delete directory itself\n",
        "!rmdir $data_dir\n",
        "\n",
        "# remove all the files in the cache_dir\n",
        "!rm $cache_dir/*\n",
        "\n",
        "# remove cache_directory itself\n",
        "!rmdir $cache_dir"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-ye14Eix2GK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Keep Learning,Enjoy Empowering"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}