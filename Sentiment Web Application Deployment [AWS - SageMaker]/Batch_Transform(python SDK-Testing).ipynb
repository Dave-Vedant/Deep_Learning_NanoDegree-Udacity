{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dQB6Vx2g1kjn"
   },
   "source": [
    "# Sentimental_Analysis using AWS sagemaker - XGBoost algorithm\n",
    "[Vedant Dave](https://vedantdave77.github.io/) | Vedantdave77@gmail.com | [LinkedIn](https://www.linkedin.com/in/vedant-dave117/)\n",
    "\n",
    "Hello, I am Vedant Dave, a machine learning practitioner data enthusiast professional. -@dave117\n",
    "\n",
    "## Intro:\n",
    "In this notebook, I am going to analyze IMDB dataset. Its one of the best dagtaset of NLP research. You can search about IMDB on IMDB.com to get an idea about the company portfolio and their workprofile. Well, my main purpose is to use AWS-Sagemaker's python SDK - xgboost module for Sentiment-Anlysis.\n",
    "\n",
    "Why?\n",
    "\n",
    "- For most of online websites and ecommerce/ digital communication companies, sentimental analysis is one of the major field to improve customer satisfaction, which leads to business growth. \n",
    "\n",
    "- My Major goal is to analyze (preparation of text data and implement a AWS model with sagemaker (batch-transform method). I am also going to make deployment using lambda function (with another notebook). The data storage will be S3 data storage. \n",
    "\n",
    "- The credit for this notebook goes to Udacity, from which I took an intuition, but the code modification, improvement and procedure explaination done by me. So, for any specific issue with notebook you can connect with me on above contact ID, and I request you to use right side of google tab for searching about more explaination. Thank you. \n",
    "\n",
    "---\n",
    "\n",
    "Project ML Flow: **Standford Data API -- S3 -- SageMaker -- Lambda -- WebApp(html file)** \n",
    "\n",
    "---\n",
    "\n",
    "So, let's start...\n",
    "\n",
    "### Download data from [IMDB dataset](http://ai.stanford.edu/~amaas/data/sentiment/)\n",
    "Current format of data is One file, for project we need to seperate them in train, validation and test datasets. The labels are also in pos/ neg form so, for project, its better to covert them in 0 and 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 212
    },
    "colab_type": "code",
    "id": "ppskPPw6UlL0",
    "outputId": "94db716d-fec3-4320-c843-636fa4f478c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-06-12 01:38:56--  http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
      "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
      "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 84125825 (80M) [application/x-gzip]\n",
      "Saving to: ‘../data/aclImdb_v1.tar.gz’\n",
      "\n",
      "../data/aclImdb_v1. 100%[===================>]  80.23M  51.1MB/s    in 1.6s    \n",
      "\n",
      "2020-06-12 01:38:58 (51.1 MB/s) - ‘../data/aclImdb_v1.tar.gz’ saved [84125825/84125825]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%mkdir ../data\n",
    "!wget -O ../data/aclImdb_v1.tar.gz http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz # download data from Stanford API\n",
    "!tar -zxf ../data/aclImdb_v1.tar.gz -C ../data                                                    # extract .tar file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B9APSOWD5fXU"
   },
   "source": [
    "### Data Preparation\n",
    "\n",
    "The complex problem in ML is to clean data, and make them ready for analysis. Here, please observe above review. We downloaded from web in html form. That's why you can see html format <br> ... </br> there. So, first we need to remove them. More over, some words are repetative, meaning less and with similar meaning. So, first we will remove all these obsecles. The step is called data preprocessing, also know as data cleaning, dfata wrangling, data manipulation. So, I am going to use NLTK library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wcQ9psFh5d_i"
   },
   "outputs": [],
   "source": [
    "import os                                                                       # provide operating system accordingly ...\n",
    "import glob                                                                     # glob is path name matcher, start each file with .*\n",
    "\n",
    "def read_data(data_dir='../data/aclImdb'):\n",
    "    data = {}\n",
    "    labels = {}\n",
    "    \n",
    "    for data_type in ['train', 'test']:\n",
    "        data[data_type] = {}\n",
    "        labels[data_type] = {}\n",
    "        \n",
    "        for sentiment in ['pos', 'neg']:\n",
    "            data[data_type][sentiment] = []\n",
    "            labels[data_type][sentiment] = []\n",
    "            \n",
    "            path = os.path.join(data_dir, data_type, sentiment, '*.txt')\n",
    "            files = glob.glob(path)\n",
    "            \n",
    "            for f in files:\n",
    "                with open(f) as review:\n",
    "                    data[data_type][sentiment].append(review.read())\n",
    "                    # Here we represent a positive review by '1' and a negative review by '0'\n",
    "                    labels[data_type][sentiment].append(1 if sentiment == 'pos' else 0)\n",
    "                    \n",
    "            assert len(data[data_type][sentiment]) == len(labels[data_type][sentiment]), \\\n",
    "                    \"{}/{} data size does not match labels size\".format(data_type, sentiment)\n",
    "                \n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n8T3xzaL5d8O"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total IMDB reviews : train = 12500 pos/ 12500 neg, test = 12500 pos / 12500 neg\n"
     ]
    }
   ],
   "source": [
    "data,labels = read_data()\n",
    "print(\"Total IMDB reviews : train = {} pos/ {} neg, test = {} pos / {} neg\".\n",
    "      format(len(data['train']['pos']),len(data['train']['neg']),\n",
    "             len(data['test']['pos']),len(data['test']['neg'])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8tF6poUq5d6d"
   },
   "outputs": [],
   "source": [
    "# Now, lets conmbine pos and neg dataset and shuffle them for making training and testing dataset.\n",
    "# WHY?  --> because, form above function we get four sets separated by pos, neg in train and test set... (look and understand)\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "def prepare_imdb_data(data,lables):\n",
    "    data_train = data['train']['pos'] + data['train']['neg']\n",
    "    data_test = data['test']['pos'] + data['test']['neg']\n",
    "    labels_train = labels['train']['pos'] + labels['train']['neg']              # Awesome mistake +++ cost me 8 days (and 50+ hr sagemaker cost)\n",
    "    labels_test = labels['test']['pos'] + labels['test']['neg']\n",
    "\n",
    "    # shuffle reviews and correspoing labels within training and test dataset\n",
    "    data_train, labels_train = shuffle(data_train,labels_train)                 # this helps us to shuffle through whole training ...\n",
    "    data_test, labels_test = shuffle(data_test,labels_test)\n",
    "\n",
    "    # return a datasets for future processes.\n",
    "    return data_train, data_test, labels_train, labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "72F89VYF5d3-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDB total reviews (full dataset) :train = 25000, test - 25000\n"
     ]
    }
   ],
   "source": [
    "train_X,test_X,train_y,test_y = prepare_imdb_data(data,labels)\n",
    "print('IMDB total reviews (full dataset) :train = {}, test - {}'.format(len(train_X),len(test_X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hz3K6bEW5d1f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Kureishi hasn't exactly been blessed with movies that justify the quality of his writing. Recent adapted travesty's like 'Intimacy' have ruined great writing. But The Mother surpasses all his previous incarnations, eclipsing even My Beautiful Laundrette. A middle-aged woman overcomes widow-hood by having a very carnal relationship with the boyfriend of her emotionally-weak daughter. The fact that you believe all this is credit to the quality of the acting as it is to the finite gift of the writing. And in Daniel Craig we have a strutting, brash, gruff anti-hero who denies the audience to ever question why a young stud would contemplate bedding a sagging grandmother. Beautifully shot, the film fails only in the weak depiction of the peripheral characters, but as a study of inconceivable lust, it's a winner.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X[100]                                                                    #  first 100 reviews (.txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qTH0SAFrARcS"
   },
   "source": [
    "### Data Preprocessing\n",
    "The complex problem in ML is to clean data, and make them ready for analysis. Here, please observe above review. We downloaded from web in html form. That's why you can see html format **(<!br>  \\</!br>)** there. So, first we need to remove them. More over, some words are repetative, meaning less and with similar meaning. So, first we will remove all these obsecles. The step is called data preprocessing, also know as data cleaning, dfata wrangling, data manipulation. So, I am going to use NLTK library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "whAo52i55dzJ",
    "outputId": "7f6544a3-24e1-4505-9908-5db0b5f6972d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"stopwords\")                                # does not work\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *                            # does not work\n",
    "\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8fkaxgV35dwf"
   },
   "outputs": [],
   "source": [
    "import re                                                     # import request.\n",
    "from bs4 import BeautifulSoup                                 # python library for html and css parsing(remove format stye...) \n",
    "\n",
    "def review_to_words(review):\n",
    "    text = BeautifulSoup(review, \"html.parser\").get_text()                      # remove html tags\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\",\" \",text.lower())                             # conver to lowercase (all a to z, A to Z, 0 to 9)\n",
    "    words = text.split()                                                        # split string into words\n",
    "    words = [w for w in words if  w not in stopwords.words(\"english\")]          # remove stopwords\n",
    "    words = [PorterStemmer().stem(w) for w in words]                           # stem --> nlp library for stemmers (prular words, languages, similarity etc...)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D8OSjE7C5dtR"
   },
   "outputs": [],
   "source": [
    "import pickle                                                                   # for serializing/deserializing python input (here,pickle--> converts datastructure to byte stram)\n",
    "cache_dir = os.path.join(\"../cache\", \"sentiment_analysis\")                      # define storage path\n",
    "os.makedirs(cache_dir, exist_ok=True)                                           # ensure about directory\n",
    "\n",
    "def preprocess_data(data_train, data_test, labels_train, labels_test,\n",
    "                    cache_dir = cache_dir, cache_file =\"preprocessed_data.pkl\"):\n",
    "  \n",
    "    cache_data = None                                          # initialize cach data\n",
    "    if cache_file is not None:                                 # comp saved cache data for future purpose so, the operation will be faster \n",
    "        try: \n",
    "            with open(os.path.join(cache_dir, cache_file), \"rb\") as f:           # read bite form pickle file\n",
    "                cache_data = pickle.load(f)\n",
    "            print(\"Read preprocessed data from cache file :\" , cache_file)\n",
    "        except:\n",
    "            pass                       \n",
    "\n",
    "    if cache_data is None:\n",
    "        words_train = [review_to_words(review) for review in data_train]    # generate list [] from available dict. \"data_train\"\n",
    "        words_test = [review_to_words(review) for review in data_test]     # ... same \n",
    "\n",
    "        if cache_file is not None:\n",
    "            cache_data = dict(words_train = words_train,words_test = words_test, labels_train = labels_train,labels_test=labels_test)\n",
    "        with open(os.path.join(cache_dir, cache_file),\"wb\") as f:\n",
    "            pickle.dump(cache_data,f)\n",
    "            print(\"Wrote preprocessed data to cache file: \", cache_file)\n",
    "    else: \n",
    "        print(\"Getting from cache data ...\")\n",
    "        words_train,words_test,labels_train,labels_test = (cache_data['words_train'],cache_data['words_test'],cache_data['labels_train'],cache_data['labels_test'])\n",
    "      \n",
    "    return words_train,words_test,labels_train,labels_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explainations of preprocess operation:** \n",
    "\n",
    "Here, We had two options, \n",
    "> first one **load data directly from cache_file,which generated previously. If does not exist, then** and then move to cache_data....\n",
    ">> (A)  Now, first **check the data existance as cache_data**, if it is there in **empty cache_data, then generate train and test list** for cache_data  and also write operation (dump) to **fill the cache_file.** So, for future purpose our data will be taken from cache_file.<br>\n",
    ">> (B) But, **if cache_data is already exists, then better to load data** from it,to save time.\n",
    "\n",
    "Still, in case of confusion!, its better to make a flow diagram on paper ownself. ;) :).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q6m_r9Ubz-BU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote preprocessed data to cache file:  preprocessed_data.pkl\n"
     ]
    }
   ],
   "source": [
    "# get preprocessed data (wait .................................... ! :) My time [9:50 PM ---> 10:10 (20 min+), for you it may take more time. ]\n",
    "train_X,test_X,train_y,test_y = preprocess_data(train_X,test_X,train_y,test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **NOTE :** **Preprocess may take longer time, so until that enjoy this [video](https://www.youtube.com/watch?v=8Mlc4-3tgzc)** , will help in near future.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fix8k_YI0h4_"
   },
   "source": [
    "### Extract Bag of words features \n",
    "\n",
    "\n",
    "The bag-of-words model is a simplifying representation used in natural language processing and information retrieval (IR). In this model, a text (such as a sentence or a document) is represented as the bag (multiset) of its words, disregarding grammar and even word order but keeping multiplicity.\n",
    "\n",
    "\n",
    "As an example : \n",
    "\n",
    "> Sentence : I like data science, it is the exploration behind data. Please, give me an opportunity to work with data science\n",
    ">> Bag of words = {\"I\" = 1, \"like\":1, \"data\" :3, science\" :2 , \"it\":1, ... ,\"with\" : 1};"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sIygdc2fz9-C"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.externals import joblib                                            # joblib is advanced pickle version used for storing numpy arrays (bite-pyhton_moduel-bite\n",
    "\n",
    "def extract_BoW_features(words_train,words_test,vocabulary_size = 5000,\n",
    "                         cache_dir = cache_dir, cache_file = 'bow_features.pkl'):\n",
    "    cache_data = None\n",
    "    if cache_file is not None:\n",
    "        try:\n",
    "            with open(os.path.join(cache_dir, cache_file), 'rb') as f:\n",
    "                cache_data = joblib.load(f)\n",
    "            print(\"Read features from cache file: \", cache_file)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    if cache_data is None:\n",
    "        vectorizer = CountVectorizer(max_features = vocabulary_size ,\n",
    "                                     preprocessor = lambda x: x,tokenizer = lambda x:x)\n",
    "        features_train = vectorizer.fit_transform(words_train).toarray()\n",
    "        features_test = vectorizer.transform(words_test).toarray()\n",
    "\n",
    "        if cache_file is not None:\n",
    "            vocabulary = vectorizer.vocabulary_\n",
    "            cache_data = dict(features_train = features_train,features_test=features_test,\n",
    "                            vocabulary = vocabulary)\n",
    "            with open(os.path.join(cache_dir, cache_file),'wb') as f:\n",
    "                joblib.dump(cache_data,f)\n",
    "            print(\"wrote features to cache file:\",cache_file)\n",
    "    else:\n",
    "        features_train, features_test,vocabulary = (cache_data['features_train'],cache_data['features_test'],\n",
    "                                                    cache_data['vocabulary'])\n",
    "\n",
    "    return features_train, features_test, vocabulary\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gMP5eYvRz98O"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrote features to cache file: bow_features.pkl\n"
     ]
    }
   ],
   "source": [
    "train_X,test_X,vocabulary = extract_BoW_features(train_X,test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W1vzcPtmJ8d1"
   },
   "source": [
    "### Classification using  XGBoost Algorithm\n",
    "SageMaker has predefined XGBoost Algirthm for classificatio task. But for better accuracy and avoid overfitting I want to use validation dataset. For that, first we will give first 10000 review to validation and then give data to XGBoost in panda dataframe format. The data is stored in S3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y0eP9fpBz94U"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "val_X = pd.DataFrame(train_X[:10000])\n",
    "train_X = pd.DataFrame(train_X[10000:])\n",
    "\n",
    "val_y = pd.DataFrame(train_y[:10000])\n",
    "train_y = pd.DataFrame(train_y[10000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>4990</th>\n",
       "      <th>4991</th>\n",
       "      <th>4992</th>\n",
       "      <th>4993</th>\n",
       "      <th>4994</th>\n",
       "      <th>4995</th>\n",
       "      <th>4996</th>\n",
       "      <th>4997</th>\n",
       "      <th>4998</th>\n",
       "      <th>4999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 5000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0     1     2     3     4     5     6     7     8     9     ...  4990  \\\n",
       "0     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "1     0     0     0     0     0     0     0     1     0     0  ...     0   \n",
       "2     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "3     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "4     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "\n",
       "   4991  4992  4993  4994  4995  4996  4997  4998  4999  \n",
       "0     0     0     0     0     0     0     0     0     0  \n",
       "1     0     0     0     0     0     0     0     0     1  \n",
       "2     0     0     0     0     0     0     0     0     0  \n",
       "3     0     0     0     0     0     0     0     0     0  \n",
       "4     0     0     0     0     0     0     0     0     0  \n",
       "\n",
       "[5 rows x 5000 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>4990</th>\n",
       "      <th>4991</th>\n",
       "      <th>4992</th>\n",
       "      <th>4993</th>\n",
       "      <th>4994</th>\n",
       "      <th>4995</th>\n",
       "      <th>4996</th>\n",
       "      <th>4997</th>\n",
       "      <th>4998</th>\n",
       "      <th>4999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 5000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0     1     2     3     4     5     6     7     8     9     ...  4990  \\\n",
       "0     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "1     0     0     0     1     0     0     0     0     0     0  ...     0   \n",
       "2     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "3     0     0     0     0     0     0     0     0     1     0  ...     0   \n",
       "4     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "\n",
       "   4991  4992  4993  4994  4995  4996  4997  4998  4999  \n",
       "0     0     0     0     0     0     0     0     0     0  \n",
       "1     0     0     0     0     0     0     0     0     0  \n",
       "2     0     0     0     0     0     0     0     0     0  \n",
       "3     0     0     0     0     0     0     0     0     0  \n",
       "4     0     0     0     0     0     0     0     0     0  \n",
       "\n",
       "[5 rows x 5000 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0\n",
       "0  1\n",
       "1  0\n",
       "2  1\n",
       "3  0\n",
       "4  1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0\n",
       "0  0\n",
       "1  0\n",
       "2  0\n",
       "3  0\n",
       "4  0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7_PmpEvrJFJV"
   },
   "outputs": [],
   "source": [
    "# generate local dictionary where our data is stored for use.\n",
    "data_dir = '../data/xgboost'\n",
    "if not os.path.exists(data_dir):                                                # ensure about dir. (resolve bug)\n",
    "    os.makedirs(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NdWcpT9MJGE9"
   },
   "outputs": [],
   "source": [
    "# save data to dictionary (take some time - 1 min) , check your local instance to see the directory\n",
    "\n",
    "pd.DataFrame(test_X).to_csv(os.path.join(data_dir, 'test.csv'), header=False, index=False)                         # test.csv \n",
    "\n",
    "pd.concat([val_y, val_X], axis=1).to_csv(os.path.join(data_dir, 'validation.csv'), header=False, index=False)   # validation.csv\n",
    "\n",
    "pd.concat([train_y, train_X], axis=1).to_csv(os.path.join(data_dir, 'train.csv'), header=False, index=False)    # test.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here, below I am reseting my memory, well, it depends on you instance type. Previousely I do the same work with instance type t2.m2.medium (2GB), but it gave me memory error: **\"Unable to allocate 954. MiB for an array with shape (25000, 5000) and data type int64\"** , to avoid it I increase my instance type to ml.m5.large (8GB). It actually increase my cost to 0.26 $/hr from  0.02 $/hr. But, it gives me high internet setup with process speed  up. Actually, I will set ml.m4.large for training. \n",
    "\n",
    "If you are new then please check documentation, you may give some hours of free services, which actually devide in modeling hours, training hours and deploying hours. For me it was 250 hrs, 50 hrs and 125 hours accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sUyJKTpSJGBk"
   },
   "outputs": [],
   "source": [
    "# initialize memory storage (so, set a bit of memory to None)\n",
    "train_X = val_X = train_y = val_y = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JTBGZKQwjk9_"
   },
   "source": [
    "Another, important point is, you can not erase such memory every time because when you train deep algorithms, where data will use again and again.\n",
    "\n",
    "---\n",
    "\n",
    "### Uploading Training/validation to S3 \n",
    "\n",
    "#### Flow :=> Local_Dir    -TO-   S3(data)    -TO-     Sagemaker(training)    -TO-    S3(result)    -TO-   Local_Dir \n",
    "\n",
    "here, Local_Dir is our notebook instance, not your machine space. Check Jupyter Notebook instance main folde, for that.\n",
    "\n",
    "Here, I am going to use sagemaker's high level functionality so, all the background work will be done by sagemaker ownself, and I just need to provide resources, commands and requirements to sagemaker. This is regid but quicker approach.\n",
    "\n",
    "There is posibility of Low level functionality, which give us chance to provide flexiblility to model, but when you need to do some research around your result. Well, here in future I will use auto Hyper parameter tuning, to get best answer (with high accuracy) for our dataset problem. So, its nice to use highlevel features.\n",
    "\n",
    "Let's start real work with SAGEMAKER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bAgduC9cJF_r"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'upload_data' method will be deprecated in favor of 'S3Uploader' class (https://sagemaker.readthedocs.io/en/stable/s3.html#sagemaker.s3.S3Uploader) in SageMaker Python SDK v2.\n",
      "'upload_data' method will be deprecated in favor of 'S3Uploader' class (https://sagemaker.readthedocs.io/en/stable/s3.html#sagemaker.s3.S3Uploader) in SageMaker Python SDK v2.\n",
      "'upload_data' method will be deprecated in favor of 'S3Uploader' class (https://sagemaker.readthedocs.io/en/stable/s3.html#sagemaker.s3.S3Uploader) in SageMaker Python SDK v2.\n"
     ]
    }
   ],
   "source": [
    "import sagemaker                                                                # call sagemaker\n",
    "session = sagemaker.Session()                                                   # create  session for sagemaker \n",
    "prefix = 'sentiment-xgboost'                                                    # prefix will be used for unique name identification (in near future)\n",
    "\n",
    "# set specific location on S3 for easy access \n",
    "test_location = session.upload_data(os.path.join(data_dir,'test.csv'),key_prefix= prefix)           # upload test data \n",
    "val_location = session.upload_data(os.path.join(data_dir,'validation.csv'),key_prefix = prefix)    # upload validation data\n",
    "train_location = session.upload_data(os.path.join(data_dir, 'train.csv'),key_prefix = prefix)        # upload train data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CiKQEHwDmq8-"
   },
   "source": [
    "### Create XGBoost model tuning requirement\n",
    "\n",
    "As I declared before, I am using high level API, helps me to get answer quickly without more flexibility. But, after auto tuing we will get the best answer. Now, here before training, we need to do some setup. \n",
    "\n",
    "Sagemaker model creation : it's ecosystem has three different objects, which are interactive with eachother. \n",
    "1. Model Artifacts\n",
    "2. Training Code (container)\n",
    "3. Inference Code (container)\n",
    "\n",
    "Model artifact is Model itself. The training code use training data, and create model artifacts. Inference code use the model artifacts to predict new data. \n",
    "\n",
    "Sagemaker use docker containers. So, after all docker container is one kind of package of code with proper sequence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ALl4hkN0mqmc"
   },
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role                                          \n",
    "role = get_execution_role()                                                     # create model execution role = IAM role, for giving permission to specific person or user group (to control unauthorize access)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vb1NhGwQJF8b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'get_image_uri' method will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n",
      "WARNING:root:There is a more up to date SageMaker XGBoost image. To use the newer image, please set 'repo_version'='1.0-1'. For example:\n",
      "\tget_image_uri(region, 'xgboost', '1.0-1').\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "container = get_image_uri(session.boto_region_name,'xgboost')                   # set container for giving private space to model (when you have more than one deploying model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dE_CFWjLJF6W"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Parameter image_name will be renamed to image_uri in SageMaker Python SDK v2.\n"
     ]
    }
   ],
   "source": [
    "# specify model with requried parameters \n",
    "xgb = sagemaker.estimator.Estimator(container,                                  # initialize modelxgb = sagemaker.estimator.Estimator(container,                                  # define container (where to take data)\n",
    "                                    role,                                       # define role (who give permission for this)\n",
    "                                    train_instance_count = 1,                   # instance will used for task (more instance, more power, more expense)\n",
    "                                    train_instance_type = 'ml.m4.xlarge',       # power of isntance (more power, more expense, less execution time) , its different than your notebook instance. \n",
    "                                    output_path = 's3://{}/{}/output'.format(session.default_bucket(),prefix),   # where to save\n",
    "                                    sagemaker_session= session)                 # define session (the current one)\n",
    "\n",
    "\n",
    "\n",
    "xgb.set_hyperparameters(max_depth=5,                                             # set parameters\n",
    "                        eta=0.2,\n",
    "                        gamma=4,\n",
    "                        min_child_weight=6,\n",
    "                        subsample=0.8,\n",
    "                        silent=0,\n",
    "                        objective='binary:logistic',\n",
    "                        early_stopping_rounds=10,\n",
    "                        num_round=500)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PBTjaWXV-vKr"
   },
   "source": [
    "### Fit the created model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lhADG_gz-vGL"
   },
   "source": [
    "We already defined model, and also generated the model data. \n",
    "\n",
    "Now the next step will be to fit data within model. Means... train our model on dataset. It takes time and for training you have two options in term of instance capacity. \n",
    "\n",
    "For me **m1.m4.xlarge** is still in free-tier hours(125hr). So, I am going to use it. otherwise the notebook instance **(m1.m5.xlarge)** which I used is better than this. But, as I discussed earlier **I had problem with cache data of instance memory. So, I used high power model building instance.** You are free to use any. \n",
    "> *Please refer the Sagemaker documentation for more information regarding price and capacity. Thank you*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "udjcJQxvJF33"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker:'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n",
      "WARNING:sagemaker:'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n"
     ]
    }
   ],
   "source": [
    "s3_input_train = sagemaker.s3_input(s3_data = train_location,content_type='csv')\n",
    "s3_input_validation = sagemaker.s3_input(s3_data = val_location, content_type= 'csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-12 02:43:55 Starting - Starting the training job...\n",
      "2020-06-12 02:43:57 Starting - Launching requested ML instances.........\n",
      "2020-06-12 02:45:32 Starting - Preparing the instances for training......\n",
      "2020-06-12 02:46:48 Downloading - Downloading input data\n",
      "2020-06-12 02:46:48 Training - Downloading the training image...\n",
      "2020-06-12 02:47:08 Training - Training image download completed. Training in progress.\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[34m[2020-06-12:02:47:09:INFO] Running standalone xgboost training.\u001b[0m\n",
      "\u001b[34m[2020-06-12:02:47:09:INFO] File size need to be processed in the node: 238.47mb. Available memory size in the node: 8469.27mb\u001b[0m\n",
      "\u001b[34m[2020-06-12:02:47:09:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[02:47:09] S3DistributionType set as FullyReplicated\u001b[0m\n",
      "\u001b[34m[02:47:10] 15000x5000 matrix with 75000000 entries loaded from /opt/ml/input/data/train?format=csv&label_column=0&delimiter=,\u001b[0m\n",
      "\u001b[34m[2020-06-12:02:47:10:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[02:47:10] S3DistributionType set as FullyReplicated\u001b[0m\n",
      "\u001b[34m[02:47:12] 10000x5000 matrix with 50000000 entries loaded from /opt/ml/input/data/validation?format=csv&label_column=0&delimiter=,\u001b[0m\n",
      "\u001b[34m[02:47:15] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 36 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[0]#011train-error:0.293133#011validation-error:0.3137\u001b[0m\n",
      "\u001b[34mMultiple eval metrics have been passed: 'validation-error' will be used for early stopping.\n",
      "\u001b[0m\n",
      "\u001b[34mWill train until validation-error hasn't improved in 10 rounds.\u001b[0m\n",
      "\u001b[34m[02:47:16] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 34 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[1]#011train-error:0.275133#011validation-error:0.2951\u001b[0m\n",
      "\u001b[34m[02:47:18] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 44 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[2]#011train-error:0.264867#011validation-error:0.2883\u001b[0m\n",
      "\u001b[34m[02:47:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 36 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[3]#011train-error:0.2648#011validation-error:0.2873\u001b[0m\n",
      "\u001b[34m[02:47:21] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[4]#011train-error:0.254#011validation-error:0.2752\u001b[0m\n",
      "\u001b[34m[02:47:22] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 40 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[5]#011train-error:0.2554#011validation-error:0.2791\u001b[0m\n",
      "\u001b[34m[02:47:23] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[6]#011train-error:0.2504#011validation-error:0.2724\u001b[0m\n",
      "\u001b[34m[02:47:24] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[7]#011train-error:0.2446#011validation-error:0.2664\u001b[0m\n",
      "\u001b[34m[02:47:26] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[8]#011train-error:0.237333#011validation-error:0.2613\u001b[0m\n",
      "\u001b[34m[02:47:27] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[9]#011train-error:0.227067#011validation-error:0.2504\u001b[0m\n",
      "\u001b[34m[02:47:28] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 38 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[10]#011train-error:0.224933#011validation-error:0.2515\u001b[0m\n",
      "\u001b[34m[02:47:30] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 38 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[11]#011train-error:0.2192#011validation-error:0.2447\u001b[0m\n",
      "\u001b[34m[02:47:31] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[12]#011train-error:0.214867#011validation-error:0.2396\u001b[0m\n",
      "\u001b[34m[02:47:32] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[13]#011train-error:0.2106#011validation-error:0.2352\u001b[0m\n",
      "\u001b[34m[02:47:33] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[14]#011train-error:0.2076#011validation-error:0.2328\u001b[0m\n",
      "\u001b[34m[02:47:35] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[15]#011train-error:0.200267#011validation-error:0.2243\u001b[0m\n",
      "\u001b[34m[02:47:36] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[16]#011train-error:0.195933#011validation-error:0.2231\u001b[0m\n",
      "\u001b[34m[02:47:37] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[17]#011train-error:0.191933#011validation-error:0.2208\u001b[0m\n",
      "\u001b[34m[02:47:38] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 34 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[18]#011train-error:0.188067#011validation-error:0.2163\u001b[0m\n",
      "\u001b[34m[02:47:40] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[19]#011train-error:0.1862#011validation-error:0.2145\u001b[0m\n",
      "\u001b[34m[02:47:41] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[20]#011train-error:0.1844#011validation-error:0.2123\u001b[0m\n",
      "\u001b[34m[02:47:42] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[21]#011train-error:0.18#011validation-error:0.2096\u001b[0m\n",
      "\u001b[34m[02:47:43] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[22]#011train-error:0.1776#011validation-error:0.2063\u001b[0m\n",
      "\u001b[34m[02:47:45] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[23]#011train-error:0.1764#011validation-error:0.2063\u001b[0m\n",
      "\u001b[34m[02:47:46] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[24]#011train-error:0.1744#011validation-error:0.2013\u001b[0m\n",
      "\u001b[34m[02:47:47] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[25]#011train-error:0.1724#011validation-error:0.1986\u001b[0m\n",
      "\u001b[34m[02:47:48] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[26]#011train-error:0.171#011validation-error:0.1985\u001b[0m\n",
      "\u001b[34m[02:47:50] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[27]#011train-error:0.168333#011validation-error:0.1966\u001b[0m\n",
      "\u001b[34m[02:47:51] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[28]#011train-error:0.167267#011validation-error:0.1957\u001b[0m\n",
      "\u001b[34m[02:47:52] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[29]#011train-error:0.166067#011validation-error:0.1938\u001b[0m\n",
      "\u001b[34m[02:47:54] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 32 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[30]#011train-error:0.164333#011validation-error:0.1916\u001b[0m\n",
      "\u001b[34m[02:47:55] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[31]#011train-error:0.162067#011validation-error:0.1897\u001b[0m\n",
      "\u001b[34m[02:47:56] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[32]#011train-error:0.161#011validation-error:0.1903\u001b[0m\n",
      "\u001b[34m[02:47:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[33]#011train-error:0.1602#011validation-error:0.1902\u001b[0m\n",
      "\u001b[34m[02:47:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[34]#011train-error:0.1572#011validation-error:0.1891\u001b[0m\n",
      "\u001b[34m[02:48:00] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[35]#011train-error:0.155733#011validation-error:0.1874\u001b[0m\n",
      "\u001b[34m[02:48:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 34 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[36]#011train-error:0.154533#011validation-error:0.1856\u001b[0m\n",
      "\u001b[34m[02:48:02] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 34 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[37]#011train-error:0.152133#011validation-error:0.1835\u001b[0m\n",
      "\u001b[34m[02:48:04] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[38]#011train-error:0.151467#011validation-error:0.1838\u001b[0m\n",
      "\u001b[34m[02:48:05] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[39]#011train-error:0.151#011validation-error:0.1836\u001b[0m\n",
      "\u001b[34m[02:48:06] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[40]#011train-error:0.150067#011validation-error:0.1815\u001b[0m\n",
      "\u001b[34m[02:48:07] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[41]#011train-error:0.149067#011validation-error:0.1815\u001b[0m\n",
      "\u001b[34m[02:48:09] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[42]#011train-error:0.147133#011validation-error:0.1818\u001b[0m\n",
      "\u001b[34m[02:48:10] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[43]#011train-error:0.1458#011validation-error:0.1805\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[02:48:11] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[44]#011train-error:0.145#011validation-error:0.1803\u001b[0m\n",
      "\u001b[34m[02:48:13] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 32 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[45]#011train-error:0.143933#011validation-error:0.1783\u001b[0m\n",
      "\u001b[34m[02:48:14] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 16 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[46]#011train-error:0.141933#011validation-error:0.1786\u001b[0m\n",
      "\u001b[34m[02:48:15] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[47]#011train-error:0.140867#011validation-error:0.1785\u001b[0m\n",
      "\u001b[34m[02:48:16] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[48]#011train-error:0.140533#011validation-error:0.1776\u001b[0m\n",
      "\u001b[34m[02:48:18] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[49]#011train-error:0.140333#011validation-error:0.1765\u001b[0m\n",
      "\u001b[34m[02:48:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[50]#011train-error:0.138467#011validation-error:0.1745\u001b[0m\n",
      "\u001b[34m[02:48:20] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[51]#011train-error:0.137667#011validation-error:0.174\u001b[0m\n",
      "\u001b[34m[02:48:21] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[52]#011train-error:0.137467#011validation-error:0.1733\u001b[0m\n",
      "\u001b[34m[02:48:23] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[53]#011train-error:0.1368#011validation-error:0.1722\u001b[0m\n",
      "\u001b[34m[02:48:24] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[54]#011train-error:0.135733#011validation-error:0.1713\u001b[0m\n",
      "\u001b[34m[02:48:25] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[55]#011train-error:0.134667#011validation-error:0.1695\u001b[0m\n",
      "\u001b[34m[02:48:27] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[56]#011train-error:0.133467#011validation-error:0.1694\u001b[0m\n",
      "\u001b[34m[02:48:28] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 16 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[57]#011train-error:0.1332#011validation-error:0.1703\u001b[0m\n",
      "\u001b[34m[02:48:29] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[58]#011train-error:0.1334#011validation-error:0.169\u001b[0m\n",
      "\u001b[34m[02:48:30] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[59]#011train-error:0.132067#011validation-error:0.1685\u001b[0m\n",
      "\u001b[34m[02:48:32] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 16 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[60]#011train-error:0.1308#011validation-error:0.1692\u001b[0m\n",
      "\u001b[34m[02:48:33] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[61]#011train-error:0.1298#011validation-error:0.1678\u001b[0m\n",
      "\u001b[34m[02:48:34] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[62]#011train-error:0.13#011validation-error:0.1669\u001b[0m\n",
      "\u001b[34m[02:48:35] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[63]#011train-error:0.129733#011validation-error:0.1658\u001b[0m\n",
      "\u001b[34m[02:48:37] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[64]#011train-error:0.129133#011validation-error:0.1639\u001b[0m\n",
      "\u001b[34m[02:48:38] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 16 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[65]#011train-error:0.128267#011validation-error:0.1647\u001b[0m\n",
      "\u001b[34m[02:48:39] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[66]#011train-error:0.1274#011validation-error:0.1653\u001b[0m\n",
      "\u001b[34m[02:48:40] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[67]#011train-error:0.125933#011validation-error:0.1647\u001b[0m\n",
      "\u001b[34m[02:48:42] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[68]#011train-error:0.1254#011validation-error:0.1642\u001b[0m\n",
      "\u001b[34m[02:48:43] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[69]#011train-error:0.125#011validation-error:0.1631\u001b[0m\n",
      "\u001b[34m[02:48:44] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[70]#011train-error:0.1248#011validation-error:0.1623\u001b[0m\n",
      "\u001b[34m[02:48:45] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[71]#011train-error:0.123733#011validation-error:0.1628\u001b[0m\n",
      "\u001b[34m[02:48:47] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[72]#011train-error:0.123#011validation-error:0.1626\u001b[0m\n",
      "\u001b[34m[02:48:48] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[73]#011train-error:0.122333#011validation-error:0.1622\u001b[0m\n",
      "\u001b[34m[02:48:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[74]#011train-error:0.1218#011validation-error:0.1623\u001b[0m\n",
      "\u001b[34m[02:48:50] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[75]#011train-error:0.120867#011validation-error:0.1621\u001b[0m\n",
      "\u001b[34m[02:48:52] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[76]#011train-error:0.12#011validation-error:0.1612\u001b[0m\n",
      "\u001b[34m[02:48:53] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[77]#011train-error:0.119467#011validation-error:0.1614\u001b[0m\n",
      "\u001b[34m[02:48:54] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[78]#011train-error:0.119267#011validation-error:0.161\u001b[0m\n",
      "\u001b[34m[02:48:55] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[79]#011train-error:0.118533#011validation-error:0.1602\u001b[0m\n",
      "\u001b[34m[02:48:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[80]#011train-error:0.118467#011validation-error:0.1592\u001b[0m\n",
      "\u001b[34m[02:48:58] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[81]#011train-error:0.116267#011validation-error:0.1593\u001b[0m\n",
      "\u001b[34m[02:48:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[82]#011train-error:0.115467#011validation-error:0.1588\u001b[0m\n",
      "\u001b[34m[02:49:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[83]#011train-error:0.115333#011validation-error:0.158\u001b[0m\n",
      "\u001b[34m[02:49:02] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[84]#011train-error:0.115533#011validation-error:0.1579\u001b[0m\n",
      "\u001b[34m[02:49:03] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[85]#011train-error:0.1138#011validation-error:0.1584\u001b[0m\n",
      "\u001b[34m[02:49:04] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 20 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[86]#011train-error:0.112733#011validation-error:0.1578\u001b[0m\n",
      "\u001b[34m[02:49:06] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[87]#011train-error:0.113133#011validation-error:0.1575\u001b[0m\n",
      "\u001b[34m[02:49:07] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[88]#011train-error:0.113333#011validation-error:0.1569\u001b[0m\n",
      "\u001b[34m[02:49:08] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 16 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[89]#011train-error:0.1124#011validation-error:0.1558\u001b[0m\n",
      "\u001b[34m[02:49:09] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[90]#011train-error:0.112133#011validation-error:0.1549\u001b[0m\n",
      "\u001b[34m[02:49:11] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[91]#011train-error:0.111533#011validation-error:0.1551\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[02:49:12] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[92]#011train-error:0.111267#011validation-error:0.1553\u001b[0m\n",
      "\u001b[34m[02:49:13] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[93]#011train-error:0.1108#011validation-error:0.1551\u001b[0m\n",
      "\u001b[34m[02:49:15] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[94]#011train-error:0.110267#011validation-error:0.1542\u001b[0m\n",
      "\u001b[34m[02:49:16] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[95]#011train-error:0.110067#011validation-error:0.1542\u001b[0m\n",
      "\u001b[34m[02:49:17] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[96]#011train-error:0.109333#011validation-error:0.1539\u001b[0m\n",
      "\u001b[34m[02:49:18] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[97]#011train-error:0.109133#011validation-error:0.1536\u001b[0m\n",
      "\u001b[34m[02:49:20] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[98]#011train-error:0.108267#011validation-error:0.1527\u001b[0m\n",
      "\u001b[34m[02:49:21] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[99]#011train-error:0.108#011validation-error:0.1524\u001b[0m\n",
      "\u001b[34m[02:49:22] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[100]#011train-error:0.1078#011validation-error:0.1524\u001b[0m\n",
      "\u001b[34m[02:49:23] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 16 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[101]#011train-error:0.107467#011validation-error:0.1525\u001b[0m\n",
      "\u001b[34m[02:49:25] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[102]#011train-error:0.1068#011validation-error:0.1522\u001b[0m\n",
      "\u001b[34m[02:49:26] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[103]#011train-error:0.106333#011validation-error:0.1528\u001b[0m\n",
      "\u001b[34m[02:49:27] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[104]#011train-error:0.105933#011validation-error:0.1528\u001b[0m\n",
      "\u001b[34m[02:49:28] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[105]#011train-error:0.105333#011validation-error:0.1526\u001b[0m\n",
      "\u001b[34m[02:49:30] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[106]#011train-error:0.1044#011validation-error:0.153\u001b[0m\n",
      "\u001b[34m[02:49:31] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 32 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[107]#011train-error:0.103867#011validation-error:0.1531\u001b[0m\n",
      "\u001b[34m[02:49:32] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 20 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[108]#011train-error:0.104#011validation-error:0.153\u001b[0m\n",
      "\u001b[34m[02:49:34] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[109]#011train-error:0.103533#011validation-error:0.1529\u001b[0m\n",
      "\u001b[34m[02:49:35] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[110]#011train-error:0.1028#011validation-error:0.1532\u001b[0m\n",
      "\u001b[34m[02:49:36] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 16 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[111]#011train-error:0.1022#011validation-error:0.1528\u001b[0m\n",
      "\u001b[34m[02:49:37] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[112]#011train-error:0.1028#011validation-error:0.1527\u001b[0m\n",
      "\u001b[34mStopping. Best iteration:\u001b[0m\n",
      "\u001b[34m[102]#011train-error:0.1068#011validation-error:0.1522\n",
      "\u001b[0m\n",
      "\n",
      "2020-06-12 02:49:48 Uploading - Uploading generated training model\n",
      "2020-06-12 02:49:48 Completed - Training job completed\n",
      "Training seconds: 201\n",
      "Billable seconds: 201\n"
     ]
    }
   ],
   "source": [
    "xgb.fit({'train': s3_input_train,'validation': s3_input_validation}) # (start 10:44 ---> 10:50 : 6+ minute)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, our model is trained, Check our instance for model info. In case if you have problem to find it, then better option is log_files of sagemaker will give you idea of background procedure.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QbeDszgHth8g"
   },
   "source": [
    "### Testing Model\n",
    "\n",
    "I will use SageMakers Batch Transform functionality.\n",
    "\n",
    "Batch Transform is a convenient way to perform inference on a large dataset in a way that is not realtime. That is, we don't necessarily need to use our model's results immediately and instead we can peform inference on a large number of samples.\n",
    "\n",
    "**Applications:**\n",
    ">Industries, which run their business continueously and want to predict their growth and customer service periodically, may be at the end of week, or end of month. They will use batch transform. So, its not used for realtime applications. Small businesses mostly use it. Sometime industry giants use it for specific problem solution. (as an example, some specific region have issue with specific type of product, then for 5W QA analysis they can use it.) \n",
    "\n",
    "---\n",
    "the following procedure takes some time (5 to 10 minute)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bBq5rqm2JF1X"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker:Parameter image will be renamed to image_uri in SageMaker Python SDK v2.\n",
      "WARNING:sagemaker:Using already existing model: xgboost-2020-06-12-02-43-55-217\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....................\u001b[34mArguments: serve\u001b[0m\n",
      "\u001b[34m[2020-06-12 02:58:13 +0000] [1] [INFO] Starting gunicorn 19.7.1\u001b[0m\n",
      "\u001b[34m[2020-06-12 02:58:13 +0000] [1] [INFO] Listening at: http://0.0.0.0:8080 (1)\u001b[0m\n",
      "\u001b[34m[2020-06-12 02:58:13 +0000] [1] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[34m[2020-06-12 02:58:13 +0000] [37] [INFO] Booting worker with pid: 37\u001b[0m\n",
      "\u001b[34m[2020-06-12 02:58:13 +0000] [38] [INFO] Booting worker with pid: 38\u001b[0m\n",
      "\u001b[34m[2020-06-12 02:58:13 +0000] [39] [INFO] Booting worker with pid: 39\u001b[0m\n",
      "\u001b[34m[2020-06-12 02:58:13 +0000] [40] [INFO] Booting worker with pid: 40\u001b[0m\n",
      "\u001b[34m[2020-06-12:02:58:13:INFO] Model loaded successfully for worker : 37\u001b[0m\n",
      "\u001b[34m[2020-06-12:02:58:13:INFO] Model loaded successfully for worker : 38\u001b[0m\n",
      "\u001b[34m[2020-06-12:02:58:13:INFO] Model loaded successfully for worker : 39\u001b[0m\n",
      "\u001b[34m[2020-06-12:02:58:13:INFO] Model loaded successfully for worker : 40\u001b[0m\n",
      "\u001b[32m2020-06-12T02:58:26.894:[sagemaker logs]: MaxConcurrentTransforms=4, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n",
      "\u001b[34m[2020-06-12:02:58:29:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-12:02:58:29:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-12:02:58:29:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-12:02:58:29:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-12:02:58:29:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-12:02:58:29:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-12:02:58:29:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-12:02:58:29:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-12:02:58:29:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-12:02:58:29:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-12:02:58:29:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-12:02:58:29:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-12:02:58:30:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-12:02:58:30:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-12:02:58:30:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-12:02:58:30:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-12:02:58:32:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-12:02:58:32:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-12:02:58:32:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-12:02:58:32:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-12:02:58:32:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-12:02:58:32:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-12:02:58:32:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-12:02:58:32:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-12:02:58:32:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-12:02:58:32:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-12:02:58:32:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-12:02:58:32:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-12:02:58:32:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-12:02:58:32:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-12:02:58:32:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-12:02:58:32:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-12:02:58:34:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-12:02:58:34:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-12:02:58:34:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-12:02:58:34:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-12:02:58:34:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-12:02:58:34:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-12:02:58:34:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-12:02:58:34:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-12:02:58:34:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-12:02:58:34:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-12:02:58:34:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-12:02:58:34:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-12:02:58:35:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-12:02:58:35:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-12:02:58:35:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-12:02:58:35:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-12:02:58:37:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-12:02:58:37:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-12:02:58:37:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-12:02:58:37:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-12:02:58:37:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-12:02:58:37:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-12:02:58:37:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-12:02:58:37:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-12:02:58:37:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-12:02:58:37:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-12:02:58:37:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-12:02:58:37:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-12:02:58:37:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-12:02:58:37:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-12:02:58:37:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-12:02:58:37:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-12:02:58:40:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-12:02:58:40:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-12:02:58:40:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-12:02:58:40:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-12:02:58:41:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-12:02:58:41:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-12:02:58:41:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-12:02:58:41:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-12:02:58:41:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-12:02:58:41:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-12:02:58:41:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-12:02:58:41:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-12:02:58:42:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-12:02:58:42:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-12:02:58:42:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-12:02:58:42:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-12:02:58:42:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-12:02:58:42:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-12:02:58:42:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-12:02:58:42:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-12:02:58:44:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-12:02:58:44:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-12:02:58:44:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-12:02:58:44:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-12:02:58:44:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-12:02:58:44:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-12:02:58:44:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-12:02:58:44:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-12:02:58:44:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-12:02:58:44:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-12:02:58:44:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-12:02:58:44:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-12:02:58:45:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-12:02:58:45:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-12:02:58:45:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-12:02:58:45:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-12:02:58:46:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-12:02:58:46:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-12:02:58:46:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-12:02:58:46:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-12:02:58:46:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-12:02:58:46:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-12:02:58:46:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-12:02:58:46:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-12:02:58:47:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-12:02:58:47:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-12:02:58:47:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-12:02:58:47:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-12:02:58:47:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-12:02:58:47:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-12:02:58:47:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-12:02:58:47:INFO] Determined delimiter of CSV input is ','\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m[2020-06-12:02:58:49:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-12:02:58:49:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-12:02:58:49:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-12:02:58:49:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-12:02:58:50:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-12:02:58:50:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-12:02:58:50:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-12:02:58:50:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-12:02:58:51:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-12:02:58:51:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-12:02:58:51:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-12:02:58:51:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-12:02:58:51:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-12:02:58:51:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-12:02:58:51:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-12:02:58:51:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-12:02:58:52:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-12:02:58:52:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-06-12:02:58:52:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-12:02:58:52:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-12:02:58:52:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-12:02:58:52:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-12:02:58:52:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-12:02:58:52:INFO] Determined delimiter of CSV input is ','\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "xgb_transformer = xgb.transformer(instance_count =1, instance_type = 'ml.m4.xlarge')       # used Batch_transform method from sagemaker\n",
    "\n",
    "xgb_transformer.transform(test_location,content_type = 'text/csv',split_type= 'Line')     # read data from test location for predictinog result.\n",
    "\n",
    "xgb_transformer.wait()                                                                     # wait for response "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2JXzONtjJFy8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 256.0 KiB/368.8 KiB (2.7 MiB/s) with 1 file(s) remaining\r",
      "Completed 368.8 KiB/368.8 KiB (3.9 MiB/s) with 1 file(s) remaining\r",
      "download: s3://sagemaker-us-west-2-337299574287/xgboost-2020-06-12-02-55-02-519/test.csv.out to ../data/xgboost/test.csv.out\r\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp --recursive $xgb_transformer.output_path $data_dir                               # save test result to s3 (for local use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3UxHTy9Ox2Ry"
   },
   "outputs": [],
   "source": [
    "# For, accuracy metric calculation. \n",
    "predictions = pd.read_csv(os.path.join(data_dir,'test.csv.out'),header=None)\n",
    "predictions = [round(num) for num in predictions.squeeze().values]              # create list of prediction values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9Y_hgNIfx2OR"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85112"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy of our model for performance evaluation.\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(test_y,predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k28nwMw96MeJ"
   },
   "source": [
    "### Clean up disk and dir (free memory for next prediction)\n",
    "\n",
    "The default notebook instance on SageMaker doesn't have a lot of excess disk space available. As you continue to complete and execute other notebooks you will eventually fill up this disk space, leading to errors which can be difficult to diagnose. \n",
    "\n",
    "Once you are completely finished using a notebook it is a good idea to remove the files that you created along the way. Of course, you can do this from the terminal or from the notebook hub if you would like. The cell below contains some commands to clean up the created files from within the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PQXA_m0mx2Jk"
   },
   "outputs": [],
   "source": [
    "# first delete the files from directory\n",
    "!rm $data_dir/*\n",
    "\n",
    "# delete directory itself\n",
    "!rmdir $data_dir\n",
    "\n",
    "# remove all the files in the cache_dir\n",
    "!rm $cache_dir/*\n",
    "\n",
    "# remove cache_directory itself\n",
    "!rmdir $cache_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j-ye14Eix2GK"
   },
   "outputs": [],
   "source": [
    "# Keep Learning,Enjoy Empowering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Sentimental_Analytics(AWS_SageMaker_BT.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
